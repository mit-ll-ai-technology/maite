{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox import (\n",
    "    load_dataset,\n",
    "    load_model,\n",
    "    list_metrics,\n",
    "    load_metric,\n",
    "    evaluate,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    " # Load test split of CIFAR-10 from TorchVision to evaluate pretrained models.\n",
    "\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "data = load_dataset(\n",
    "    provider=\"torchvision\",\n",
    "    dataset_name=\"CIFAR10\",\n",
    "    task=\"image-classification\",\n",
    "    split=\"test\",\n",
    "    root=\"~/.cache/torchvision/datasets\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "data.set_transform(lambda x: {\"image\": to_tensor(x[\"image\"]), \"label\": x[\"label\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Get mapping from integers to class names\n",
    " \n",
    "int2name = data.features[\"label\"][\"names\"]\n",
    "int2name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ViT model (vision transformer)\n",
    "\n",
    "model = load_model(\n",
    "    provider=\"huggingface\",\n",
    "    model_name=\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\",\n",
    "    task=\"image-classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "ImageClassifierOutput(loss=None, logits=tensor([[-0.2819, -0.5155, -0.4322,  3.3778, -0.4935,  0.0070, -0.2550, -0.5460,\n",
      "         -0.2329, -0.5659]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Verify model interface works with data\n",
    "\n",
    "# HuggingFace models have a preprocessor to convert PIL images to tensors\n",
    "input = model.preprocessor([data[0][\"image\"]])\n",
    "print(input[\"image\"].shape)\n",
    "\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List and Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy',\n",
       " 'AUROC',\n",
       " 'AveragePrecision',\n",
       " 'BLEUScore',\n",
       " 'CalibrationError',\n",
       " 'CatMetric',\n",
       " 'CharErrorRate',\n",
       " 'CHRFScore',\n",
       " 'ConcordanceCorrCoef',\n",
       " 'CohenKappa',\n",
       " 'ConfusionMatrix',\n",
       " 'CosineSimilarity',\n",
       " 'CramersV',\n",
       " 'Dice',\n",
       " 'TweedieDevianceScore',\n",
       " 'ErrorRelativeGlobalDimensionlessSynthesis',\n",
       " 'ExactMatch',\n",
       " 'ExplainedVariance',\n",
       " 'ExtendedEditDistance',\n",
       " 'F1Score']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the first 20 metrics from TorchMetrics\n",
    "\n",
    "list_metrics(provider=\"torchmetrics\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUC',\n",
       " 'BinaryAccuracy',\n",
       " 'BinaryAUPRC',\n",
       " 'BinaryAUROC',\n",
       " 'BinaryBinnedAUROC',\n",
       " 'BinaryBinnedPrecisionRecallCurve',\n",
       " 'BinaryConfusionMatrix',\n",
       " 'BinaryF1Score',\n",
       " 'BinaryNormalizedEntropy',\n",
       " 'BinaryPrecision',\n",
       " 'BinaryPrecisionRecallCurve',\n",
       " 'BinaryRecall',\n",
       " 'BinaryRecallAtFixedPrecision',\n",
       " 'BLEUScore',\n",
       " 'Cat',\n",
       " 'ClickThroughRate',\n",
       " 'HitRate',\n",
       " 'Max',\n",
       " 'Mean',\n",
       " 'MeanSquaredError']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the first 20 metrics from TorchEval\n",
    "\n",
    "list_metrics(provider=\"torcheval\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metrics in TorchMetrics: 79\n",
      "Number of metrics in TorchEval: 50\n"
     ]
    }
   ],
   "source": [
    "# Count number of metrics from each provider\n",
    "\n",
    "print(f'Number of metrics in TorchMetrics: {len(list_metrics(provider=\"torchmetrics\"))}')\n",
    "print(f'Number of metrics in TorchEval: {len(list_metrics(provider=\"torcheval\"))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure collection of classification metrics (with class-specific results)\n",
    "\n",
    "metrics = dict(\n",
    "    accuracy_te=load_metric(provider=\"torcheval\", metric_name=\"MulticlassAccuracy\", num_classes=10, average=\"none\"),\n",
    "    accuracy_tm=load_metric(provider=\"torchmetrics\", metric_name=\"Accuracy\", task=\"multiclass\", num_classes=10, average=\"none\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Helper function for converting/displaying metrics dictionaries as dataframes \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def metrics_to_df(names, output):\n",
    "    \"\"\" Converts metrics result dictionary to a dataframe. \"\"\"\n",
    "    results = dict(\n",
    "        classes=list(names),\n",
    "        accuracy_te=[output[\"accuracy_te\"][i].item() for i in range(10)],\n",
    "        accuracy_tm=[output[\"accuracy_tm\"][i].item() for i in range(10)],\n",
    "    )\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create evaluator for image classification task\n",
    "\n",
    "evaluator = evaluate(task=\"image-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Select subset of test data (for purpose of demo)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "indices = torch.randperm(len(data))[:1024]\n",
    "data_subset = torch.utils.data.Subset(data, indices)\n",
    "\n",
    "len(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33abb2e8231f4424a7c206018e3deffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Run evaluation\n",
    " \n",
    " # Reset metrics\n",
    "[m.reset() for m in metrics.values()]\n",
    "\n",
    "output = evaluator(\n",
    "    model,\n",
    "    data_subset,\n",
    "    metric=metrics,\n",
    "    batch_size=32,\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy (TorchEval) = 0.978\n",
      "Model accuracy (TorchMetrics) = 0.978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>accuracy_te</th>\n",
       "      <th>accuracy_tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airplane</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automobile</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deer</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>frog</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ship</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      classes  accuracy_te  accuracy_tm\n",
       "0    airplane        0.973        0.973\n",
       "1  automobile        1.000        1.000\n",
       "2        bird        0.991        0.991\n",
       "3         cat        1.000        1.000\n",
       "4        deer        0.989        0.989\n",
       "5         dog        0.917        0.917\n",
       "6        frog        0.966        0.966\n",
       "7       horse        0.991        0.991\n",
       "8        ship        0.991        0.991\n",
       "9       truck        0.963        0.963"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print metrics\n",
    "\n",
    "df = metrics_to_df(int2name, output)\n",
    "\n",
    "print(f\"Model accuracy (TorchEval) = {df.accuracy_te.mean():0.3f}\")\n",
    "print(f\"Model accuracy (TorchMetrics) = {df.accuracy_tm.mean():0.3f}\")\n",
    "\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jatic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa47504221f63fab356dc2bcdd78fe77b0a482aecdf8651b01d04ba193e9df16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
