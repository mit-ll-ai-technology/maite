{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "\n",
    "Be sure to install the optional requirements:\n",
    "\n",
    "```shell\n",
    "$ pip install -e .[hydra_zen,smqtk,huggingface]\n",
    "```\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from io import BytesIO\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch as tr\n",
    "from PIL import Image\n",
    "\n",
    "from jatic_toolbox.interop import ObjectDetectionOutput\n",
    "from jatic_toolbox.protocols import (\n",
    "    HasObjectDetections,\n",
    "    ImageType,\n",
    "    ObjectDetector,\n",
    "    ModelOutput,\n",
    "    NDArray\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Pipeline\n",
    "\n",
    "Here we define a simple detection pipeline using `jatic_toolbox.protocols`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "def eval_image_detector(\n",
    "    detector: ObjectDetector, data: Sequence[ImageType], threshold: float = 0.4\n",
    ") -> ObjectDetectionOutput:\n",
    "    \"\"\"\n",
    "    Evalulator for Object Detection Models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    detector : Model\n",
    "        A `jatic_toolbox.protocols.Model` object.\n",
    "\n",
    "    img_iter : Sequence[ImageType]\n",
    "        A sequence of images (PIL.Image, NDArray, or torch.Tensor)\n",
    "\n",
    "    threshold : float = 0.4\n",
    "        Value to threshold detections.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ObjectDetectionOutput\n",
    "        Collections of bounding boxes and scores.\n",
    "    \"\"\"\n",
    "    output = detector(data)\n",
    "    assert isinstance(output, HasObjectDetections)\n",
    "    boxes = output.boxes\n",
    "    scores = output.scores\n",
    "\n",
    "    boxes_threshold: List[NDArray] = []\n",
    "    scores_threshold: List[List[Dict[str, float]]] = []\n",
    "    for boxes_per_image, scores_per_image in zip(boxes, scores):\n",
    "\n",
    "        _image_boxes = []\n",
    "        _image_scores: List[Dict[str, float]] = []\n",
    "        for boxes, score_dict in zip(boxes_per_image, scores_per_image):\n",
    "            cls_idx = max(score_dict, key=score_dict.get)\n",
    "            conf = score_dict[cls_idx]\n",
    "\n",
    "            if conf >= threshold:\n",
    "                _image_boxes.append(boxes)\n",
    "                _image_scores.append({cls_idx: conf})\n",
    "        \n",
    "        boxes_threshold.append(np.asarray(_image_boxes))\n",
    "        scores_threshold.append(_image_scores)\n",
    "\n",
    "    return ObjectDetectionOutput(boxes=boxes_threshold, scores=scores_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.kitware.com/api/v1/item/623880f14acac99f429fe3ca/download\"\n",
    "response = requests.get(url)\n",
    "img_pil = Image.open(BytesIO(response.content))\n",
    "img = np.asarray(img_pil)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dets(ax, dets: ObjectDetectionOutput, image_index: int, add_prediction: bool=False):\n",
    "    all_boxes = np.asarray(dets.boxes)\n",
    "    all_scores = dets.scores\n",
    "    assert len(all_boxes) == len(all_scores), \"must have same batch size\"\n",
    "    bboxes = all_boxes[image_index]\n",
    "    scores = all_scores[image_index]\n",
    "\n",
    "    assert len(bboxes) == len(scores), \"must have same number of detections and scores per image\"\n",
    "\n",
    "    for bbox, score_dict in zip(bboxes, scores):\n",
    "        cls_idx = max(score_dict, key=score_dict.get)\n",
    "        conf = score_dict[cls_idx]\n",
    "\n",
    "        x1, y1 = bbox[:2]\n",
    "        x2, y2 = bbox[-2:]\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                linewidth=1,\n",
    "                edgecolor=\"r\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if add_prediction:\n",
    "            ax.text(x1, y1, f\"{cls_idx}: {conf:.1f}\", fontsize=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.interop.smqtk import CenterNetVisdrone\n",
    "\n",
    "model = CenterNetVisdrone()\n",
    " \n",
    "print(f\"Loading URL: {url}\")\n",
    "response = requests.get(url)\n",
    "img_pil = Image.open(BytesIO(response.content))\n",
    "img = np.asarray(img_pil)\n",
    "dets = eval_image_detector(model, [img], threshold=0.4)\n",
    "\n",
    "# run validation check, throws an error if\n",
    "# boxes is not a list of NumPy arrays\n",
    "# with a detection index and box dimension.\n",
    "assert len(dets.boxes) == 1\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8))\n",
    "axs.set_title(f\"Model: SMQTK CenterNet\")\n",
    "axs.imshow(img)\n",
    "axs.axis(\"off\")\n",
    "show_dets(axs, dets, image_index=0, add_prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.interop.huggingface import HuggingFaceObjectDetector\n",
    "\n",
    "model = HuggingFaceObjectDetector(\"facebook/detr-resnet-50\")\n",
    " \n",
    "print(f\"Loading URL: {url}\")\n",
    "response = requests.get(url)\n",
    "img_pil = Image.open(BytesIO(response.content))\n",
    "img = np.asarray(img_pil)\n",
    "dets = eval_image_detector(model, [img], threshold=0.4)\n",
    "\n",
    "# run validation check, throws an error if\n",
    "# boxes is not a list of NumPy arrays\n",
    "# with a detection index and box dimension.\n",
    "assert len(dets.boxes) == 1\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8))\n",
    "axs.set_title(f\"Model: HuggingFace Facebook DETR ResNet-50\")\n",
    "axs.imshow(img)\n",
    "axs.axis(\"off\")\n",
    "show_dets(axs, dets, image_index=0, add_prediction=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface with IPyWidgets\n",
    "\n",
    "In this section we will demonstrate using `hydra-zen` configs generated for HuggingFace and SMQTK object detector to build\n",
    "a UI for selecting different models and visualizing detections using IPyWidgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra_zen import instantiate\n",
    "\n",
    "from jatic_toolbox._internals.interop.hydra_zen import create_smqtk_model_config\n",
    "\n",
    "# create the configs and add to the config store (this is an in-memory only storage)\n",
    "jatic_configs = create_smqtk_model_config()\n",
    "jatic_configs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.interop.hydra_zen import create_huggingface_model_config\n",
    "\n",
    "from huggingface_hub import (\n",
    "    HfApi,\n",
    "    ModelFilter,\n",
    "    ModelSearchArguments,\n",
    ")\n",
    "\n",
    "# using the HuggingFace API to search for available modles\n",
    "# that meet a given query.\n",
    "api = HfApi()\n",
    "model_args = ModelSearchArguments()\n",
    "\n",
    "filt = ModelFilter(\n",
    "    task=model_args.pipeline_tag.ObjectDetection,\n",
    "    model_name=\"detr\",\n",
    "    library=model_args.library.PyTorch,\n",
    "    \n",
    ")\n",
    "\n",
    "# create configs and add to the config store\n",
    "jatic_configs = create_huggingface_model_config(filter=filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# extract the config names for all models in the config store\n",
    "models = [k[1] for k in jatic_configs[\"model\"].keys()]\n",
    "\n",
    "# add the models to a drop down menu\n",
    "obj_det = widgets.Dropdown(\n",
    "    options=models,\n",
    "    value=models[0],\n",
    "    description='Object Detectors:',\n",
    "    layout=widgets.Layout(width=\"max-content\")\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Launch!\", layout=widgets.Layout(margin=\"30px\"))\n",
    "output = widgets.Output()\n",
    "\n",
    "display(obj_det, button, output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        if len(obj_det.value) > 0:\n",
    "            clear_output()\n",
    "\n",
    "            print(f\"Loading URL: {url}\")\n",
    "            response = requests.get(url)\n",
    "            img_pil = Image.open(BytesIO(response.content))\n",
    "            img = np.asarray(img_pil)\n",
    "\n",
    "            print(f\"Launching run on {obj_det.value}\")\n",
    "\n",
    "            model = instantiate(jatic_configs[\"model\", obj_det.value])\n",
    "            dets = eval_image_detector(model, [img], threshold=0.4)\n",
    "\n",
    "            fig, axs = plt.subplots(figsize=(8, 8))\n",
    "            axs.set_title(f\"Model: {obj_det.value}\")\n",
    "            axs.imshow(img)\n",
    "            axs.axis(\"off\")\n",
    "            show_dets(axs, dets, 0, add_prediction=True)\n",
    "            show_inline_matplotlib_plots()\n",
    "        else:\n",
    "            print(f\"Please select a model\")\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96488da073e8e9e2ba35a3258322ede4639e0f673528857740635564285f59e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
