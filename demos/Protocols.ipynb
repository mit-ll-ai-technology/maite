{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Protocols\n",
    "\n",
    "> Justin Goodwin, MIT Lincoln Laboratory\n",
    "\n",
    "> Feb. 14, 2023\n",
    "\n",
    "\n",
    "## Why Type Annotations and Protocols?\n",
    "\n",
    "Python type annotations and protocols in API design helps to improve code quality, readability, and collaboration, while also enabling better tooling and more flexible code.\n",
    "\n",
    "  - Improved readability: Type annotations provide clear and concise documentation for what each function or method is supposed to receive as inputs and return as outputs. This makes the code more readable and easier to understand, especially for developers who are new to the project.\n",
    "  - Enhanced code quality: Type annotations help catch errors early in the development process, making it easier to detect and fix bugs. This can result in improved code quality and reduced maintenance costs over time.\n",
    "  - Better tooling: Type annotations enable powerful tools such as type checkers, linters, and IDEs to provide more accurate and helpful insights into the code. For example, with type annotations, a type checker can verify that a function is being passed the correct types of arguments and that it is returning the expected type of output.\n",
    "  - Improved collaboration: Type annotations serve as a shared understanding between developers about the expected inputs and outputs of functions and methods. This helps to ensure that everyone is on the same page, reducing the chance of misunderstandings and increasing collaboration efficiency.\n",
    "  - Protocols: Python's support for protocols allows for the creation of highly flexible and reusable code. Protocols allow you to define a set of methods and properties that a class must implement, without specifying a particular implementation. This means that different classes can implement the same protocol in different ways, making it easier to write reusable code that works with a variety of types.\n",
    "\n",
    "\n",
    "## Summary of Review\n",
    "This review will highlight the protocols in the `jatic_toolbox.protocols` package, which have been developed to:\n",
    "\n",
    "  1. Specify objects that can be converted into arrays: `ArrayLike` and `ShapedArray`.\n",
    "  2. Define collections of homogeneous objects: `TypedCollection`.\n",
    "  3. Provide a standard interface for augmenting images, videos, bounding boxes, keypoints, etc.: `Augmentations`.\n",
    "  4. Establish a standard interface for machine learning models: `Model`, which includes `Classifier` and `ObjectDetection`.\n",
    "     - Define the output of ML models: `ModelOutput` with specific interfaces for logits, probabilities, and object detections: `HasLogits`, `HasProbs`, and `HasObjectDetections`, respectively.\n",
    "\n",
    "## Getting The Most from this Notebook\n",
    "\n",
    "To maximize the benefits of this notebook, it is recommended to use an Integrated Development Environment (IDE) such as Visual Studio Code (VSCode). By using an IDE, you can take advantage of static type checking. Specifically, if you load the notebook within VSCode, the IDE will utilize Pyright within the Jupyter Notebook.\n",
    "\n",
    "This combination of VSCode and Pyright will provide many benefits, such as catching type-related errors early in the development process and helping you write more efficient and effective code. Additionally, with the use of an IDE, you will have access to a wide range of tools and features designed to make your programming experience smoother and more streamlined. Overall, using VSCode with Pyright within a Jupyter Notebook is the best way to utilize this notebook and achieve the best possible results.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Protocols](#Protocols)\n",
    "2. [Arrays](#arrays)\n",
    "3. [TypedCollction](#typed-collection)\n",
    "4. [Augmentation](#augmentation)\n",
    "5. [Model](#model)\n",
    "\n",
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no additional packages needed for this\n",
    "#!pip install jatic_toolbox\n",
    "#!pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, NamedTuple, Sequence\n",
    "from typing_extensions import Protocol, reveal_type\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocols\n",
    "\n",
    "### Motivation\n",
    "Protocols are a recent addition to the Python language, first introduced in version 3.8 (however, backports exist for version 3.6+). They are a way to define a set of behaviors that a Python object should have, without specifying a concrete class hierarchy or using mixins. A key feature of Python is its \"duck-typed\" capability, meaning that \"if it walks like a duck and quacks like a duck, then we call it a duck\".\n",
    "\n",
    "Protocols embrace this philosophy by allowing objects to be checked for compatibility based on the methods they have, rather than their inheritance. For example, instead of requiring an object to inherit from a specific class or mixin in order to be considered \"array-like\", you can define a protocol that specifies what it means for an object to be \"array-like\", such as being able to be manipulated as a Numpy array or PyTorch tensor. If an object has the methods and attributes required by the protocol, then it can be considered \"array-like\", regardless of its class hierarchy.\n",
    "\n",
    "Protocols offer several benefits over traditional inheritance-based approaches to defining behaviors in Python. They remove the need for frameworks to broadcast concrete class hierarchies and mixins, and they allow for static type checking and limited runtime type checking based on structures, not inheritance. This can lead to more flexible and maintainable code, as well as improved type checking and error catching."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Designing Interface with Concrete Types vs. Protocols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD PARTY and Mixins\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DetectMixin(ABC):\n",
    "    @ abstractmethod\n",
    "    def detect(self, x: NDArray) -> List[NDArray]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def evaluate_concrete_detector(any_detector: DetectMixin):\n",
    "    ...\n",
    "\n",
    "\n",
    "# import third_party import ThirdPartyDetector\n",
    "class ThirdPartyDetector:\n",
    "    def detect(self, x: NDArray) -> List[NDArray]:\n",
    "        ...\n",
    "\n",
    "# type-checker error: `third_party.detector` not \n",
    "#                     subclass of `DetectMixin`\n",
    "evaluate_concrete_detector(ThirdPartyDetector())  \n",
    "\n",
    "# Create class wrapper to appease interface\n",
    "\n",
    "class DetectWrapper(DetectMixin):\n",
    "    def __init__(self, third_party_detector) -> None:\n",
    "        self.detector = third_party_detector\n",
    "\n",
    "    def detect(self, x: NDArray) -> List[NDArray]:\n",
    "        return self.detector.detect(x)\n",
    "\n",
    "wrapped_detector = DetectWrapper(ThirdPartyDetector())\n",
    "\n",
    "# type-checker OK (but at what cost!)\n",
    "evaluate_concrete_detector(wrapped_detector)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportsDetection(Protocol):\n",
    "    def detect(self, x: NDArray) -> List[NDArray]:\n",
    "        ...\n",
    "\n",
    "def evaluate_detector(any_detector: SupportsDetection):\n",
    "    ...\n",
    "\n",
    "\n",
    "# import third_party import ThirdPartyDetector\n",
    "class ThirdPartyDetector:\n",
    "    def detect(self, x: NDArray) -> List[NDArray]:\n",
    "        ...\n",
    "\n",
    "# type-check OK if `third_party.detector` has \n",
    "#  `.detect(self, x: Array) -> list[Array]`\n",
    "evaluate_detector(ThirdPartyDetector())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "\n",
    "### Motivation\n",
    "\n",
    "An `ArrayLike` defines a common interface for objects that can be manipulated as arrays, regardless of the specific implementation. This allows code to be written in a more generic way, allowing it to work with different array-like objects without having to worry about the details of the specific implementation. With an `ArrayLike` protocol, vendors can write functions and algorithms that operate on arrays without JATIC defining the specific implementation of arrays to use. \n",
    "\n",
    "This will improve code readability and maintainability, as well as make it easier to switch to different array implementations if needed. For example, vendors can write functions that takes an `ArrayLike` object as input and perform some mathematical operation on the elements. This function would work with any object that satisfies the `ArrayLike` protocol, such as a numpy `ndarray`, a PyTorch `tensor`, or a custom object that implements the same methods and attributes as the `ArrayLike` protocol. In addition, an `ArrayLike` protocol is useful for providing type hints and improving code safety, as it can be used in conjunction with a static type checker to ensure that the correct types of objects are being passed as arguments. This can help catch errors before they cause problems at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch as tr\n",
    "import numpy as np\n",
    "from jatic_toolbox.protocols import ArrayLike\n",
    "\n",
    "# a function that takes ArrayLike as an input\n",
    "def f(data: ArrayLike):\n",
    "    ...\n",
    "\n",
    "# type check errors: cannot be assigned to parameter \"data\" of type \"ArrayLike\" in function \"f\"\n",
    "f(1)\n",
    "f(True) \n",
    "f(1 + 1j)\n",
    "f(dict(x=1))\n",
    "f([dict(x=1)]) \n",
    "f([\"string\"])\n",
    "f([1 + 1j])\n",
    "f((1,))\n",
    "f([True, False])\n",
    "\n",
    "class MyBadArray:\n",
    "    def not_array(self) -> List[int]: ...\n",
    "\n",
    "f(MyBadArray())\n",
    "\n",
    "# passes\n",
    "f(np.asarray([1., 2.]))\n",
    "f(np.zeros(2))\n",
    "f(tr.zeros(2))\n",
    "\n",
    "class MyArray:\n",
    "    def __array__(self) -> List[int]: ...\n",
    "\n",
    "f(MyArray())\n",
    "\n",
    "# fails for pillow array\n",
    "x: Image.Image = Image.fromarray(np.zeros((10, 10, 3)).astype(np.uint8))\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.protocols import ImageType\n",
    "\n",
    "# a function that takes ArrayLike as an input\n",
    "def g(data: ImageType):\n",
    "    ...\n",
    "\n",
    "# type check errors: cannot be assigned to parameter \"data\" of type \"ArrayLike\" in function \"f\"\n",
    "g(1)\n",
    "g(True) \n",
    "g(1 + 1j)\n",
    "g(dict(x=1))\n",
    "g([dict(x=1)]) \n",
    "g([\"string\"])\n",
    "g([1 + 1j])\n",
    "g((1,))\n",
    "g([True, False])\n",
    "\n",
    "# passes\n",
    "g(np.asarray([1., 2.]))\n",
    "g(np.zeros(2))\n",
    "g(tr.zeros(2))\n",
    "\n",
    "# passes for pillow array\n",
    "x: Image.Image = Image.fromarray(np.zeros((10, 10, 3)).astype(np.uint8))\n",
    "g(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.protocols import ShapedArray\n",
    "\n",
    "# a function that takes ArrayLike as an input\n",
    "def h(data: ShapedArray):\n",
    "    ...\n",
    "\n",
    "# type check errors: cannot be assigned to parameter \"data\" of type \"ArrayLike\" in function \"f\"\n",
    "h(1)\n",
    "h(True) \n",
    "h(1 + 1j)\n",
    "h(dict(x=1))\n",
    "h([dict(x=1)]) \n",
    "h(\"string\")  \n",
    "h((1,))\n",
    "h([1., 2.])\n",
    "h([True, False])\n",
    "h([1 + 1j])\n",
    "\n",
    "class HasShape:\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int,...]: ...\n",
    "\n",
    "h(HasShape())\n",
    "\n",
    "# passing\n",
    "f(np.zeros(2))\n",
    "\n",
    "class MyArray:\n",
    "    def __array__(self) -> List[int]: ...\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int,...]: ...\n",
    "\n",
    "h(MyArray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typed Collection\n",
    "\n",
    "A `TypedCollection` is a homogeneous collection of Python objects that can be used to define a consistent type in an interface. For example, if the inputs and outputs of the interface are expected to be a NumPy `NDArray` or PyTorch `Tensor`, a `TypedCollection` type can be used to define this consistency.\n",
    "\n",
    "Consider the following example where the function `foo` only accepts inputs as individual float values:\n",
    "\n",
    "```python\n",
    "def foo(*inputs: float) -> Tuple[float, ...]\n",
    "   ...\n",
    "```\n",
    "\n",
    "However, what if we want the function to accept inputs in the form of a list or a dictionary of float values? This can be achieved by defining a `TypedCollection` type alias as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "TypedCollection: TypeAlias = Union[T, Sequence[T], Mapping[Any, T], Mapping[Any, Sequence[T]]]\n",
    "```\n",
    "\n",
    "With this definition, we can redefine the function `foo` to accept inputs in a nested collection form:\n",
    "\n",
    "```python\n",
    "def foo_nested(*inputs: TypedCollection[float]) -> TypedCollection[float]\n",
    "   ...\n",
    "```\n",
    "\n",
    "Now, the function `foo_nested` can accept inputs in the form of individual float values, a list of float values, or a dictionary with float values as its values. This makes the interface more flexible and easier to use. For example, calling `foo(1.0, [1.0, 2.0])` would fail type checking, whereas `foo_nested(1.0, [1.0, 2.0])` would pass.  See more examples below for passing and failing interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.protocols import TypedCollection\n",
    "\n",
    "# on it's own it's just a simple type\n",
    "def f(data: TypedCollection[float]):\n",
    "    ...\n",
    "\n",
    "# failing type check\n",
    "f(\"string\")\n",
    "f([\"string\"])\n",
    "f((\"string\",))\n",
    "f(dict(x=\"string\"))\n",
    "\n",
    "f(1+1j)\n",
    "f([1+1j])\n",
    "f((1+1j,))\n",
    "f(dict(x=1+1j))\n",
    "\n",
    "# passing\n",
    "f(1.0)\n",
    "f([1.0])\n",
    "f((1.0,))\n",
    "f(dict(x=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(*inputs: TypedCollection[float]):\n",
    "    ...      \n",
    "\n",
    "# failing type check\n",
    "f(1.0, \"string\")\n",
    "f(1.0, [\"string\"])\n",
    "f([1.0], (\"string\",))\n",
    "f(1.0, dict(x=\"string\"))\n",
    "\n",
    "# passing\n",
    "f(1.0, 1.0)\n",
    "f([1.0], dict(x=1.0))\n",
    "f((1.0,), [1.0, 2.0], 1.0)\n",
    "f(dict(x=1.0))\n",
    "f(dict(x=[1., 2.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use Case with `TypedCollection`\n",
    "\n",
    "In this explanation, how to use `TypedCollection` with PyTrees to support variable number of \n",
    "arguments to a python function (variadic arguments). There are multiple variations of this concept, including:\n",
    "\n",
    "- [PyTorch](https://github.com/pytorch/pytorch/blob/master/torch/utils/_pytree.py) \n",
    "- [JAX PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html)\n",
    "- [Optimized PyTrees](https://github.com/metaopt/optree)\n",
    "\n",
    "For the purpose of this explanation, we will be focusing on PyTorch's implementation of `PyTree`. It's worth noting that none of these implementations require special tensors or containers to function and are strictly python only. The two functions that are most relevant to this example are `torch.utils._pytree.tree_flatten` and `torch.utils._pytree.tree_unflatten`. For demonstration purposes, we'll be using simple lists in the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils._pytree import tree_flatten, tree_unflatten\n",
    "\n",
    "input1: TypedCollection[int] = [1, 2, 3]\n",
    "input2: TypedCollection[int] = (4, 5, 6)\n",
    "input3: TypedCollection[int] = dict(a=[7,8,9])\n",
    "\n",
    "\n",
    "flat_inputs, tree_spec = tree_flatten((input1, input2, input3))\n",
    "# flat_inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "flat_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11, 12, 13], (14, 15, 16), {'a': [17, 18, 19]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 10 to each value\n",
    "augment = [x + 10 for x in flat_inputs]\n",
    "\n",
    "# move back to original\n",
    "aug_inputs = tree_unflatten(augment, tree_spec)\n",
    "# aug_inputs = ([11, 12, 13], (14, 15, 16), {\"a\": [17, 18, 19])\n",
    "aug_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put it all together using a simple interface example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(*inputs: TypedCollection[int]) -> Tuple[TypedCollection[int], ...]:\n",
    "    flat_inputs, tree_spec = tree_flatten(inputs)\n",
    "    augment = [x + 10 for x in flat_inputs]\n",
    "    return tree_unflatten(augment, tree_spec)\n",
    "\n",
    "aug_input1, aug_input2, aug_input3 = foo(input1, input2, input3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets demonstrate wiht NumPy arrays instead of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2, 2))\n",
    "y = np.ones((2, 2))\n",
    "\n",
    "def foo_numpy(*inputs: TypedCollection[np.ndarray]) -> Tuple[TypedCollection[np.ndarray], ...]:\n",
    "    flat_inputs, tree_spec = tree_flatten(inputs)\n",
    "    augment = [x + 10 for x in flat_inputs]\n",
    "    return tree_unflatten(augment, tree_spec)\n",
    "\n",
    "aug_input1, aug_input2 = foo_numpy(x, dict(a=y))\n",
    "\n",
    "\n",
    "# flat_inputs\n",
    "# [array([[0., 0.],\n",
    "#        [0., 0.]]),\n",
    "# array([[1., 1.],\n",
    "#        [1., 1.]])]\n",
    "\n",
    "# aug_inputs\n",
    "# (array([[10., 10.],\n",
    "#        [10., 10.]]),\n",
    "# {'a': array([[11., 11.],\n",
    "#             [11., 11.]])})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be evident how this concept is beneficial for common ML tasks in object detection and segmentation, where the same augmentations are often required to be applied to collections of input and target variables. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "A data augmentation interface should allow for applying transformations to a uniform set of objects with a shared type, such as PyTorch Tensors. This provides a versatile API to perform a variety of augmentations on different types of data, including images, sequences, videos, bounding boxes, and segmentation masks. The flexibility of the interface makes it easy to use different types of data and modify the processing pipeline as needed, giving developers and users more control over the data processing pipeline. The interface eliminates the need to explicitly define inputs, allowing for customization of transformations across different combinations of collections, such as images and bounding boxes.\n",
    "\n",
    "The data augmentation interface should also provide a clear way to set the random number generator (RNG) state, without relying on a global random state. This allows for greater control over the RNG and more reproducible results, as described in a [NumPy post on RNG best practices](https://albertcthomas.github.io/good-practices-random-number-generators/). By explicitly supporting this feature, the augmentation pipelines will be isolated from global entropy, ensuring that results are consistent and can be easily reproduced, especially during testing and evaluation.\n",
    "\n",
    "\n",
    "### Protocol Interface Overview\n",
    "\n",
    "The Augmentation protocol is defined using the 'TypedCollection' as follows:\n",
    "\n",
    "```python\n",
    "class Augmentation(Protocol[T]):\n",
    "    def __call__(\n",
    "        self, *inputs: TypedCollection[T], rng: Optional[RandomStates] = None,\n",
    "    ) -> Union[TypedCollection[T], Tuple[TypedCollection[T], ...]]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "This protocol allows for a variety of augmentations, including:\n",
    "\n",
    "```python\n",
    "def aug1(data: Tensor) -> Tensor:\n",
    "    # Augment a single data tensor and return the augmented tensor.\n",
    "    ...\n",
    "\n",
    "def aug2(data: Tensor, boxes: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    # Augment data and boxes and output augmented data and boxes.\n",
    "    ...\n",
    "\n",
    "def aug2(data: Sequence[Tensor], mapped_data: Dict[str, Tensor]) -> Tuple[Sequence[Tensor], Dict[str, Tensor]]:\n",
    "    # Augment data that is a sequence (e.g, video) and a dictionary of tensor values.\n",
    "    # The return value has the data and mapped_data augmented.\n",
    "    ...\n",
    "```\n",
    "\n",
    "Note that due to limitations in handling variadic arguments for Python type hinting, none of the above functions will satisfy the `Augmentation` protocol. Instead, the protocol can only be satisfied by defining an interface using variadic arguments. An example of this is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from jatic_toolbox.protocols import Augmentation\n",
    "\n",
    "def auger(x: Augmentation[np.ndarray]):\n",
    "    ...\n",
    "\n",
    "\n",
    "def A(*inputs: TypedCollection[np.ndarray], rng: Optional[int] = None) -> TypedCollection[np.ndarray]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def AA(*inputs: TypedCollection[np.ndarray]) -> TypedCollection[np.ndarray]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def B(data: np.ndarray, boxes: np.ndarray, rng: Optional[int] = None) -> TypedCollection[np.ndarray]:\n",
    "    ...\n",
    "\n",
    "\n",
    "# passes pyright\n",
    "auger(A)\n",
    "\n",
    "\n",
    "# fails pyright\n",
    "auger(AA)\n",
    "auger(B)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Based on TorchVision V2 Prototype Interface\n",
    "\n",
    "The torchvision library has recently developed a new prototype interface for its transforms, which can be found in the torchvision/prototype directory on GitHub and is described in a recent blog post ([source](https://github.com/pytorch/vision/tree/main/torchvision/prototype), [blog post](https://pytorch.org/blog/extending-torchvisions-transforms-to-object-detection-segmentation-and-video-tasks/)). This new interface represents a major advancement by supporting transforms not only on images but also on videos, bounding boxes, labels, and segmentation masks. It accomplishes this by introducing a Datapoint class, a subtype of Tensor, to describe various types of data. The interface also provides a flexible API for mapping transformations over collections of tensors, making it easier to incorporate various features in each transformation. The Transform class provides a simple API for implementing transforms, allowing developers to draw random parameters and transform individual tensors or features. Additionally, the interface continues to support batches of tensors and features, as well as GPU support.\n",
    "\n",
    "The prototype transform library has not been officially released so we do not contain example interfaces to support `torchvision` V2 in the `jatic_toolbox` yet.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Augmentation Implementaion\n",
    "\n",
    "Let's consider a more concreate `Augmentation` example of implementing and applying a `RandomCrop` augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runtime type is 'ndarray'\n",
      "Runtime type is 'dict'\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Tuple\n",
    "from numpy.random import Generator, default_rng\n",
    "from torch.utils._pytree import tree_flatten, tree_unflatten\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomCrop(Augmentation):\n",
    "    def __init__(self, size: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Randomly crop the last two dimension of an array (e.g., height and width of image).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : Tuple[int, int]\n",
    "            The desired dimensions of the last two dimensions of the array.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = size\n",
    "\n",
    "    def _get_params(\n",
    "        self, flat_inputs: List[ArrayLike], rng: Generator\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Calculate the parameters of the random crop to be applied for all inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        flat_inputs : List[ArrayLike]\n",
    "            A set of inputs\n",
    "\n",
    "        rng: Generator\n",
    "        \"\"\"\n",
    "        assert len(flat_inputs) > 0\n",
    "        h, w = np.asarray(flat_inputs[0]).shape\n",
    "        th, tw = self.output_size\n",
    "\n",
    "        dw = 0\n",
    "        if tw < w:\n",
    "            dw = rng.integers(0, w - tw)\n",
    "\n",
    "        dh = 0\n",
    "        if th < h:\n",
    "            dh = rng.integers(0, h - th)\n",
    "\n",
    "        return dict(bottom=dh, top=dh + th, left=dw, right=dw + tw)\n",
    "\n",
    "    def _transform(self, inpt: np.ndarray, params: Dict[str, int]) -> np.ndarray:\n",
    "        \"\"\"Apply the augmentation.\"\"\"\n",
    "        return inpt[\n",
    "            ..., params[\"bottom\"] : params[\"top\"], params[\"left\"] : params[\"right\"]\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "        self, *inputs: TypedCollection[ArrayLike], rng: Optional[Any] = None\n",
    "    ) -> Tuple[TypedCollection[np.ndarray], ...]:\n",
    "        if rng is None:\n",
    "            rng = default_rng()\n",
    "\n",
    "        # flatten the inputs\n",
    "        flat_inputs, spec = tree_flatten(inputs)\n",
    "\n",
    "        # calculate the parameters for cropping all inputs\n",
    "        params = self._get_params(flat_inputs, rng=rng)\n",
    "\n",
    "        # apply the augmentation\n",
    "        flat_outputs = [self._transform(inpt, params) for inpt in flat_inputs]\n",
    "\n",
    "        # return augmented objects in original format\n",
    "        return tree_unflatten(flat_outputs, spec)\n",
    "\n",
    "\n",
    "# this should pass pyright\n",
    "cropper = RandomCrop((2, 1))\n",
    "\n",
    "a, b = cropper(np.zeros((10, 10)), dict(val=[np.ones((10, 10))]))\n",
    "reveal_type(a)  # prints: Runtime type is 'ndarray'\n",
    "reveal_type(b)  # prints: Runtime type is 'dict'\n",
    "\n",
    "auger(cropper)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few examples of executing `RandomCrop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (2, 1)\n",
      "y: (1, 2, 1)\n",
      "z: (1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "random_crop = RandomCrop((2, 1))\n",
    "\n",
    "# Example 1: Simple Array\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "xout, = random_crop(x, rng=default_rng(0))\n",
    "print(\"x:\", xout.shape)\n",
    "# prints \"x: (2, 1)\"\n",
    "\n",
    "# Example 2: Multiple Arrays\n",
    "y = np.arange(100, 120).reshape(1, 5, 4)\n",
    "xout, yout = random_crop(x, y, rng=default_rng(0))\n",
    "print(\"y:\", yout.shape)\n",
    "# prints \"y: (1, 2, 1)\"\n",
    "\n",
    "# Example 2: Multiple Arrays\n",
    "xy, = random_crop([x, y], rng=default_rng(0))\n",
    "\n",
    "# Example 3: List of Array, Tuple, and Dict\n",
    "z = dict(val=np.arange(1000, 1032).reshape(1, 2, 4, 4))\n",
    "xyz, = random_crop([x, y, z], rng=default_rng(0))\n",
    "print(\"z:\", xyz[2][\"val\"].shape)\n",
    "# prints \"z: (1, 2, 2, 1)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "TODO: Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jatic_toolbox.protocols import Classifier, ObjectDetector\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class LogitsOutput(NamedTuple):\n",
    "    logits: Tensor\n",
    "\n",
    "\n",
    "class ProbsOutput(NamedTuple):\n",
    "    probs: Tensor\n",
    "\n",
    "\n",
    "class BadClassifierOutput(NamedTuple):\n",
    "    outputs: ArrayLike\n",
    "\n",
    "\n",
    "class Detections(NamedTuple):\n",
    "    scores: Sequence[Sequence[dict]]\n",
    "    boxes: Tensor\n",
    "    foo: int\n",
    "\n",
    "\n",
    "class BadDetections(NamedTuple):\n",
    "    boxes: ArrayLike\n",
    "\n",
    "\n",
    "def classifier(data: TypedCollection[Tensor]) -> LogitsOutput:\n",
    "    ...\n",
    "\n",
    "\n",
    "def classifier2(data: TypedCollection[Tensor]) -> ProbsOutput:\n",
    "    ...\n",
    "\n",
    "\n",
    "def bad_output_classifier(data: ArrayLike) -> BadClassifierOutput:\n",
    "    ...\n",
    "\n",
    "\n",
    "def bad_input_classifier(data: Sequence[int]) -> BadClassifierOutput:\n",
    "    ...\n",
    "\n",
    "\n",
    "def detector(data: TypedCollection[Tensor]) -> Detections:\n",
    "    ...\n",
    "\n",
    "\n",
    "def bad_output_detector(data: TypedCollection[Tensor]) -> BadDetections:\n",
    "    ...\n",
    "\n",
    "\n",
    "#####################\n",
    "# Check function call\n",
    "#####################\n",
    "\n",
    "# passes\n",
    "classifier(np.zeros(2))\n",
    "classifier(dict(x=np.zeros(2)))\n",
    "detector(np.zeros(2))\n",
    "detector(dict(x=np.zeros(2)))\n",
    "\n",
    "# fails\n",
    "classifier([1, 2])\n",
    "detector([1, 2])\n",
    "classifier(\"hi\")\n",
    "detector(True)\n",
    "\n",
    "\n",
    "#################\n",
    "# Check interface\n",
    "#################\n",
    "\n",
    "\n",
    "# test functions\n",
    "def eval_classifier(f: Classifier[Tensor]):\n",
    "    ...\n",
    "\n",
    "\n",
    "def eval_detector(f: ObjectDetector[Tensor]):\n",
    "    ...\n",
    "\n",
    "\n",
    "# passes\n",
    "eval_classifier(classifier)\n",
    "eval_classifier(classifier2)\n",
    "eval_detector(detector)\n",
    "\n",
    "\n",
    "# does not pass\n",
    "eval_classifier(bad_output_classifier)\n",
    "eval_classifier(bad_input_classifier)\n",
    "eval_classifier(detector)\n",
    "eval_detector(bad_output_detector)\n",
    "eval_detector(classifier)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jatic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ff0748c4294df5fbc1d62e8785ea3f46b8f1eb5cb1e561b8691238609bfac4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
