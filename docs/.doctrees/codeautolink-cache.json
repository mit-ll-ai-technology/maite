{
  "api_reference": [],
  "changes": [],
  "explanation": [],
  "explanation/maite_vision": [],
  "explanation/protocol_overview": [
    {
      "source": "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "import numpy as np\nimport torch\n\nfrom maite.protocols import ArrayLike\n\n\ndef my_numpy_fn(x: ArrayLike) -> np.ndarray:\n    arr = np.asarray(x)\n    # ...\n    return arr\n\n\ndef my_torch_fn(x: ArrayLike) -> torch.Tensor:\n    tensor = torch.as_tensor(x)\n    # ...\n    return tensor\n\n\n# can apply NumPy function to PyTorch Tensor\nnp_out = my_numpy_fn(torch.rand(2, 3))\n\n# can apply PyTorch function to NumPy array\ntorch_out = my_torch_fn(np.random.rand(2, 3))\n\n# note: no performance hit from conversion when all `ArrayLike`s are from same library\n# or when can share the same underlying memory\ntorch_out = my_torch_fn(torch.rand(2, 3))",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "asarray"
          ],
          "code_str": "np.asarray",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "numpy.asarray"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "random",
            "rand"
          ],
          "code_str": "np.random.rand",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "numpy.random.rand"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "concept-bridging-arraylikes",
        "headings": [
          "Overview of MAITE Protocols",
          "1 Concept: Bridging ArrayLikes"
        ]
      },
      "doc_lineno": 37
    },
    {
      "source": "# define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)\nclass DatumMetadataType(TypedDict):\n    id: str|int\n\nInputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\nTargetType: TypeAlias = ArrayLike  # shape-(Cl) tensor of one-hot encoded true class or predicted probabilities",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 97
    },
    {
      "source": "# fmt: off\n\n# import protocol classes\nfrom maite.protocols.image_classification import (  # noqa: I001\n    Dataset,                                        # noqa: F401\n    DataLoader,                                     # noqa: F401\n    Model,                                          # noqa: F401\n    Augmentation,                                   # noqa: F401\n    Metric                                          # noqa: F401\n)\n\n# import type aliases\nfrom maite.protocols.image_classification import (\n    InputType,                                      # noqa: F401\n    TargetType,                                     # noqa: F401\n    DatumMetadataType,                              # noqa: F401\n)\n\n# fmt: on",
      "names": [
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset"
          ],
          "code_str": "Dataset",
          "lineno": 4,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "DataLoader"
          ],
          "code_str": "DataLoader",
          "lineno": 4,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.DataLoader"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Model"
          ],
          "code_str": "Model",
          "lineno": 4,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Model"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation"
          ],
          "code_str": "Augmentation",
          "lineno": 4,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Metric"
          ],
          "code_str": "Metric",
          "lineno": 4,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Metric"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 123
    },
    {
      "source": "import maite.protocols.image_classification as ic  # noqa: I001\n\n# model: ic.Model = load_model(...)",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 148
    },
    {
      "source": "# define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)\nclass DatumMetadataType(TypedDict):\n    id: str|int\n\nclass ObjectDetectionTarget(Protocol):\n    @property\n    def boxes(self) -> ArrayLike: ...  # shape-(D_i, 4) tensor of bounding boxes w/format X0, Y0, X1, Y1\n\n    @property\n    def labels(self) -> ArrayLike: ... # shape-(D_i) tensor of labels for each box\n\n    @property\n    def scores(self) -> ArrayLike: ... # shape-(D_i) tensor of scores for each box or\n                                       # shape-(D_i, Cl) tensor of scores for each box/class (scores may be either probabilities or logits)\n\nInputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\nTargetType: TypeAlias = ObjectDetectionTarget",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "property"
          ],
          "code_str": "property",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "property"
        },
        {
          "import_components": [
            "property"
          ],
          "code_str": "property",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "property"
        },
        {
          "import_components": [
            "property"
          ],
          "code_str": "property",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "property"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "object-detection",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.2 Object Detection"
        ]
      },
      "doc_lineno": 160
    },
    {
      "source": "print(ic.Model.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "models",
        "headings": [
          "Overview of MAITE Protocols",
          "3 Models"
        ]
      },
      "doc_lineno": 194
    },
    {
      "source": "import maite.protocols.object_detection as od\n\nprint(od.Model.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "models",
        "headings": [
          "Overview of MAITE Protocols",
          "3 Models"
        ]
      },
      "doc_lineno": 297
    },
    {
      "source": "print(ic.Dataset.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 404
    },
    {
      "source": "print(ic.DataLoader.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 516
    },
    {
      "source": "print(od.DataLoader.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 547
    },
    {
      "source": "print(ic.Augmentation.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "augmentations",
        "headings": [
          "Overview of MAITE Protocols",
          "5 Augmentations"
        ]
      },
      "doc_lineno": 592
    },
    {
      "source": "print(od.Augmentation.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "augmentations",
        "headings": [
          "Overview of MAITE Protocols",
          "5 Augmentations"
        ]
      },
      "doc_lineno": 671
    },
    {
      "source": "print(ic.Metric.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "metrics",
        "headings": [
          "Overview of MAITE Protocols",
          "6 Metrics"
        ]
      },
      "doc_lineno": 812
    },
    {
      "source": "print(od.Metric.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "metrics",
        "headings": [
          "Overview of MAITE Protocols",
          "6 Metrics"
        ]
      },
      "doc_lineno": 928
    },
    {
      "source": "from maite.tasks import evaluate, predict\n# we can also import from object_detection module\n# where the function call signature is the same",
      "names": [
        {
          "import_components": [
            "maite",
            "tasks",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.tasks.evaluate"
        },
        {
          "import_components": [
            "maite",
            "tasks",
            "predict"
          ],
          "code_str": "predict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.tasks.predict"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "procedures",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Procedures"
        ]
      },
      "doc_lineno": 1104
    },
    {
      "source": "print(evaluate.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "procedures",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Procedures"
        ]
      },
      "doc_lineno": 1110
    },
    {
      "source": "print(predict.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "procedures",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Procedures"
        ]
      },
      "doc_lineno": 1175
    },
    {
      "source": "Make predictions for a given model & data source with optional augmentation.\n\nParameters\n----------\nmodel : SomeModel\n    Maite Model object.\n\ndataloader : SomeDataloader | None, (default=None)\n    Compatible maite dataloader.\n\ndataset : SomeDataset | None, (default=None)\n    Compatible maite dataset.\n\nbatch_size : int, (default=1)\n    Batch size for use with dataset (ignored if dataset=None).\n\naugmentation : SomeAugmentation | None, (default=None)\n    Compatible maite augmentation.\n\nreturn_augmented_data : bool, (default=False)\n    Set to True to return post-augmentation data as a function output.\n\nReturns\n-------\ntuple[Sequence[Sequence[SomeTargetType], Sequence[tuple[Sequence[SomeInputType], Sequence[SomeTargetType], Sequence[SomeMetadataType]]],\n    A tuple of the predictions (as a sequence of batches) and a sequence\n    of tuples containing the information associated with each batch.\n    Note that the second return argument will be empty if\n    return_augmented_data is False.\n\nRaises\n------\nValueError\n    If neither a dataloader nor a dataset is provided.",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "procedures",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Procedures"
        ]
      },
      "doc_lineno": 1183
    }
  ],
  "explanation/type_hints_for_API_design": [
    {
      "source": "def count_vowels(x: str) -> int:  # `: str` and `-> int` are the annotations\n    return sum(1 for char in x if char in set(\"aeiouAEIOU\"))\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "sum"
          ],
          "code_str": "sum",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "sum"
        },
        {
          "import_components": [
            "set"
          ],
          "code_str": "set",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "set"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "a-quick-introduction-to-writing-statically-typed-python-code",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "A quick introduction to writing statically typed Python code"
        ]
      },
      "doc_lineno": 24
    },
    {
      "source": "from typing import Callable, List, Mapping, Optional, TypeVar, Union, Protocol, Literal\n\n# The following are type annotations that could be included, e.g., in a\n# function' signature\n\n# Either an integer or a float\nUnion[int, float]\n\n# Either a boolean or `None`\nOptional[bool]\n\n# Either the string 'cat' or the string 'dog'\nLiteral[\"cat\", \"dog\"]\n\n# Any object supporting the call syntax \u2013 f(...) \u2013 that accepts three\n# integer-values inputs and returns a string\nCallable[[int, int, int], str]\n\n# A list of an arbitrary number of strings\nList[str]\n\n# A mapping, such as a dictionary, whose keys are strings\n# and whose values are floats\nMapping[str, float]\n\n# A type named Person that has two attributes: `name` (str) and `age` (int)\n# and a \"greeting\" method, which accepts no inputs and returns a string\nclass Person:\n    name: str\n    age: int\n\n    def greeting(self) -> str:\n      ...\n\n# A function that accepts a single input and returns an output of the\n# same type\nT = TypeVar(\"T\")\ndef type_preserving_func(x: T) -> T:\n  ...\n\n# The following describes *any* type that exposes the method: \n# `<obj>.open(file_path: str)`\n# Note: this is a protocol, which enables a feature known as \"structural subtyping\".\n# This will be an important feature that we discuss later\nclass Openable(Protocol):\n    def open(self, file_path: str):\n        ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "List",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "typing",
            "Mapping"
          ],
          "code_str": "Mapping",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Mapping"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "typing",
            "TypeVar"
          ],
          "code_str": "TypeVar",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "Union"
          ],
          "code_str": "Union",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Union"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Literal"
        },
        {
          "import_components": [
            "typing",
            "Union"
          ],
          "code_str": "Union",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Union"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "bool"
          ],
          "code_str": "bool",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "bool"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "typing.Literal"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "List",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "Mapping"
          ],
          "code_str": "Mapping",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "typing.Mapping"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 32,
          "end_lineno": 32,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "TypeVar"
          ],
          "code_str": "TypeVar",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 38,
          "end_lineno": 38,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 38,
          "end_lineno": 38,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 45,
          "end_lineno": 45,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 46,
          "end_lineno": 46,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "a-quick-introduction-to-writing-statically-typed-python-code",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "A quick introduction to writing statically typed Python code"
        ]
      },
      "doc_lineno": 36
    },
    {
      "source": "# contents of example.py\nfrom typing import Iterable\n\ndef get_data_registry() -> dict[str, int]:\n    ...\n\ndef process_data(x: Iterable[int]) -> int:\n    data_total = sum(x)\n    return data_total\n\ndef run_app():\n    registry = get_data_registry()\n    process_data(registry.keys())  # <-- static type checker flags error here!\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "sum"
          ],
          "code_str": "sum",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "sum"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "static-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Static Type Checkers"
        ]
      },
      "doc_lineno": 96
    },
    {
      "source": "# Demonstrating pyright's ability to infer types through un-annotated functions\ndef make_int() -> int: ...\ndef add(x, y): return x + y  # note: not annotated!\n\nx, y = make_int(), make_int()\nz = add(x, y)\n\nreveal_type(z)  # pyright reveals: int, mypy reveals: Any\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "static-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Static Type Checkers"
        ]
      },
      "doc_lineno": 132
    },
    {
      "source": "from beartype import beartype\n\n@beartype\ndef process_age(age: int) -> int:\n    return age\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "runtime-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Runtime type checkers"
        ]
      },
      "doc_lineno": 154
    },
    {
      "source": ">>> process_age(\"hello\")\nBeartypeCallHintParamViolation: @beartyped process_age() parameter x='hello' violates type hint class 'int', as 'hello' not instance of int.\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "runtime-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Runtime type checkers"
        ]
      },
      "doc_lineno": 163
    },
    {
      "source": "# before PEP 585\nimport typing\n\ndef f(ages: typing.List[int], records: typing.Dict[str, int]): ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "typing.List",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "typing.Dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 253
    },
    {
      "source": "# after PEP 585\nfrom __future__ import annotations  # required for Python < 3.9\n\ndef f(ages: list[int], records: dict[str, int]): ...\n",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "__future__",
            "annotations"
          ],
          "code_str": "annotations",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "__future__.annotations"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 259
    },
    {
      "source": "# before PEP 604\ndef f(x: typing.Union[int, str]): ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 268
    },
    {
      "source": "# after PEP 604\nfrom __future__ import annotations  # required for Python < 3.10\n\ndef f(x: int | str): ...\n",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "__future__",
            "annotations"
          ],
          "code_str": "annotations",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "__future__.annotations"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 272
    },
    {
      "source": "from typing import Sequence\nfrom typing_extensions import TypeAlias\nimport torch as tr\n\nScalars: TypeAlias = int | float | complex\n# Supports array-likes from 0D to 2D structures\nArrayLike: TypeAlias = Scalars | Sequence[Scalars] | Sequence[Sequence[Scalars]] \n\ndef to_tensor(x: ArrayLike) -> tr.Tensor:\n    ...\n\nto_tensor(0)  # static type checker: OK\nto_tensor([1, []])  # static type checker: ERROR!\nto_tensor([1, 1])  # static type checker: OK\nto_tensor([[2+1j, 3+0j], [1-8j, 2+10j]])  # static type checker: OK\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "complex"
          ],
          "code_str": "complex",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "complex"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 281
    },
    {
      "source": "from typing_extensions import TypeVarTuple, Unpack, TypeAlias\nfrom typing import Generic, Any\nimport torch\nfrom typing import NewType\n\nShape = TypeVarTuple(\"Shape\")\n\n# A PyTorch tensor with additional shape type information\n# This is a so-called \"variadic generic\": the Shape type variable can vary in length/contents\nclass Tensor(Generic[Unpack[Shape]], torch.Tensor):\n    ...\n\n# Declaring descriptive aliases for common array dimensions\nHeight: TypeAlias = int\nWidth: TypeAlias = int\nChannel: TypeAlias = int\nTime: TypeAlias = int\nBatch: TypeAlias = int\n\n# Some representative utility functions for loading tensor data\ndef load_time_series(path: str) -> Tensor[Time]: ...\ndef load_image(path: str) -> Tensor[Channel, Height, Width]: ...\ndef load_video(path: str) -> Tensor[Time, Channel, Height, Width]: ...\n\n# Some functions working with tensors..\n# Stack multiple Tensors along a leading \"Batch\" dimension\ndef stack(*arrs: Tensor[Unpack[Shape]]) -> Tensor[Batch, Unpack[Shape]]: ...\n\n# Get the resolution, HxW, from any shape-(..., H, W) tensor\ndef get_img_resolution(img: Tensor[Unpack[tuple[Any, ...]], Height, Width]) -> tuple[Height, Width]: ...\n\nlist_of_images = [load_image(p) for p in [\"a.png\", \"b.png\"]]  # list[Tensor[Channel, Height, Width]]\nimg_tensor = stack(*list_of_images)  # Tensor[Batch, Channel, Height, Width]\nimg_res = get_img_resolution(img_tensor)  # Tuple[Height, Width]\n\nlist_of_videos = [load_video(p) for p in [\"a.mp4\", \"b.mp4\"]]  # list[Tensor[Time, Channel, Height, Width]]\nvideo_tensor = stack(*list_of_videos)  # Tensor[Batch, Time, Channel, Height, Width]\nvideo_res = get_img_resolution(video_tensor)  # Tuple[Height, Width]\n\ntime_series = load_time_series(\"data.pkl\")  # Tensor[Time]\n# attempting to get the resolution of a shape-(Time,) tensor...\nget_img_resolution(time_series)  # static type check: error!\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Generic"
          ],
          "code_str": "Generic",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Generic"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "NewType"
          ],
          "code_str": "NewType",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.NewType"
        },
        {
          "import_components": [
            "typing",
            "Generic"
          ],
          "code_str": "Generic",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "typing.Generic"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "typing.Any"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 307
    },
    {
      "source": "img_tensor = load_image(\"img.png\")  # type-checker sees: Tensor[Channel, Height, Width]\nimg_tensor = img_tensor * 2  # type-checker sees: Tensor\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 356
    },
    {
      "source": "import abc\nfrom typing import Sequence, Any, Dict, Tuple\nfrom typing_extensions import TypeAlias\nfrom torch import Tensor\n\nfrom our_library import BoundingBox\n\nClassScores: TypeAlias = Dict[Any, float]\n\nclass OurDetectorAPI(abc.ABC):\n    @abc.abstractmethod\n    def detect(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]: \n        raise NotImplemented() \n",
      "names": [
        {
          "import_components": [
            "abc"
          ],
          "code_str": "abc",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "abc"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "abc",
            "ABC"
          ],
          "code_str": "abc.ABC",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "abc.ABC"
        },
        {
          "import_components": [
            "abc",
            "abstractmethod"
          ],
          "code_str": "abc.abstractmethod",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "abc.abstractmethod"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "NotImplemented"
          ],
          "code_str": "NotImplemented",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "NotImplemented"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 376
    },
    {
      "source": "class BoundingBox:\n    def __init__(self, left: float, top: float, right: float, bottom: float):\n        # check that bbox coords satisfy, e.g., left <= right\n        # use bbox coords to construct vertices\n        ...\n    def compute_box_area(self) -> float: ...\n    def get_intersection(self, other_box: \"BoundingBox\") -> \"BoundingBox\": ...\n",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "float"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 394
    },
    {
      "source": "def measure_detector_precision_and_recall(model: OurDetectorAPI) -> float:\n    if not isinstance(model, OurDetectorAPI):\n        raise TypeError(\"You've gotta be one of us!\")\n    \n    data = load_data()\n    detections = model.detect(data)\n    ...\n",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 406
    },
    {
      "source": "from our_library import BoundingBox, OurDetectorAPI\nfrom their_library import TheirDetector\n\nclass SadCompatShim(OurDetectorAPI):\n    def __init__(self, actual_detector: TheirDetector):\n        self.det = actual_detector\n\n    # this is the best-case scenario\n    def detect(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]:\n        their_bboxes, their_scores = self.det.their_detection_method(img)\n        our_bboxes = [BoundingBox(*bbox) for bbox in their_bboxes]\n        return list(zip(our_bboxes, their_scores))\n",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "zip"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 427
    },
    {
      "source": "from typing import Any, Dict, Sequence, Tuple, Protocol, runtime_checkable\n\nfrom torch import Tensor\nfrom typing_extensions import TypeAlias, runtime_checkable\n\nClassScores: TypeAlias = Dict[Any, float]\n\n@runtime_checkable\nclass BoundingBox(Protocol):\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n@runtime_checkable  # <-- enables `isinstance` checks to look for necessary structure [1]\nclass OurDetectorAPI(Protocol):\n    def __call__(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]:\n        ...\n\ndef measure_detector_precision_and_recall(model: OurDetectorAPI) -> float:\n    if not isinstance(model, OurDetectorAPI):  # <-- [1]: I.e, this still works!\n        raise TypeError(\"You've gotta be one of us!\")\n    \n    data = load_data()\n    detections = model(data)\n    ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "runtime_checkable"
          ],
          "code_str": "runtime_checkable",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.runtime_checkable"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 455
    },
    {
      "source": "# Implementing an \"empty\" detector in the old API\nfrom our_library import OurDetectorAPI  # <- our library must be installed\n\nclass EmptyDetector(OurDetectorAPI):\n    def detect(self, img):\n        return []\n\nempty_detector = EmptyDetector()\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 493
    },
    {
      "source": "# Implementing an \"empty\" detector in the new API\nempty_detector = lambda img: []\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 506
    },
    {
      "source": "@runtime_checkable\nclass Configurable(Protocol):\n    def ___special_config_interface__(self) -> dict[str, Any]: ...\n\nclass ConfigurableDetector(OurDetectorAPI, Configurable, Protocol): ...\n\ndef orchestrate_detector(model: ConfigurableDetector): ...\n",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 518
    },
    {
      "source": "empty_detector = lambda x: []\norchestrate_detector(empty_detector)  # static type checker: error\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 530
    },
    {
      "source": "from torch import Tensor\nimport torch.nn as nn\nfrom typing import Iterable\n\ndef load_data() -> Tensor: ...\ndef load_model() -> nn.Module: ...\n\ndef measure_data_distr(img_batch: Tensor):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"Not image batch-like\")\n    # <actual functionality here>\n\ndef compute_accuracy(img_batch: Tensor, model: nn.Module):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"not batch-like\")\n    # <actual functionality here>\n\ndef compute_calibration(img_batch: Tensor, models: Iterable[nn.Module]):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"Not image batch-like\")\n    # <actual functionality here>\n\nif __name__ == \"__main__\":\n    tensor = load_data()\n    model = load_model()\n\n    measure_data_distr(tensor)\n    compute_accuracy(tensor, model)\n    compute_calibration(tensor, [model])\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "validate-early-in-your-program-and-use-narrow-types-to-prove-that-you-did-so",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so"
        ]
      },
      "doc_lineno": 544
    },
    {
      "source": "from typing import cast\n\n# 0. Define types that describe specific validated states that your\n#    library depends on across multiple interfaces\nfrom our_library.narrow_types import NonEmpty\n\n# Returns unstructured/unvalidated data\ndef stream_data() -> tuple[str, ...]: ...\n\n# Create functions that can validate that the data satisfies\n# specific properties and ascribe to the validated data a new \n# type, which serves as proof of validation\ndef parse_stream(stream: tuple[str, ...]) -> NonEmpty[tuple[str, ...]]:\n    if not isinstance(stream, tuple): raise TypeError(\"not tuple\")\n    if not stream: raise TypeError(\"is empty\")\n    if not all(isinstance(item, str) for item in stream): raise TypeError(\"not strings\")\n    \n    # The sole purpose of this line is for infroming the static type checker.\n    # We don't actually use the `NonEmpty` type to change our data\n    # at all!\n    proven_data = cast(NonEmpty[tuple[str, ...]], stream)  \n    return proven_data\n\n# Design downstream functions to require this narrowed type, which can only\n# be obtained by going through the parsing process. Now functions operate\n# safely without having to re-validate the data at every turn, and their\n# requirements are now explicitly documented via annotations\ndef consumer1(data: NonEmpty[tuple[str, ...]]): ...\ndef consumer2(data: NonEmpty[tuple[str, ...]]): ...\ndef consumer3(data: NonEmpty[tuple[str, ...]]): ...\n\nif __name__ == \"__main__\":\n    # 1. Start with unstructured data\n    data = stream_data()  # type checker sees: tuple[str, ...]\n    \n    # Attempting to pass `data` to, e.g., `consumer1` would produce\n    # a static type checking error.\n\n    # 2. Enter \"parsing\" phase of program, where we validate the data\n    #    and ascribe a narrowed type to the data.\n    #    This is where we handle and log errors.\n    try:\n        # input: tuple[str, ...]  (less-structured)\n        parsed_data = parse_stream(data)\n        # output: NonEmpty[tuple[str, ...]]  (more-structured)\n    except TypeError:\n        # log error\n        # cue graceful recovery\n        ...\n\n    # 3. Enter \"execution\" phase of program: the 'illegal state' of having empty data\n    #    here is impossible, assuming we rely faithfully on our type checker, because\n    #    we are working with data that has been \"proven\" to be valid\n    consumer1(parsed_data)  # type checker: OK\n    consumer2(parsed_data)  # type checker: OK\n    consumer3(parsed_data)  # type checker: OK\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "all"
          ],
          "code_str": "all",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "all"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 28,
          "end_lineno": 28,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 28,
          "end_lineno": 28,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 46,
          "end_lineno": 46,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "validate-early-in-your-program-and-use-narrow-types-to-prove-that-you-did-so",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so"
        ]
      },
      "doc_lineno": 588
    },
    {
      "source": "x: Any   # starting with: x can be Any type\n\n# narrow x via isinstance:\nif isinstance(x, int):\n    # type checker narrows x to `int` here\n    ...\nelif isinstance(x, str):\n    # type checker narrows x to `str` here\n    ...\n\ny: int | list[int]  # starting with: y can be an int or list of ints\n\n# narrow y via assert\nassert isinstance(y, list)\ny  # type checker narows y to list[int]  (it is impossible for y to be an int here at runtime)\n\n# via casting\nfrom typing import cast\n\nz: Any  # starting with: x can be Any type\n\n# Warning: you can lie to the type checker using `cast`.\n# `cast` doesn't do any processing at runtime\nout = cast(tuple[int, int], z)  # type checker sees `out` as `tuple[int, int]`\n",
      "names": [
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 18,
          "end_lineno": 18,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 18,
          "end_lineno": 18,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 655
    },
    {
      "source": "from typing import Any\n\nfrom typing_extensions import TypeGuard\n\nclass NonNegativeInt(int):\n    ...\n\n# this is our type-guard, which can narrow int -> NonNegativeInt\ndef is_non_negative_int(x: int) -> TypeGuard[NonNegativeInt]: \n    return 0 < x\n\ndef process_age(x: NonNegativeInt): ...\n\ndef main(x: int):\n    if is_non_negative_int(x):\n        # x is narrowed to NonNegativeInt\n        process_age(x)\n    else:\n        # x is an int here\n        # log error\n        ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 684
    },
    {
      "source": "from torch import Tensor\nimport torch.nn as nn\nfrom typing import cast, Iterable\nfrom typing_extensions import TypeGuard\n\n\nclass TensorBCHW(Tensor):\n    \"\"\"Signals that a PyTorch tensor has been validated to\n    be shaped like a batch of images: (B, C, H, W)\"\"\"\n    ...\n\ndef load_data() -> Tensor: ...\ndef load_model() -> nn.Module: ...\n\n\ndef is_batch_of_images(\n    x: Tensor, expected_channel_size: int\n) -> TypeGuard[TensorBCHW]:\n    return isinstance(x, Tensor) and x.ndim == 4 and x.shape[1] == expected_channel_size\n        \n\ndef measure_data_distr(batch: TensorBCHW): ...\n\ndef compute_accuracy(batch: TensorBCHW, model: nn.Module): ...\n\ndef compute_calibration(batch: TensorBCHW, models: Iterable[nn.Module]): ...\n\n\nif __name__ == \"__main__\":\n    model = load_model()\n    tensor = load_data()\n    \n    # type checker sees tensor as: Tensor\n    if not is_batch_of_images(tensor, expected_channel_size=3):\n        raise TypeError(\"not a batch!\")\n    # type checker sees tensor as: TensorBCHW\n\n\n    # static type-checker ensures input is `TensorBCHW`\n    measure_data_distr(tensor)\n    compute_accuracy(tensor, model)\n    compute_calibration(tensor, [model])\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 715
    },
    {
      "source": "def count_vowels(text: str) -> int: ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "additional-resources",
        "headings": []
      },
      "doc_lineno": 230
    },
    {
      "source": "def count_vowels(text: Iterable[Hashable]) -> int: ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "additional-resources",
        "headings": []
      },
      "doc_lineno": 235
    }
  ],
  "generated/maite.interop.metrics.torchmetrics.TMClassificationMetric": [
    {
      "source": ">>> import torch\n>>> import torchmetrics.classification\n>>> from typing_extensions import Sequence\n>>> from maite.protocols import image_classification as ic, MetricMetadata\n>>> from maite.interop.metrics.torchmetrics import TMClassificationMetric\n>>>\n>>> preds: Sequence[ic.TargetType] = [\n...     torch.tensor([0.1, 0.8, 0.1]),\n...     torch.tensor([0.6, 0.2, 0.2]),\n...     torch.tensor([0.4, 0.3, 0.3]),\n... ]\n>>> target: Sequence[ic.TargetType] = [\n...     torch.tensor([0, 1, 0]),\n...     torch.tensor([1, 0, 0]),\n...     torch.tensor([0, 0, 1]),\n... ]\n>>> metadatas: Sequence[ic.DatumMetadataType] = [{\"id\": 1}, {\"id\": 2}, {\"id\": 3}]\n>>> # Create native TorchMetrics metric\n>>> classification_metric = torchmetrics.classification.MulticlassAccuracy(\n...     num_classes=3\n... )\n>>>\n>>> # Add additional field to base MetricMetadata\n>>> class MyMetricMetadata(MetricMetadata):\n...     num_classes: int\n>>> metadata: MyMetricMetadata = {\"id\": \"Multiclass Accuracy\", \"num_classes\": 3}\n>>>\n>>> # Wrap metric and apply to sample data\n>>> wrapped_classification_metric: ic.Metric = TMClassificationMetric(\n...     classification_metric, metadata=metadata\n... )\n>>> wrapped_classification_metric.update(preds, target, metadatas)\n>>> result = wrapped_classification_metric.compute()\n>>> result  # doctest: +SKIP\n{'MulticlassAccuracy': tensor(0.6667)}\n>>> print(\n...     f\"{result['MulticlassAccuracy'].item():0.3f}\"\n... )  # consistent formatting for doctest\n0.667",
      "names": [
        {
          "import_components": [
            "maite",
            "interop",
            "metrics",
            "torchmetrics",
            "TMClassificationMetric"
          ],
          "code_str": "TMClassificationMetric",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "maite._internals.interop.metrics.torchmetrics.TMClassificationMetric"
        },
        {
          "import_components": [
            "torchmetrics",
            "classification",
            "MulticlassAccuracy"
          ],
          "code_str": "torchmetrics.classification.MulticlassAccuracy",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "torchmetrics.classification.accuracy.MulticlassAccuracy"
        },
        {
          "import_components": [
            "torchmetrics",
            "classification",
            "MulticlassAccuracy",
            "()"
          ],
          "code_str": "classification_metric",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "torchmetrics.classification.accuracy.MulticlassAccuracy"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 25,
          "end_lineno": 25,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "maite",
            "interop",
            "metrics",
            "torchmetrics",
            "TMClassificationMetric"
          ],
          "code_str": "TMClassificationMetric",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "maite._internals.interop.metrics.torchmetrics.TMClassificationMetric"
        },
        {
          "import_components": [
            "torchmetrics",
            "classification",
            "MulticlassAccuracy",
            "()"
          ],
          "code_str": "classification_metric",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "torchmetrics.classification.accuracy.MulticlassAccuracy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Metric"
          ],
          "code_str": "ic.Metric",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Metric"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Metric",
            "()"
          ],
          "code_str": "wrapped_classification_metric",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Metric"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 36,
          "end_lineno": 36,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.interop.metrics.torchmetrics.TMClassificationMetric",
        "ref_id": "metrics-torchmetrics-tmclassificationmetric",
        "headings": [
          "metrics.torchmetrics.TMClassificationMetric"
        ]
      },
      "doc_lineno": 83
    }
  ],
  "generated/maite.interop.metrics.torchmetrics.TMDetectionMetric": [
    {
      "source": ">>> from typing import Sequence\n>>> import torch\n>>> import torchmetrics.detection\n>>> from maite.protocols import object_detection as od\n>>> from maite.interop.torchmetrics import TMDetectionMetric\n>>> from dataclasses import dataclass\n>>> @dataclass\n... class ObjectDetectionTarget_Impl:\n...     boxes: torch.Tensor\n...     labels: torch.Tensor\n...     scores: torch.Tensor\n>>> detection_metric = torchmetrics.detection.MeanAveragePrecision(iou_type=\"bbox\")\n>>> output_transform = lambda x: x[\"map_50\"]\n>>> wrapped_detect_metric: od.Metric = TMDetectionMetric(\n...     detection_metric, output_key=\"MAP\", output_transform=output_transform\n... )\n>>> preds: Sequence[od.TargetType] = [\n...     ObjectDetectionTarget_Impl(\n...         boxes=torch.Tensor([[258.0, 41.0, 606.0, 285.0]]),\n...         labels=torch.Tensor([0]),\n...         scores=torch.Tensor([0.536]),\n...     )\n... ]\n>>> targets: Sequence[od.TargetType] = [\n...     ObjectDetectionTarget_Impl(\n...         boxes=torch.Tensor([[214.0, 41.0, 562.0, 285.0]]),\n...         labels=torch.Tensor([0]),\n...         scores=torch.Tensor([1.0]),\n...     )\n... ]\n>>> metadatas: Sequence[od.DatumMetadataType] = [{\"id\": 1}]\n>>> wrapped_detect_metric.update(preds, targets, metadatas)\n>>> results = wrapped_detect_metric.compute()\n>>> print(results)\n{'MAP': tensor(1.)}",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "maite",
            "interop",
            "torchmetrics",
            "TMDetectionMetric"
          ],
          "code_str": "TMDetectionMetric",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "maite._internals.interop.metrics.torchmetrics_detection.TMDetectionMetric"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "torchmetrics",
            "detection",
            "MeanAveragePrecision"
          ],
          "code_str": "torchmetrics.detection.MeanAveragePrecision",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "torchmetrics.detection.mean_ap.MeanAveragePrecision"
        },
        {
          "import_components": [
            "torchmetrics",
            "detection",
            "MeanAveragePrecision",
            "()"
          ],
          "code_str": "detection_metric",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "torchmetrics.detection.mean_ap.MeanAveragePrecision"
        },
        {
          "import_components": [
            "maite",
            "interop",
            "torchmetrics",
            "TMDetectionMetric"
          ],
          "code_str": "TMDetectionMetric",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "maite._internals.interop.metrics.torchmetrics_detection.TMDetectionMetric"
        },
        {
          "import_components": [
            "torchmetrics",
            "detection",
            "MeanAveragePrecision",
            "()"
          ],
          "code_str": "detection_metric",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "torchmetrics.detection.mean_ap.MeanAveragePrecision"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "object_detection",
            "Metric"
          ],
          "code_str": "od.Metric",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "maite._internals.protocols.object_detection.Metric"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "object_detection",
            "Metric",
            "()"
          ],
          "code_str": "wrapped_detect_metric",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "maite._internals.protocols.object_detection.Metric"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "object_detection",
            "TargetType"
          ],
          "code_str": "od.TargetType",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "maite._internals.protocols.object_detection.ObjectDetectionTarget"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "object_detection",
            "TargetType"
          ],
          "code_str": "od.TargetType",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "maite._internals.protocols.object_detection.ObjectDetectionTarget"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 31,
          "end_lineno": 31,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.interop.metrics.torchmetrics.TMDetectionMetric",
        "ref_id": "metrics-torchmetrics-tmdetectionmetric",
        "headings": [
          "metrics.torchmetrics.TMDetectionMetric"
        ]
      },
      "doc_lineno": 62
    }
  ],
  "generated/maite.interop.models.yolo.YoloObjectDetector": [
    {
      "source": ">>> import io\n>>> import contextlib\n>>> import numpy as np\n>>> from typing_extensions import Sequence\n>>> from ultralytics import YOLO\n>>> from maite.interop.models.yolo import YoloObjectDetector\n>>> from maite.protocols import object_detection as od",
      "names": [
        {
          "import_components": [
            "io"
          ],
          "code_str": "io",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "io"
        },
        {
          "import_components": [
            "contextlib"
          ],
          "code_str": "contextlib",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "contextlib"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "interop",
            "models",
            "yolo",
            "YoloObjectDetector"
          ],
          "code_str": "YoloObjectDetector",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "maite._internals.interop.models.yolo.YoloObjectDetector"
        }
      ],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 33
    },
    {
      "source": ">>> with (\n...     contextlib.redirect_stdout(io.StringIO()),\n...     contextlib.redirect_stderr(io.StringIO()),\n... ):\n...     yolov5_model = YOLO(\"yolov5nu\")\n>>> metadata = ModelMetadata(id=\"YOLOv5nu\", index2label=yolov5_model.names)\n>>> wrapped_yolov5_model = YoloObjectDetector(yolov5_model, metadata)",
      "names": [],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 45
    },
    {
      "source": ">>> with (\n...     contextlib.redirect_stdout(io.StringIO()),\n...     contextlib.redirect_stderr(io.StringIO()),\n... ):\n...     yolov5_model = YOLO(\"./yolov5nu.pt\")\n>>> metadata = ModelMetadata(id=\"YOLOv5nu\", index2label=yolov5_model.names)\n>>> wrapped_yolov5_model = YoloObjectDetector(yolov5_model, metadata)",
      "names": [],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 55
    },
    {
      "source": ">>> with (\n...     contextlib.redirect_stdout(io.StringIO()),\n...     contextlib.redirect_stderr(io.StringIO()),\n... ):\n...     yolov8_model = YOLO(\"yolov8n\")\n>>> metadata = ModelMetadata(id=\"YOLOv8n\", index2label=yolov8_model.names)\n>>> wrapped_yolov8_model = YoloObjectDetector(yolov8_model, metadata)",
      "names": [],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 65
    },
    {
      "source": ">>> with (\n...     contextlib.redirect_stdout(io.StringIO()),\n...     contextlib.redirect_stderr(io.StringIO()),\n... ):\n...     yolov8_model = YOLO(\"./yolov8n.pt\")\n>>> metadata = ModelMetadata(id=\"YOLOv8n\", index2label=yolov8_model.names)\n>>> wrapped_yolov8_model = YoloObjectDetector(yolov8_model, metadata)",
      "names": [],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 75
    },
    {
      "source": ">>> N_DATAPOINTS = 5  # datapoints in dataset\n>>> C = 3  # number of color channels\n>>> H = 5  # img height\n>>> W = 6  # img width\n>>> batch_data: Sequence[od.InputType] = list(np.random.rand(N_DATAPOINTS, C, H, W))\n>>> model_results: Sequence[od.TargetType] = wrapped_yolov8_model(batch_data)\n>>> print(model_results)\n[ObjectDetectionTargets(boxes=array([], shape=(0, 4), dtype=float32), labels=array([], dtype=uint8), scores=array([], dtype=float32)), ObjectDetectionTargets(boxes=array([], shape=(0, 4), dtype=float32), labels=array([], dtype=uint8), scores=array([], dtype=float32)), ObjectDetectionTargets(boxes=array([], shape=(0, 4), dtype=float32), labels=array([], dtype=uint8), scores=array([], dtype=float32)), ObjectDetectionTargets(boxes=array([], shape=(0, 4), dtype=float32), labels=array([], dtype=uint8), scores=array([], dtype=float32)), ObjectDetectionTargets(boxes=array([], shape=(0, 4), dtype=float32), labels=array([], dtype=uint8), scores=array([], dtype=float32))]",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.interop.models.yolo.YoloObjectDetector",
        "ref_id": "models-yolo-yoloobjectdetector",
        "headings": [
          "models.yolo.YoloObjectDetector"
        ]
      },
      "doc_lineno": 86
    }
  ],
  "generated/maite.protocols.ArrayLike": [],
  "generated/maite.protocols.image_classification.Augmentation": [
    {
      "source": ">>> import copy\n>>> import numpy as np\n>>> from typing import Any\n>>> from collections.abc import Sequence\n>>> from maite.protocols import ArrayLike, DatumMetadata, AugmentationMetadata\n>>>\n>>> class EnrichedDatumMetadata(DatumMetadata):\n...     new_key: int  # add a field to those already in DatumMetadata\n...\n>>> class ImageAugmentation:\n...     def __init__(self, aug_name: str):\n...         self.metadata: AugmentationMetadata = {'id': aug_name}\n...     def __call__(\n...         self,\n...         data_batch: tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadata]]\n...     ) -> tuple[Sequence[np.ndarray], Sequence[np.ndarray], Sequence[EnrichedDatumMetadata]]:\n...         inputs, targets, mds = data_batch\n...         # We copy data passed into the constructor to avoid mutating original inputs\n...         # By using np.ndarray constructor, the static type-checker will let us treat\n...         # generic ArrayLike as a more narrow return type\n...         inputs_aug = [copy.copy(np.array(input)) for input in inputs]\n...         targets_aug = [copy.copy(np.array(target)) for target in targets]\n...         # Modify inputs_aug, targets_aug, or mds_aug as needed\n...         # In this example, we just add a new metadata field\n...         mds_aug = []\n...         for i, md in enumerate(mds):\n...             mds_aug.append(EnrichedDatumMetadata(**md, new_key=i))\n...         return inputs_aug, targets_aug, mds_aug",
      "names": [
        {
          "import_components": [
            "copy"
          ],
          "code_str": "copy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "copy"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "collections",
            "abc"
          ],
          "code_str": "collections.abc",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "collections.abc"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "copy",
            "copy"
          ],
          "code_str": "copy.copy",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "copy.copy"
        },
        {
          "import_components": [
            "numpy",
            "array"
          ],
          "code_str": "np.array",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "numpy.array"
        },
        {
          "import_components": [
            "copy",
            "copy"
          ],
          "code_str": "copy.copy",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "copy.copy"
        },
        {
          "import_components": [
            "numpy",
            "array"
          ],
          "code_str": "np.array",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "numpy.array"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Augmentation",
        "ref_id": "image-classification-augmentation",
        "headings": [
          "image_classification.Augmentation"
        ]
      },
      "doc_lineno": 70
    },
    {
      "source": ">>> from maite.protocols import image_classification as ic\n>>> im_aug: ic.Augmentation = ImageAugmentation(aug_name = 'an_augmentation')",
      "names": [
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation"
          ],
          "code_str": "ic.Augmentation",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation",
            "()"
          ],
          "code_str": "im_aug",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Augmentation",
        "ref_id": "image-classification-augmentation",
        "headings": [
          "image_classification.Augmentation"
        ]
      },
      "doc_lineno": 76
    }
  ],
  "generated/maite.protocols.image_classification.DataLoader": [],
  "generated/maite.protocols.image_classification.Dataset": [
    {
      "source": ">>> import numpy as np\n>>> from typing import Any\n>>> from typing_extensions import TypedDict\n>>> from maite.protocols import ArrayLike, DatasetMetadata",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "image-classification-dataset",
        "headings": [
          "image_classification.Dataset"
        ]
      },
      "doc_lineno": 47
    },
    {
      "source": ">>> N_CLASSES: int = 5\n>>> N_DATUM: int = 10\n>>> images: list[np.ndarray] = [np.random.rand(3, 32, 16) for _ in range(N_DATUM)]\n>>> targets: np.ndarray = np.eye(N_CLASSES)[np.random.choice(N_CLASSES, N_DATUM)]",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "image-classification-dataset",
        "headings": [
          "image_classification.Dataset"
        ]
      },
      "doc_lineno": 55
    },
    {
      "source": ">>> class MyDatumMetadata(DatumMetadata):\n...     hour_of_day: float\n>>> datum_metadata = [\n...     MyDatumMetadata(id=i, hour_of_day=np.random.rand() * 24)\n...     for i in range(N_DATUM)\n... ]",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "image-classification-dataset",
        "headings": [
          "image_classification.Dataset"
        ]
      },
      "doc_lineno": 65
    },
    {
      "source": ">>> class ImageDataset:\n...     def __init__(\n...         self,\n...         dataset_name: str,\n...         index2label: dict[int, str],\n...         images: list[np.ndarray],\n...         targets: np.ndarray,\n...         datum_metadata: list[MyDatumMetadata],\n...     ):\n...         self.images = images\n...         self.targets = targets\n...         self.metadata = DatasetMetadata(\n...             {\"id\": dataset_name, \"index2label\": index2label}\n...         )\n...         self._datum_metadata = datum_metadata\n...\n...     def __len__(self) -> int:\n...         return len(images)\n...\n...     def __getitem__(\n...         self, ind: int\n...     ) -> tuple[np.ndarray, np.ndarray, MyDatumMetadata]:\n...         return self.images[ind], self.targets[ind], self._datum_metadata[ind]",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "image-classification-dataset",
        "headings": [
          "image_classification.Dataset"
        ]
      },
      "doc_lineno": 92
    },
    {
      "source": ">>> from maite.protocols import image_classification as ic\n>>> dataset: ic.Dataset = ImageDataset(\n...     \"a_dataset\",\n...     {i: f\"class_name_{i}\" for i in range(N_CLASSES)},\n...     images,\n...     targets,\n...     datum_metadata,\n... )",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset"
          ],
          "code_str": "ic.Dataset",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset",
            "()"
          ],
          "code_str": "dataset",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "image-classification-dataset",
        "headings": [
          "image_classification.Dataset"
        ]
      },
      "doc_lineno": 104
    }
  ],
  "generated/maite.protocols.image_classification.Metric": [
    {
      "source": ">>> from typing import Any, Sequence\n>>> import numpy as np\n>>> from maite.protocols import ArrayLike\n>>> from maite.protocols import image_classification as ic",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "image-classification-metric",
        "headings": [
          "image_classification.Metric"
        ]
      },
      "doc_lineno": 40
    },
    {
      "source": ">>> class MyAccuracy:\n...     metadata: MetricMetadata = {\"id\": \"Example Multiclass Accuracy\"}\n...\n...     def __init__(self):\n...         self._total = 0\n...         self._correct = 0\n...\n...     def reset(self) -> None:\n...         self._total = 0\n...         self._correct = 0\n...\n...     def update(\n...         self,\n...         pred_batch: Sequence[ArrayLike],\n...         target_batch: Sequence[ArrayLike],\n...         metadata_batch: Sequence[DatumMetadataType],\n...     ) -> None:\n...         model_preds = [np.array(r) for r in pred_batch]\n...         true_onehot = [np.array(r) for r in target_batch]\n...\n...         # Stack into single array, convert to class indices\n...         model_classes = np.vstack(model_preds).argmax(axis=1)\n...         truth_classes = np.vstack(true_onehot).argmax(axis=1)\n...\n...         # Compare classes and update running counts\n...         same = model_classes == truth_classes\n...         self._total += len(same)\n...         self._correct += same.sum().item()\n...\n...     def compute(self) -> dict[str, Any]:\n...         if self._total > 0:\n...             return {\"accuracy\": self._correct / self._total}\n...         else:\n...             raise Exception(\"No batches processed yet.\")",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 27,
          "end_lineno": 27,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "Exception"
          ],
          "code_str": "Exception",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "Exception"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "image-classification-metric",
        "headings": [
          "image_classification.Metric"
        ]
      },
      "doc_lineno": 75
    },
    {
      "source": ">>> accuracy: ic.Metric = MyAccuracy()",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "image-classification-metric",
        "headings": [
          "image_classification.Metric"
        ]
      },
      "doc_lineno": 80
    },
    {
      "source": ">>> # batch 1\n>>> model_preds = [\n...     np.array([0.8, 0.1, 0.0, 0.1]),\n...     np.array([0.1, 0.2, 0.6, 0.1]),\n... ]  # predicted classes: 0, 2\n>>> true_onehot = [\n...     np.array([1.0, 0.0, 0.0, 0.0]),\n...     np.array([0.0, 1.0, 0.0, 0.0]),\n... ]  # true classes: 0, 1\n>>> metadatas: list[ic.DatumMetadataType] = [{\"id\": 1}, {\"id\": 2}]\n>>> accuracy.update(model_preds, true_onehot, metadatas)\n>>> print(accuracy.compute())\n{'accuracy': 0.5}\n>>>\n>>> # batch 2\n>>> model_preds = [\n...     np.array([0.1, 0.1, 0.7, 0.1]),\n...     np.array([0.0, 0.1, 0.0, 0.9]),\n... ]  # predicted classes: 2, 3\n>>> true_onehot = [\n...     np.array([0.0, 0.0, 1.0, 0.0]),\n...     np.array([0.0, 0.0, 0.0, 1.0]),\n... ]  # true classes: 2, 3\n>>> metadatas: list[ic.DatumMetadataType] = [{\"id\": 3}, {\"id\": 4}]\n>>> accuracy.update(model_preds, true_onehot, metadatas)\n>>>\n>>> print(accuracy.compute())\n{'accuracy': 0.75}",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 27,
          "end_lineno": 27,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "image-classification-metric",
        "headings": [
          "image_classification.Metric"
        ]
      },
      "doc_lineno": 111
    }
  ],
  "generated/maite.protocols.image_classification.Model": [
    {
      "source": ">>> import maite.protocols.image_classification as ic\n>>> import numpy as np\n>>> import numpy.typing as npt\n>>> from maite.protocols import ArrayLike, ModelMetadata\n>>> from typing import Sequence",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "typing"
          ],
          "code_str": "numpy.typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy.typing"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "image-classification-model",
        "headings": [
          "image_classification.Model"
        ]
      },
      "doc_lineno": 41
    },
    {
      "source": ">>> class LinearClassifier:\n...     def __init__(self) -> None:\n...         # Set up required metadata attribute using the default `ModelMetadata` type,\n...         # using class name for the ID\n...         self.metadata: ModelMetadata = {\"id\": self.__class__.__name__}\n...\n...         # Initialize weights\n...         rng = np.random.default_rng(12345678)\n...         num_classes = 10\n...         flattened_size = 3 * 32 * 32\n...         self.weights = -0.2 + 0.4 * rng.random((flattened_size, num_classes))\n...         self.bias = -0.2 + 0.4 * rng.random((1, num_classes))\n...\n...     def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[npt.NDArray]:\n...         # Convert each element in batch to ndarray, flatten,\n...         # then combine into 4D array of shape-(N, C, H, W)\n...         batch_np = np.vstack([np.asarray(x).flatten() for x in batch])\n...\n...         # Send input batch through model\n...         out = batch_np @ self.weights + self.bias\n...         out = np.exp(out) / np.sum(\n...             np.exp(out), axis=1, keepdims=True\n...         )  # softmax\n...\n...         # Restructure to sequence of shape-(10,) probabilities\n...         return [row for row in out]",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "image-classification-model",
        "headings": [
          "image_classification.Model"
        ]
      },
      "doc_lineno": 71
    },
    {
      "source": ">>> batch_size = 8\n>>> rng = np.random.default_rng(12345678)\n>>> batch: Sequence[ArrayLike] = [\n...     -0.2 + 0.4 * rng.random((3, 32, 32)) for _ in range(batch_size)\n... ]\n>>>\n>>> model: ic.Model = LinearClassifier()\n>>> out = model(batch)",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "image-classification-model",
        "headings": [
          "image_classification.Model"
        ]
      },
      "doc_lineno": 82
    },
    {
      "source": ">>> np.set_printoptions(\n...     floatmode=\"fixed\", precision=2\n... )  # for reproducible output for doctest\n>>> for probs in out:  # doctest: +NORMALIZE_WHITESPACE\n...     print(np.round(probs, 2))\n[0.16 0.10 0.16 0.14 0.04 0.02 0.06 0.04 0.17 0.10]\n[0.21 0.16 0.04 0.07 0.08 0.05 0.09 0.03 0.18 0.09]\n[0.15 0.11 0.13 0.11 0.09 0.09 0.07 0.04 0.19 0.02]\n[0.04 0.08 0.14 0.07 0.12 0.20 0.11 0.06 0.14 0.04]\n[0.03 0.08 0.06 0.05 0.17 0.18 0.09 0.03 0.12 0.19]\n[0.09 0.04 0.10 0.03 0.32 0.05 0.07 0.04 0.15 0.09]\n[0.15 0.05 0.10 0.05 0.11 0.14 0.04 0.08 0.08 0.20]\n[0.11 0.11 0.08 0.11 0.08 0.05 0.24 0.03 0.08 0.12]",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "image-classification-model",
        "headings": [
          "image_classification.Model"
        ]
      },
      "doc_lineno": 98
    }
  ],
  "generated/maite.protocols.object_detection.Augmentation": [
    {
      "source": ">>> import numpy as np\n>>> np.random.seed(1)\n>>> import copy\n>>> from dataclasses import dataclass\n>>> from typing import Any, Sequence\n>>> from maite.protocols import AugmentationMetadata, object_detection as od",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "random",
            "seed"
          ],
          "code_str": "np.random.seed",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "numpy.random.seed"
        },
        {
          "import_components": [
            "copy"
          ],
          "code_str": "copy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "copy"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 50
    },
    {
      "source": ">>> N_DATAPOINTS = 3  # datapoints in dataset\n>>> N_CLASSES = 2  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 10  # img height\n>>> W = 10  # img width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 58
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray\n>>> xb: Sequence[od.InputType] = list(np.zeros((N_DATAPOINTS, C, H, W)))\n>>> yb: Sequence[od.TargetType] = list(\n...     MyObjectDetectionTarget(boxes=np.empty(0), labels=np.empty(0), scores=np.empty(0))\n...     for _ in range(N_DATAPOINTS)\n... )\n>>> mdb: Sequence[od.DatumMetadataType] = list({\"id\": i} for i in range(N_DATAPOINTS))\n>>> # Display the first datum in batch, first color channel, and only first 5 rows and cols\n>>> np.set_printoptions(floatmode='fixed', precision=3)  # for reproducible output for doctest\n>>> np.array(xb[0])[0][:5, :5]  # doctest: +NORMALIZE_WHITESPACE\narray([[0.000, 0.000, 0.000, 0.000, 0.000],\n       [0.000, 0.000, 0.000, 0.000, 0.000],\n       [0.000, 0.000, 0.000, 0.000, 0.000],\n       [0.000, 0.000, 0.000, 0.000, 0.000],\n       [0.000, 0.000, 0.000, 0.000, 0.000]])",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 90
    },
    {
      "source": ">>> np_noise = lambda shape: np.round(np.random.random(shape), 3)\n>>> class ImageAugmentation:\n...     def __init__(self, aug_func: Any, metadata: AugmentationMetadata):\n...         self.aug_func = aug_func\n...         self.metadata = metadata\n...     def __call__(\n...         self,\n...         batch: tuple[Sequence[od.InputType], Sequence[od.TargetType], Sequence[od.DatumMetadataType]],\n...     ) -> tuple[Sequence[od.InputType], Sequence[od.TargetType], Sequence[od.DatumMetadataType]]:\n...         xb, yb, mdb = batch\n...         # Copy data passed into the constructor to avoid mutating original inputs\n...         xb_aug = [copy.copy(input) for input in xb]\n...         # Add random noise to the input batch data, xb\n...         # (Note that all batch data dimensions (shapes) are the same in this example)\n...         shape = np.array(xb[0]).shape\n...         xb_aug = [x + self.aug_func(shape) for x in xb]\n...         # Note that this example augmentation only affects inputs--not targets\n...         return xb_aug, yb, mdb",
      "names": [
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 112
    },
    {
      "source": ">>> noise: od.Augmentation = ImageAugmentation(np_noise, metadata={\"id\": \"np_rand_noise\"})",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 117
    },
    {
      "source": ">>> xb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))\n>>> # Display the first datum in batch, first color channel, and only first 5 rows and cols\n>>> np.array(xb_aug[0])[0][:5, :5]  # doctest: +NORMALIZE_WHITESPACE\narray([[0.417, 0.720, 0.000, 0.302, 0.147],\n       [0.419, 0.685, 0.204, 0.878, 0.027],\n       [0.801, 0.968, 0.313, 0.692, 0.876],\n       [0.098, 0.421, 0.958, 0.533, 0.692],\n       [0.989, 0.748, 0.280, 0.789, 0.103]])",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "object-detection-augmentation",
        "headings": [
          "object_detection.Augmentation"
        ]
      },
      "doc_lineno": 130
    }
  ],
  "generated/maite.protocols.object_detection.DataLoader": [],
  "generated/maite.protocols.object_detection.Dataset": [
    {
      "source": ">>> import numpy as np\n>>> from dataclasses import dataclass\n>>> from maite.protocols import (\n...     DatumMetadata,\n...     DatasetMetadata,\n...     object_detection as od,\n... )",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 50
    },
    {
      "source": ">>> N_DATUM = 5  # data points in dataset\n>>> N_CLASSES = 2  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 10  # image height\n>>> W = 10  # image width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 58
    },
    {
      "source": ">>> def generate_random_bbox(\n...     n_classes: int, min_size: int = 2, max_size: int = 4\n... ) -> np.ndarray:\n...     # Generate random coordinates for top-left corner of bbox\n...     x1 = np.random.randint(0, W - min_size)\n...     y1 = np.random.randint(0, H - min_size)\n...     # Generate random width and height, ensuring bounding box stays within image boundaries\n...     bbox_width = np.random.randint(min_size, min(max_size, W - x1))\n...     bbox_height = np.random.randint(min_size, min(max_size, H - y1))\n...     # Set coordinates for bottom-right corner of bbox\n...     x2 = x1 + bbox_width\n...     y2 = y1 + bbox_height\n...     # Pick random class label\n...     label = np.random.choice(n_classes)\n...     return np.array([x1, y1, x2, y2, label])",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "min"
          ],
          "code_str": "min",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "min"
        },
        {
          "import_components": [
            "min"
          ],
          "code_str": "min",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "min"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 81
    },
    {
      "source": ">>> def generate_random_annotation(max_num_detections: int = 2) -> np.ndarray:\n...     num_detections = np.random.choice(max_num_detections + 1)\n...     annotation = [\n...         generate_random_bbox(N_CLASSES) for _ in range(num_detections)\n...     ]\n...     return np.vstack(annotation) if num_detections > 0 else np.empty(0)",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 88
    },
    {
      "source": ">>> images: list[np.ndarray] = list(np.random.rand(N_DATUM, C, H, W))\n>>> annotations: list[np.ndarray] = [\n...     generate_random_annotation() for _ in range(N_DATUM)\n... ]\n>>> hour_of_day: list[int] = [np.random.choice(24) for _ in range(N_DATUM)]\n>>> dataset: list[tuple] = list(zip(images, annotations, hour_of_day))",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "tuple"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 97
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 106
    },
    {
      "source": ">>> class MyDatumMetadata(DatumMetadata):\n...     hour_of_day: int",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 112
    },
    {
      "source": ">>> class ImageDataset:\n...     # Set up required dataset-level metadata\n...     metadata: DatasetMetadata = {\n...         \"id\": \"Dummy Dataset\",\n...         \"index2label\": {i: f\"class_name_{i}\" for i in range(N_CLASSES)},\n...     }\n...\n...     def __init__(self, dataset: list[tuple[np.ndarray, np.ndarray, int]]):\n...         self.dataset = dataset\n...\n...     def __len__(self) -> int:\n...         return len(self.dataset)\n...\n...     def __getitem__(\n...         self, index: int\n...     ) -> tuple[np.ndarray, od.ObjectDetectionTarget, od.DatumMetadataType]:\n...         if index < 0 or index >= len(self):\n...             raise IndexError(\n...                 f\"Index {index} is out of range for the dataset, which has length {len(self)}.\"\n...             )\n...         image, annotations, hour_of_day = self.dataset[index]\n...         # Structure ground truth target\n...         boxes, labels = [], []\n...         for _, ann in enumerate(annotations):\n...             bbox = ann[:-1]\n...             label = ann[-1:]\n...             if len(bbox) != 0:\n...                 boxes.append(bbox)\n...                 labels.append(label)\n...         od_target = MyObjectDetectionTarget(\n...             boxes=np.array(boxes),\n...             labels=np.array(labels),\n...             scores=np.ones(len(boxes)),\n...         )\n...         # Structure datum-level metadata\n...         datum_metadata: MyDatumMetadata = {\n...             \"id\": str(index),\n...             \"hour_of_day\": hour_of_day,\n...         }\n...         return image, od_target, datum_metadata",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "IndexError"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 27,
          "end_lineno": 27,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 33,
          "end_lineno": 33,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 157
    },
    {
      "source": ">>> maite_od_dataset: od.Dataset = ImageDataset(dataset)",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "object-detection-dataset",
        "headings": [
          "object_detection.Dataset"
        ]
      },
      "doc_lineno": 162
    }
  ],
  "generated/maite.protocols.object_detection.Metric": [
    {
      "source": ">>> from dataclasses import dataclass\n>>> from maite.protocols import ArrayLike, MetricMetadata\n>>> from typing import Any, Sequence\n>>> import maite.protocols.object_detection as od\n>>> import numpy as np",
      "names": [
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "numpy"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": ">>> class MyIoUMetric:\n...     def __init__(self, id: str):\n...         self.pred_boxes = []  # elements correspond to predicted boxes in single image\n...         self.target_boxes = []  # elements correspond to ground truth boxes in single image\n...         # Store provided id for this metric instance\n...         self.metadata = MetricMetadata(id=id)\n...\n...     def reset(self) -> None:\n...         self.pred_boxes = []\n...         self.target_boxes = []\n...\n...     def update(\n...         self,\n...         pred_batch: Sequence[od.ObjectDetectionTarget],\n...         target_batch: Sequence[od.ObjectDetectionTarget],\n...         metadata_batch: Sequence[od.DatumMetadataType],\n...     ) -> None:\n...         self.pred_boxes.extend(pred_batch)\n...         self.target_boxes.extend(target_batch)\n...\n...     @staticmethod\n...     def iou_vec(boxes_a: ArrayLike, boxes_b: ArrayLike) -> np.ndarray:\n...         # Break up points into separate columns\n...         x0a, y0a, x1a, y1a = np.split(boxes_a, 4, axis=1)\n...         x0b, y0b, x1b, y1b = np.split(boxes_b, 4, axis=1)\n...         # Calculate intersections\n...         xi_0, yi_0 = np.split(\n...             np.maximum(\n...                 np.append(x0a, y0a, axis=1), np.append(x0b, y0b, axis=1)\n...             ),\n...             2,\n...             axis=1,\n...         )\n...         xi_1, yi_1 = np.split(\n...             np.minimum(\n...                 np.append(x1a, y1a, axis=1), np.append(x1b, y1b, axis=1)\n...             ),\n...             2,\n...             axis=1,\n...         )\n...         ints: np.ndarray = np.maximum(0, xi_1 - xi_0) * np.maximum(\n...             0, yi_1 - yi_0\n...         )\n...         # Calculate unions (as sum of areas minus their intersection)\n...         unions: np.ndarray = (\n...             (x1a - x0a) * (y1a - y0a)\n...             + (x1b - x0b) * (y1b - y0b)\n...             - (xi_1 - xi_0) * (yi_1 - yi_0)\n...         )\n...         return ints / unions\n...\n...     def compute(self) -> dict[str, Any]:\n...         mean_iou_by_img: list[float] = []\n...         for pred_box, tgt_box in zip(self.pred_boxes, self.target_boxes):\n...             single_img_ious = self.iou_vec(pred_box.boxes, tgt_box.boxes)\n...             mean_iou_by_img.append(float(np.mean(single_img_ious)))\n...         return {\"mean_iou\": np.mean(np.array(mean_iou_by_img)).item()}",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "staticmethod"
          ],
          "code_str": "staticmethod",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "staticmethod"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 52,
          "end_lineno": 52,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 52,
          "end_lineno": 52,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 53,
          "end_lineno": 53,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 53,
          "end_lineno": 53,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 54,
          "end_lineno": 54,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 56,
          "end_lineno": 56,
          "context": "none",
          "resolved_location": "float"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 106
    },
    {
      "source": ">>> iou_metric: od.Metric = MyIoUMetric(id=\"IoUMetric\")",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 110
    },
    {
      "source": ">>> prediction_boxes: list[tuple[int, int, int, int]] = [\n...     (1, 1, 12, 12),\n...     (100, 100, 120, 120),\n...     (180, 180, 270, 270),\n... ]",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 120
    },
    {
      "source": ">>> target_boxes: list[tuple[int, int, int, int]] = [\n...     (1, 1, 10, 10),\n...     (100, 100, 120, 120),\n...     (200, 200, 300, 300),\n... ]",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 126
    },
    {
      "source": ">>> @dataclass\n... class ObjectDetectionTargetImpl:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 137
    },
    {
      "source": ">>> num_boxes = len(target_boxes)\n>>> fake_labels = np.random.randint(0, 9, num_boxes)\n>>> fake_scores = np.zeros(num_boxes)\n>>> pred_batch = [\n...     ObjectDetectionTargetImpl(\n...         boxes=np.array(prediction_boxes), labels=fake_labels, scores=fake_scores\n...     )\n... ]\n>>> target_batch: Sequence[ObjectDetectionTargetImpl] = [\n...     ObjectDetectionTargetImpl(\n...         boxes=np.array(target_boxes), labels=fake_labels, scores=fake_scores\n...     )\n... ]\n>>> metadata_batch: Sequence[od.DatumMetadataType] = [{\"id\": 1}]",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 152
    },
    {
      "source": ">>> iou_metric.update(pred_batch, target_batch, metadata_batch)\n>>> print(iou_metric.compute())\n{'mean_iou': 0.6802112029384757}",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "object-detection-metric",
        "headings": [
          "object_detection.Metric"
        ]
      },
      "doc_lineno": 158
    }
  ],
  "generated/maite.protocols.object_detection.Model": [
    {
      "source": ">>> from dataclasses import dataclass\n>>> from typing import Sequence\n>>> import numpy as np\n>>> import maite.protocols.object_detection as od\n>>> from maite.protocols import ModelMetadata",
      "names": [
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 40
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": ">>> N_DATAPOINTS = 2  # datapoints in dataset\n>>> N_CLASSES = 5  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 32  # img height\n>>> W = 32  # img width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 56
    },
    {
      "source": ">>> simple_batch: list[np.ndarray] = [\n...     np.random.rand(C, H, W) for _ in range(N_DATAPOINTS)\n... ]",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 62
    },
    {
      "source": ">>> class ObjectDetectionDummyModel:\n...     metadata: ModelMetadata = {\"id\": \"ObjectDetectionDummyModel\"}\n...\n...     def __call__(\n...         self, batch: Sequence[od.InputType]\n...     ) -> Sequence[MyObjectDetectionTarget]:\n...         # For the simplicity, we don't provide an object detection model here, but the output from a model.\n...         DETECTIONS_PER_IMG = (\n...             2  # number of bounding boxes detections per image/datapoints\n...         )\n...         all_boxes = np.array(\n...             [[1, 3, 5, 9], [2, 5, 8, 12], [4, 10, 8, 20], [3, 5, 6, 15]]\n...         )  # all detection boxes for N_DATAPOINTS\n...         all_predictions = list()\n...         for datum_idx in range(N_DATAPOINTS):\n...             boxes = all_boxes[datum_idx : datum_idx + DETECTIONS_PER_IMG]\n...             labels = np.random.randint(N_CLASSES, size=DETECTIONS_PER_IMG)\n...             scores = np.random.rand(DETECTIONS_PER_IMG)\n...             predictions = MyObjectDetectionTarget(boxes, labels, scores)\n...             all_predictions.append(predictions)\n...         return all_predictions",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 87
    },
    {
      "source": ">>> od_dummy_model: od.Model = ObjectDetectionDummyModel()\n>>> od_dummy_model.metadata\n{'id': 'ObjectDetectionDummyModel'}\n>>> predictions = od_dummy_model(simple_batch)\n>>> predictions  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n[MyObjectDetectionTarget(boxes=array([[ 1,  3,  5,  9], [ 2,  5,  8, 12]]), labels=array([..., ...]), scores=array([..., ...])),\nMyObjectDetectionTarget(boxes=array([[ 2,  5,  8, 12], [ 4, 10,  8, 20]]), labels=array([..., ...]), scores=array([..., ...]))]",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "object-detection-model",
        "headings": [
          "object_detection.Model"
        ]
      },
      "doc_lineno": 98
    }
  ],
  "generated/maite.protocols.object_detection.ObjectDetectionTarget": [],
  "generated/maite.tasks.augment_dataloader": [],
  "generated/maite.tasks.evaluate": [],
  "generated/maite.tasks.evaluate_from_predictions": [],
  "generated/maite.tasks.predict": [],
  "how_to/static_typing": [
    {
      "source": "{\n    \"python.analysis.typeCheckingMode\": \"basic\"\n}",
      "names": [],
      "example": {
        "document": "how_to/static_typing",
        "ref_id": "running-pyright-as-a-vs-code-extension",
        "headings": [
          "Enable Static Type Checking",
          "Static Typing Checking with Pyright",
          "Running Pyright as a VS Code Extension"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": "# Wrong type being assigned to variable x\nx:int = \"abc\"",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "how_to/static_typing",
        "ref_id": "running-pyright-as-a-vs-code-extension",
        "headings": [
          "Enable Static Type Checking",
          "Static Typing Checking with Pyright",
          "Running Pyright as a VS Code Extension"
        ]
      },
      "doc_lineno": 60
    }
  ],
  "how_to/wrap_image_classification_dataset": [
    {
      "source": "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "from typing import Literal, cast\n\nimport datasets\nimport numpy as np\nimport numpy.typing as npt\nimport PIL.Image\nfrom IPython.display import display\n\nimport maite.protocols.image_classification as ic\nfrom maite.protocols import DatasetMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Literal"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "typing"
          ],
          "code_str": "numpy.typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "numpy.typing"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 29
    },
    {
      "source": "Python implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\nIPython : 8.37.0\ndatasets: 4.1.1\nmaite   : 0.9.0\nnumpy   : 2.2.6\nPIL     : 11.3.0",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": "cifar10_dataset_dict = cast(\n    datasets.DatasetDict, datasets.load_dataset(path=\"uoft-cs/cifar10\")\n)\ncifar10_dataset_dict",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 62
    },
    {
      "source": "README.md: 0.00B [00:00, ?B/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 73
    },
    {
      "source": "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 79
    },
    {
      "source": "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 85
    },
    {
      "source": "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 91
    },
    {
      "source": "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 97
    },
    {
      "source": "DatasetDict({\n    train: Dataset({\n        features: ['img', 'label'],\n        num_rows: 50000\n    })\n    test: Dataset({\n        features: ['img', 'label'],\n        num_rows: 10000\n    })\n})",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 104
    },
    {
      "source": "cifar10_train: datasets.Dataset = cifar10_dataset_dict[\"train\"]\ncifar10_test: datasets.Dataset = cifar10_dataset_dict[\"test\"]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 122
    },
    {
      "source": "for label_number, label_name in enumerate(cifar10_train.features[\"label\"].names):\n    print(f\"Label {label_number}: {label_name}\")",
      "names": [
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 135
    },
    {
      "source": "Label 0: airplane\nLabel 1: automobile\nLabel 2: bird\nLabel 3: cat\nLabel 4: deer\nLabel 5: dog\nLabel 6: frog\nLabel 7: horse\nLabel 8: ship\nLabel 9: truck",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 143
    },
    {
      "source": "for i in range(0, len(cifar10_train), 10000):\n    item = cifar10_train[i]\n    label, img = item[\"label\"], item[\"img\"]\n    label_name = cifar10_train.features[\"label\"].names[label]\n    print(f\"CIFAR-10 Train {i}\")\n    print(f\"Label: {label} {label_name}\")\n    display(img)",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 157
    },
    {
      "source": "CIFAR-10 Train 0\nLabel: 0 airplane",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 170
    },
    {
      "source": "CIFAR-10 Train 10000\nLabel: 8 ship",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 180
    },
    {
      "source": "CIFAR-10 Train 20000\nLabel: 1 automobile",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 190
    },
    {
      "source": "CIFAR-10 Train 30000\nLabel: 1 automobile",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 200
    },
    {
      "source": "CIFAR-10 Train 40000\nLabel: 6 frog",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 210
    },
    {
      "source": "# Extend DatasetMetadata to record whether dataset is a train or test split\nclass CustomDatasetMetadata(DatasetMetadata):\n    split: str\n\n\nclass Cifar10Dataset:\n    def __init__(\n        self, cifar10_dataset_split: datasets.Dataset, split: Literal[\"train\", \"test\"]\n    ):\n        # Save the CIFAR-10 dataset given by the user. This is helpful if you want to\n        # sample the dataset using the Hugging Face API prior to using it.\n        self.dataset = cifar10_dataset_split\n\n        # Create a dictionary mapping label number to label name from the label metadata\n        # in the underlying dataset.\n        index2label = {\n            i: label for i, label in enumerate(self.dataset.features[\"label\"].names)\n        }\n\n        # Create required metadata attribute (with custom split key)\n        self.metadata: DatasetMetadata = CustomDatasetMetadata(\n            id=\"CIFAR-10\", index2label=index2label, split=split\n        )\n\n        # Get the number of classes used in the dataset\n        num_classes = self.dataset.features[\"label\"].num_classes\n\n        # Create a collection of target vectors to be used for the one-hot encoding of labels\n        self.targets = np.eye(num_classes)\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[npt.NDArray, npt.NDArray, ic.DatumMetadataType]:\n        # Look up item in the dataset, which returns a dictionary with two keys:\n        # - \"img\": PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n        # - \"label\": int\n        item = self.dataset[index]\n        img_pil = item[\"img\"]\n        label = item[\"label\"]\n\n        # Convert the PIL image to a NumPy array\n        img_hwc = np.array(img_pil)  # shape (H, W, C)\n\n        # Use MAITE array index convention for representing images: shape (C, H, W)\n        img_chw = img_hwc.transpose(2, 0, 1)\n\n        # Get one-hot encoded tensor indicating the class label for this image\n        target = self.targets[label, :].copy()\n\n        # CIFAR-10 does not have any extra metadata, so we record only the index of this datum\n        metadata: ic.DatumMetadataType = {\"id\": index}\n\n        return img_chw, target, metadata",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 31,
          "end_lineno": 31,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 32,
          "end_lineno": 32,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 36,
          "end_lineno": 36,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 251
    },
    {
      "source": "train_dataset: ic.Dataset = Cifar10Dataset(\n    cifar10_dataset_split=cifar10_train, split=\"train\"\n)\ntrain_dataset.metadata",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 315
    },
    {
      "source": "{'id': 'CIFAR-10',\n 'index2label': {0: 'airplane',\n  1: 'automobile',\n  2: 'bird',\n  3: 'cat',\n  4: 'deer',\n  5: 'dog',\n  6: 'frog',\n  7: 'horse',\n  8: 'ship',\n  9: 'truck'},\n 'split': 'train'}",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 327
    },
    {
      "source": "test_dataset: ic.Dataset = Cifar10Dataset(\n    cifar10_dataset_split=cifar10_test, split=\"test\"\n)\ntest_dataset.metadata",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 344
    },
    {
      "source": "{'id': 'CIFAR-10',\n 'index2label': {0: 'airplane',\n  1: 'automobile',\n  2: 'bird',\n  3: 'cat',\n  4: 'deer',\n  5: 'dog',\n  6: 'frog',\n  7: 'horse',\n  8: 'ship',\n  9: 'truck'},\n 'split': 'test'}",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 356
    },
    {
      "source": "print(f\"CIFAR-10 size: train={len(train_dataset)}, test={len(test_dataset)}\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 373
    },
    {
      "source": "CIFAR-10 size: train=50000, test=10000",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 380
    },
    {
      "source": "def print_datum(dataset, index):\n    img_arr_chw, target, datum_metadata = dataset[index]\n    print(f\"Datum {datum_metadata['id']}\")\n    print(f\"  Input Image Array: {str(img_arr_chw)[:30]}...\")\n    print(f\"    shape={img_arr_chw.shape}\")\n    print(f\"    dtype={img_arr_chw.dtype}\")\n    display(PIL.Image.fromarray(img_arr_chw.transpose(1, 2, 0)))\n    print(f\"  Target: {target}\")\n    label_index = np.argmax(target)\n    print(f\"    target index: {np.argmax(target)}\")\n    print(f\"    target label: {dataset.metadata['index2label'][label_index]}\")\n    print(\"  Metadata:\")\n    print(f\"    {datum_metadata}\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 385
    },
    {
      "source": "for i in [0, 3000, 6000]:\n    print_datum(test_dataset, i)\n    print()",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 401
    },
    {
      "source": "Datum 0\n  Input Image Array: [[[158 159 165 ... 137 126 116...\n    shape=(3, 32, 32)\n    dtype=uint8",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 410
    },
    {
      "source": "  Target: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n    target index: 3\n    target label: cat\n  Metadata:\n    {'id': 0}\n\nDatum 3000\n  Input Image Array: [[[17 17 18 ... 23 21 20]\n  [2...\n    shape=(3, 32, 32)\n    dtype=uint8",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 422
    },
    {
      "source": "  Target: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n    target index: 5\n    target label: dog\n  Metadata:\n    {'id': 3000}\n\nDatum 6000\n  Input Image Array: [[[201 211 211 ... 191 183 188...\n    shape=(3, 32, 32)\n    dtype=uint8",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 441
    },
    {
      "source": "Target: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n  target index: 8\n  target label: ship\nMetadata:\n  {'id': 6000}",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 459
    }
  ],
  "how_to/wrap_image_classification_model": [
    {
      "source": "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "import io\nimport json\nimport urllib.request\nfrom typing import Callable, Sequence\n\nimport numpy as np\nimport PIL.Image\nimport torch as pt\nimport torchvision\nfrom IPython.display import display\n\nimport maite.protocols.image_classification as ic\nfrom maite.protocols import ArrayLike, ModelMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "io"
          ],
          "code_str": "io",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "io"
        },
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "urllib",
            "request"
          ],
          "code_str": "urllib.request",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "urllib.request"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 28
    },
    {
      "source": "Python implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\njson       : 2.0.9\ntorchvision: 0.23.0\nIPython    : 8.37.0\nmaite      : 0.9.0\nnumpy      : 2.2.6\nPIL        : 11.3.0\ntorch      : 2.8.0",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 50
    },
    {
      "source": "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n\nmodel = torchvision.models.resnet50(\n    weights=model_weights\n)  # weights will download automatically\n\nmodel = model.eval()  # set the ResNet50 model to eval mode",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 68
    },
    {
      "source": "0%|          | 0.00/97.8M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 87
    },
    {
      "source": "15%|\u2588\u258c        | 14.9M/97.8M [00:00<00:00, 155MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 92
    },
    {
      "source": "35%|\u2588\u2588\u2588\u258d      | 34.0M/97.8M [00:00<00:00, 182MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 97
    },
    {
      "source": "55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 53.5M/97.8M [00:00<00:00, 192MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 102
    },
    {
      "source": "75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 73.4M/97.8M [00:00<00:00, 198MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 107
    },
    {
      "source": "95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 93.2M/97.8M [00:00<00:00, 202MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 112
    },
    {
      "source": ".. parsed-literal::\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": "labels_url = \"https://raw.githubusercontent.com/raghakot/keras-vis/refs/heads/master/resources/imagenet_class_index.json\"\nresponse = urllib.request.urlopen(labels_url).read()\nlabels = json.loads(response.decode(\"utf-8\"))\n\nimg_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01491361_tiger_shark.JPEG\"\nimage_data = urllib.request.urlopen(img_url).read()\nexample_img_1 = PIL.Image.open(io.BytesIO(image_data))\n\nimg_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01695060_Komodo_dragon.JPEG\"\nimage_data = urllib.request.urlopen(img_url).read()\nexample_img_2 = PIL.Image.open(io.BytesIO(image_data))\n\nexample_imgs = [example_img_1, example_img_2]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 129
    },
    {
      "source": "dict(list(labels.items())[:4])",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 150
    },
    {
      "source": "{'0': ['n01440764', 'tench'],\n '1': ['n01443537', 'goldfish'],\n '2': ['n01484850', 'great_white_shark'],\n '3': ['n01491361', 'tiger_shark']}",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 159
    },
    {
      "source": "def prediction_label(logits):\n    logits = logits.unsqueeze(0)\n    label_pred = logits.argmax().item()\n    return f\"{label_pred} {labels[str(label_pred)]}\"\n\n\npreprocess = model_weights.transforms()\n\nfor example_img in example_imgs:\n    input = preprocess(example_img)\n    logits = model(input.unsqueeze(0))  # use unsqueeze to add batch dimension\n\n    print(\n        f\"\"\"\n    ResNet50 Outputs\n    ================\n    Result Type: {type(logits)}\n    Result Shape: {logits.shape}\n    Sample Prediction: {prediction_label(logits)}\n    \"\"\"\n    )\n\n    display(example_img)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "type"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 174
    },
    {
      "source": "ResNet50 Outputs\n================\nResult Type: <class 'torch.Tensor'>\nResult Shape: torch.Size([1, 1000])\nSample Prediction: 3 ['n01491361', 'tiger_shark']",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 204
    },
    {
      "source": "ResNet50 Outputs\n================\nResult Type: <class 'torch.Tensor'>\nResult Shape: torch.Size([1, 1000])\nSample Prediction: 48 ['n01695060', 'Komodo_dragon']",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 219
    },
    {
      "source": "class TorchvisionResNetModel:\n    def __init__(\n        self,\n        model: torchvision.models.ResNet,\n        preprocess: Callable[[pt.Tensor], pt.Tensor],\n    ) -> None:\n        self.metadata: ModelMetadata = {\n            \"id\": \"Torchvision ResNet ImageNet 1k\",\n            \"index2label\": {\n                int(idx): label for idx, [_wordnetid, label] in labels.items()\n            },\n        }\n        self.model = model\n        self.preprocess = preprocess\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[pt.Tensor]:\n        # Preprocess the inputs to ensure they match the model's input format\n        imgs_chw = []\n        for img_chw in batch:\n            imgs_chw.append(self.preprocess(pt.as_tensor(img_chw)))\n\n        # Create a shape-(N,C,H,W) tensor from the list of (C,H,W) tensors\n        # Note: Images have been preprocessed to all have the same shape\n        img_nchw = pt.stack(imgs_chw)\n\n        # Call the model\n        logits = self.model(img_nchw)\n\n        # Convert the shape-(N,num_classes) logits tensor into a list of shape-(num_classes,) tensors\n        return [t for t in logits]\n\n\n# Wrap the Torchvision ResNet model\nwrapped_model: ic.Model = TorchvisionResNetModel(model, preprocess)",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "create-the-maite-model-wrapper",
        "headings": [
          "Wrap an Image Classification Model",
          "3 Create the MAITE Model Wrapper"
        ]
      },
      "doc_lineno": 261
    },
    {
      "source": "def pil_image_to_maite(img_pil):\n    # Convert the PIL image to a Numpy array\n    img_hwc = np.array(img_pil)  # shape (H, W, C)\n\n    # Use MAITE array index convention for representing images: (C, H, W)\n    img_chw = img_hwc.transpose(2, 0, 1)\n\n    return img_chw\n\n\nbatch = [pil_image_to_maite(i) for i in example_imgs]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 304
    },
    {
      "source": "predictions = wrapped_model(batch)\n\nprint(\n    f\"\"\"\nResNet50 Outputs\n================\nResult Type: {type(predictions)}\nIndividual Prediction Type: {type(predictions[0])}\nIndividual Prediction Shape: {pt.as_tensor(predictions[0]).shape}\nSample Predictions:\"\"\"\n)\nfor prediction, example_img in zip(predictions, example_imgs):\n    print(f\"    Predicted label = {prediction_label(prediction)}\")\n    display(example_img)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 318
    },
    {
      "source": "ResNet50 Outputs\n================\nResult Type: <class 'list'>\nIndividual Prediction Type: <class 'torch.Tensor'>\nIndividual Prediction Shape: torch.Size([1000])\nSample Predictions:\n    Predicted label = 3 ['n01491361', 'tiger_shark']",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 339
    },
    {
      "source": "Predicted label = 48 ['n01695060', 'Komodo_dragon']",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 354
    }
  ],
  "how_to/wrap_object_detection_dataset": [
    {
      "source": "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "import json\nimport pprint\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport matplotlib.pyplot as plt\nimport requests\nimport torch\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.ops.boxes import box_convert\nfrom torchvision.transforms.functional import pil_to_tensor\n\nfrom maite.protocols import DatasetMetadata, DatumMetadata\nfrom maite.protocols import object_detection as od\n\n%matplotlib inline\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "pprint"
          ],
          "code_str": "pprint",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "pprint"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "pathlib"
          ],
          "code_str": "pathlib",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "pathlib"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Any"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 43
    },
    {
      "source": "Python implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\njson       : 2.0.9\ntorchvision: 0.23.0\nrequests   : 2.32.5\nmatplotlib : 3.10.6\nmaite      : 0.9.0\ntorch      : 2.8.0",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 69
    },
    {
      "source": "def download_images(coco_json_subset: dict[str, Any], root: Path):\n    \"\"\"Download a subset of COCO images.\n\n    Parameters\n    ----------\n    coco_json_subset : dict[str, Any]\n        COCO val2017_first4 JSON file.\n    root : Path\n        Location of COCO data.\n    \"\"\"\n    root.mkdir(parents=True, exist_ok=True)\n\n    for image in coco_json_subset[\"images\"]:\n        url = image[\"coco_url\"]\n        filename = Path(root) / image[\"file_name\"]\n        if filename.exists():\n            print(f\"skipping {url}\")\n        else:\n            print(f\"saving {url} to {filename} ... \", end=\"\")\n            r = requests.get(url)\n            with open(filename, \"wb\") as f:\n                f.write(r.content)\n            print(\"done\")",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "open"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 96
    },
    {
      "source": "COCO_ROOT = Path(\"../sample_data/coco/coco_val2017_subset\")\ncoco_subset_json = dict()\nann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\ncoco_subset_json = json.load(open(ann_subset_file, \"r\"))\ndownload_images(coco_subset_json, root=COCO_ROOT)",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "open"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 122
    },
    {
      "source": "done",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 152
    },
    {
      "source": "tv_dataset = CocoDetection(\n    root=str(COCO_ROOT),\n    annFile=str(ann_subset_file),\n)\n\nprint(f\"\\n{len(tv_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 168
    },
    {
      "source": "loading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n\nlen(tv_dataset) = 4",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 180
    },
    {
      "source": "img, annotations = tv_dataset[0]\n\n# `img` is a PIL Image\nplt.imshow(img)\nplt.axis(\"off\")  # Optional: Hide the axes\nplt.show()\nprint(f\"{type(img) = }\")\n\n# `annotations` is a list of dictionaries (corresponding to 14 objects in this case)\nprint(f\"{len(annotations) = }\")\n\n# Each annotation dictionary contains the object's bounding box (bbox) plus other info\nprint(f\"{annotations[0].keys() = }\")\n\n# Note that the COCO bounding box format is [x_min, y_min, width, height]\nprint(f\"{annotations[0]['bbox'] = }\")\n\n# Class/category labels and ids are available via the `cats` attribute on the dataset itself\nprint(f\"{tv_dataset.coco.cats[1] = }\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 194
    },
    {
      "source": "type(img) = <class 'PIL.Image.Image'>\nlen(annotations) = 14\nannotations[0].keys() = dict_keys(['segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', 'id'])\nannotations[0]['bbox'] = [102.49, 118.47, 7.9, 17.31]\ntv_dataset.coco.cats[1] = {'supercategory': 'person', 'id': 1, 'name': 'person'}",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 223
    },
    {
      "source": "@dataclass\nclass CocoDetectionTarget:\n    boxes: torch.Tensor\n    labels: torch.Tensor\n    scores: torch.Tensor\n\n\nclass CocoDetectionDataset:\n    def __init__(self, dataset: CocoDetection, id: str):\n        self._dataset = dataset\n\n        # Get mapping from COCO category to name\n        index2label = {k: v[\"name\"] for k, v in dataset.coco.cats.items()}\n\n        # Add dataset-level metadata attribute, with required id and optional index2label mapping\n        self.metadata: DatasetMetadata = {\n            \"id\": id,\n            \"index2label\": index2label,\n        }\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[torch.Tensor, CocoDetectionTarget, DatumMetadata]:\n        # Get original data item\n        img_pil, annotations = self._dataset[index]\n\n        # Format input\n        img_pt = pil_to_tensor(img_pil)\n\n        # Format ground truth\n        num_boxes = len(annotations)\n        boxes = torch.zeros(num_boxes, 4)\n        for i, ann in enumerate(annotations):\n            bbox = torch.as_tensor(ann[\"bbox\"])\n            boxes[i, :] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n\n        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n        scores = torch.ones(num_boxes)\n\n        # Format metadata\n        datum_metadata: DatumMetadata = {\n            \"id\": self._dataset.ids[index],\n        }\n\n        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 25,
          "end_lineno": 25,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 36,
          "end_lineno": 36,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 283
    },
    {
      "source": "coco_dataset: od.Dataset = CocoDetectionDataset(tv_dataset, \"COCO Detection Subset\")",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 334
    },
    {
      "source": "print(f\"{len(coco_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 356
    },
    {
      "source": "len(coco_dataset) = 4",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 363
    },
    {
      "source": "pprint.pp(coco_dataset.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 368
    },
    {
      "source": "{'id': 'COCO Detection Subset',\n 'index2label': {1: 'person',\n                 2: 'bicycle',\n                 3: 'car',\n                 4: 'motorcycle',\n                 5: 'airplane',\n                 6: 'bus',\n                 7: 'train',\n                 8: 'truck',\n                 9: 'boat',\n                 10: 'traffic light',\n                 11: 'fire hydrant',\n                 13: 'stop sign',\n                 14: 'parking meter',\n                 15: 'bench',\n                 16: 'bird',\n                 17: 'cat',\n                 18: 'dog',\n                 19: 'horse',\n                 20: 'sheep',\n                 21: 'cow',\n                 22: 'elephant',\n                 23: 'bear',\n                 24: 'zebra',\n                 25: 'giraffe',\n                 27: 'backpack',\n                 28: 'umbrella',\n                 31: 'handbag',\n                 32: 'tie',\n                 33: 'suitcase',\n                 34: 'frisbee',\n                 35: 'skis',\n                 36: 'snowboard',\n                 37: 'sports ball',\n                 38: 'kite',\n                 39: 'baseball bat',\n                 40: 'baseball glove',\n                 41: 'skateboard',\n                 42: 'surfboard',\n                 43: 'tennis racket',\n                 44: 'bottle',\n                 46: 'wine glass',\n                 47: 'cup',\n                 48: 'fork',\n                 49: 'knife',\n                 50: 'spoon',\n                 51: 'bowl',\n                 52: 'banana',\n                 53: 'apple',\n                 54: 'sandwich',\n                 55: 'orange',\n                 56: 'broccoli',\n                 57: 'carrot',\n                 58: 'hot dog',\n                 59: 'pizza',\n                 60: 'donut',\n                 61: 'cake',\n                 62: 'chair',\n                 63: 'couch',\n                 64: 'potted plant',\n                 65: 'bed',\n                 67: 'dining table',\n                 70: 'toilet',\n                 72: 'tv',\n                 73: 'laptop',\n                 74: 'mouse',\n                 75: 'remote',\n                 76: 'keyboard',\n                 77: 'cell phone',\n                 78: 'microwave',\n                 79: 'oven',\n                 80: 'toaster',\n                 81: 'sink',\n                 82: 'refrigerator',\n                 84: 'book',\n                 85: 'clock',\n                 86: 'vase',\n                 87: 'scissors',\n                 88: 'teddy bear',\n                 89: 'hair drier',\n                 90: 'toothbrush'}}",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 375
    },
    {
      "source": "# Get first datum\nimage, target, datum_metadata = coco_dataset[0]\n\n# Display image\nplt.imshow(img)\nplt.axis(\"off\")  # Optional: Hide the axes\nplt.show()\n\n# Bridge/convert ArrayLike's to PyTorch tensors\nimage = torch.as_tensor(image)\nboxes = torch.as_tensor(target.boxes)\nlabels = torch.as_tensor(target.labels)\nscores = torch.as_tensor(target.scores)\n\n# Print shapes\nprint(f\"{image.shape = }\")  # image has height 230 and weight 352\nprint(f\"{boxes.shape = }\")  # there are 14 bounding boxes\nprint(f\"{labels.shape = }\")\nprint(f\"{scores.shape = }\")\n\n# Print datum-level metadata\nprint(f\"{datum_metadata = }\")  # this datum corresponds to image file 000000037777.jpg",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 460
    },
    {
      "source": "image.shape = torch.Size([3, 230, 352])\nboxes.shape = torch.Size([14, 4])\nlabels.shape = torch.Size([14])\nscores.shape = torch.Size([14])\ndatum_metadata = {'id': 37777}",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 492
    }
  ],
  "how_to/wrap_object_detection_model": [
    {
      "source": "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "import io\nimport urllib.request\nfrom dataclasses import dataclass\nfrom typing import Sequence\n\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom IPython.display import display\nfrom torchvision.models.detection import (\n    FasterRCNN,\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n    fasterrcnn_resnet50_fpn_v2,\n)\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image\nfrom torchvision.utils import draw_bounding_boxes\nfrom ultralytics import YOLO\n\nimport maite.protocols.object_detection as od\nfrom maite.protocols import ModelMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "io"
          ],
          "code_str": "io",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "io"
        },
        {
          "import_components": [
            "urllib",
            "request"
          ],
          "code_str": "urllib.request",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "urllib.request"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "numpy"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 32
    },
    {
      "source": "Python implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\ntorchvision: 0.23.0\nnumpy      : 2.2.6\nultralytics: 8.3.203\nIPython    : 8.37.0\nmaite      : 0.9.0\ntorch      : 2.8.0\nPIL        : 11.3.0",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 68
    },
    {
      "source": "# Load R-CNN model\nrcnn_weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\nrcnn_model = fasterrcnn_resnet50_fpn_v2(weights=rcnn_weights, box_score_thresh=0.9)\nrcnn_model.eval()  # set the RCNN to eval mode (defaults to training)\n\n# Load YOLOv8 Nano model\nyolov8_model = YOLO(\"yolov8n.pt\")  # weights will download automatically",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 90
    },
    {
      "source": "0%|          | 0.00/167M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 109
    },
    {
      "source": "8%|\u258a         | 13.1M/167M [00:00<00:01, 136MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 114
    },
    {
      "source": "17%|\u2588\u258b        | 27.9M/167M [00:00<00:00, 147MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 119
    },
    {
      "source": "31%|\u2588\u2588\u2588       | 51.2M/167M [00:00<00:00, 191MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 124
    },
    {
      "source": "45%|\u2588\u2588\u2588\u2588\u258d     | 74.4M/167M [00:00<00:00, 211MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 129
    },
    {
      "source": "58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 97.5M/167M [00:00<00:00, 222MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 134
    },
    {
      "source": "72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 121M/167M [00:00<00:00, 229MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 139
    },
    {
      "source": "86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 144M/167M [00:00<00:00, 233MB/s]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 144
    },
    {
      "source": ".. parsed-literal::\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": ".. parsed-literal::\n\n\n\n",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": null
    },
    {
      "source": "img_url = \"https://github.com/pytorch/vision/blob/main/test/assets/encode_jpeg/grace_hopper_517x606.jpg?raw=true\"\nimage_data = urllib.request.urlopen(img_url).read()\npil_img_1 = PIL.Image.open(io.BytesIO(image_data))\n\nimg_url = \"https://www.ultralytics.com/images/bus.jpg\"\nimage_data = urllib.request.urlopen(img_url).read()\npil_img_2 = PIL.Image.open(io.BytesIO(image_data))",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 188
    },
    {
      "source": "# Convert to PyTorch tensors\ntensor_img_1 = pil_to_tensor(pil_img_1)\ntensor_img_2 = pil_to_tensor(pil_img_2)\nrcnn_imgs = [tensor_img_1, tensor_img_2]\n\n# Get the inference transforms assocated with these pretrained weights\npreprocess = rcnn_weights.transforms()\n\n# Apply inference preprocessing transforms\nbatch = [preprocess(img) for img in rcnn_imgs]\n\nrcnn_preds = rcnn_model(batch)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 200
    },
    {
      "source": "yolo_imgs = [pil_img_1, pil_img_2]\n\nyolo_preds = yolov8_model(yolo_imgs, verbose=False)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 217
    },
    {
      "source": "rcnn_pred = rcnn_preds[0]\nyolo_pred = yolo_preds[0]\n\nrcnn_boxes = rcnn_pred[\"boxes\"]\nyolo_boxes = yolo_pred.boxes\n\nprint(\n    f\"\"\"\nR-CNN Outputs\n=============\nResult Type: {type(rcnn_pred)}\nResult Attributes: {rcnn_pred.keys()}\nBox Types: {type(rcnn_boxes)}\n\nYOLO Outputs\n============\nResult Type: {type(yolo_pred)}\nResult Attributes: {yolo_pred._keys}\nBox Types: {type(yolo_boxes)}\n\"\"\"\n)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "type"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 225
    },
    {
      "source": "R-CNN Outputs\n=============\nResult Type: <class 'dict'>\nResult Attributes: dict_keys(['boxes', 'labels', 'scores'])\nBox Types: <class 'torch.Tensor'>\n\nYOLO Outputs\n============\nResult Type: <class 'ultralytics.engine.results.Results'>\nResult Attributes: ('boxes', 'masks', 'probs', 'keypoints', 'obb')\nBox Types: <class 'ultralytics.engine.results.Boxes'>",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 253
    },
    {
      "source": "imgs: Sequence[od.InputType] = [tensor_img_1, tensor_img_2]\n\n\n@dataclass\nclass ObjectDetectionTargetImpl:\n    boxes: np.ndarray\n    labels: np.ndarray\n    scores: np.ndarray\n\n\ndef render_wrapped_results(imgs, preds, model_metadata):\n    names = model_metadata[\"index2label\"]\n    for img, pred in zip(imgs, preds):\n        pred_labels = [names[label] for label in pred.labels]\n        box = draw_bounding_boxes(\n            img,\n            boxes=torch.as_tensor(pred.boxes),\n            labels=pred_labels,\n            colors=\"red\",\n            width=4,\n            font=\"DejaVuSans\",  # if necessary, change to TrueType font available on your system\n            font_size=30,\n        )\n        im = to_pil_image(box.detach())\n        h, w = im.size\n        im = im.resize((h // 2, w // 2))\n        display(im)",
      "names": [
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "zip"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "create-maite-wrappers-for-the-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 299
    },
    {
      "source": "class WrappedRCNN:\n    def __init__(\n        self,\n        model: FasterRCNN,\n        weights: FasterRCNN_ResNet50_FPN_V2_Weights,\n        id: str,\n        **kwargs,\n    ):\n        self.model = model\n        self.weights = weights\n        self.kwargs = kwargs\n\n        # Add required model metadata attribute\n        index2label = {\n            i: category for i, category in enumerate(weights.meta[\"categories\"])\n        }\n        self.metadata: ModelMetadata = {\"id\": id, \"index2label\": index2label}\n\n    def __call__(\n        self, batch: Sequence[od.InputType]\n    ) -> Sequence[ObjectDetectionTargetImpl]:\n        # Get MAITE inputs ready for native model\n        preprocess = self.weights.transforms()\n        batch = [preprocess(img) for img in batch]\n\n        # Perform inference\n        results = self.model(batch, **self.kwargs)\n\n        # Restructure results to conform to MAITE\n        all_detections = []\n        for result in results:\n            boxes = result[\"boxes\"].detach().numpy()\n            labels = result[\"labels\"].detach().numpy()\n            scores = result[\"scores\"].detach().numpy()\n            all_detections.append(\n                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n            )\n\n        return all_detections\n\n\nwrapped_rcnn_model: od.Model = WrappedRCNN(\n    rcnn_model, rcnn_weights, \"TorchVision.FasterRCNN_ResNet50_FPN_V2\"\n)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 339
    },
    {
      "source": "wrapped_rcnn_preds = wrapped_rcnn_model(imgs)\nwrapped_rcnn_preds",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 386
    },
    {
      "source": "[ObjectDetectionTargetImpl(boxes=array([[     5.1904,       40.48,      513.56,      601.49],\n        [     215.86,      414.02,      297.24,      482.01]], dtype=float32), labels=array([ 1, 32]), scores=array([    0.99896,     0.97159], dtype=float32)),\n ObjectDetectionTargetImpl(boxes=array([[     53.438,      401.78,      235.58,      907.62],\n        [     21.099,       224.5,       802.2,      756.62],\n        [     220.23,      404.51,      347.92,      860.75],\n        [     668.02,      399.47,         810,       881.8],\n        [    0.87665,      544.77,      75.116,      878.61]], dtype=float32), labels=array([1, 6, 1, 1, 1]), scores=array([    0.99916,     0.99915,     0.99888,     0.99819,     0.98551], dtype=float32))]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 396
    },
    {
      "source": "render_wrapped_results(imgs, wrapped_rcnn_preds, wrapped_rcnn_model.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 406
    },
    {
      "source": "class WrappedYOLO:\n    def __init__(self, model: YOLO, id: str, **kwargs):\n        self.model = model\n        self.kwargs = kwargs\n\n        # Add required model metadata attribute\n        self.metadata: ModelMetadata = {\n            \"id\": id,\n            \"index2label\": model.names,  # already a mapping from integer class index to string name\n        }\n\n    def __call__(\n        self, batch: Sequence[od.InputType]\n    ) -> Sequence[ObjectDetectionTargetImpl]:\n        # Get MAITE inputs ready for native model\n        # Bridge/convert input to np.ndarray and tranpose (C, H, W) -> (H, W, C)\n        batch = [np.asarray(x).transpose((1, 2, 0)) for x in batch]\n\n        # Perform inference\n        results = self.model(batch, **self.kwargs)\n\n        # Restructure results to conform to MAITE\n        all_detections = []\n        for result in results:\n            detections = result.boxes\n            if detections is None:\n                continue\n            detections = detections.cpu().numpy()\n            boxes = np.asarray(detections.xyxy)\n            labels = np.asarray(detections.cls, dtype=np.uint8)\n            scores = np.asarray(detections.conf)\n            all_detections.append(\n                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n            )\n\n        return all_detections\n\n\nwrapped_yolov8_model: od.Model = WrappedYOLO(\n    yolov8_model, id=\"Ultralytics.YOLOv8\", verbose=False\n)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 439
    },
    {
      "source": "wrapped_yolo_preds = wrapped_yolov8_model(imgs)\nwrapped_yolo_preds",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 485
    },
    {
      "source": "[ObjectDetectionTargetImpl(boxes=array([[     9.0822,      20.553,         517,      604.41],\n        [     218.77,      414.25,      300.06,      538.04],\n        [     199.25,      414.16,      300.59,      605.61]], dtype=float32), labels=array([ 0, 27, 27], dtype=uint8), scores=array([    0.89559,     0.60285,     0.47824], dtype=float32)),\n ObjectDetectionTargetImpl(boxes=array([[     50.009,      397.59,      243.39,      905.03],\n        [     223.38,      405.18,      344.82,      857.35],\n        [     670.62,      378.47,      809.87,      875.35],\n        [     29.816,      228.97,      797.11,      751.33],\n        [    0.15491,       550.5,      61.616,      870.62]], dtype=float32), labels=array([0, 0, 0, 5, 0], dtype=uint8), scores=array([    0.87989,     0.87631,     0.86818,     0.84481,     0.44715], dtype=float32))]",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 495
    },
    {
      "source": "render_wrapped_results(imgs, wrapped_yolo_preds, wrapped_yolov8_model.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 506
    },
    {
      "source": "# Get predictions from both models for first image in batch\nwrapped_rcnn_pred = wrapped_rcnn_preds[0]\nwrapped_yolo_pred = wrapped_yolo_preds[0]\n\nwrapped_rcnn_fields = vars(wrapped_rcnn_pred).keys()\nwrapped_yolo_fields = vars(wrapped_yolo_pred).keys()\n\nprint(\n    f\"\"\"\nWrapped R-CNN Outputs\n=====================\nResult Type: {type(wrapped_rcnn_pred)}\nResult Attributes: {wrapped_rcnn_fields}\n\nYOLO Outputs\n============\nResult Type: {type(wrapped_yolo_pred)}\nResult Attributes: {wrapped_yolo_fields}\n\"\"\"\n)",
      "names": [
        {
          "import_components": [
            "vars"
          ],
          "code_str": "vars",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "vars"
        },
        {
          "import_components": [
            "vars"
          ],
          "code_str": "vars",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "vars"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "type"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 519
    },
    {
      "source": "Wrapped R-CNN Outputs\n=====================\nResult Type: <class '__main__.ObjectDetectionTargetImpl'>\nResult Attributes: dict_keys(['boxes', 'labels', 'scores'])\n\nYOLO Outputs\n============\nResult Type: <class '__main__.ObjectDetectionTargetImpl'>\nResult Attributes: dict_keys(['boxes', 'labels', 'scores'])",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 546
    }
  ],
  "how_tos": [],
  "index": [],
  "tutorials": [],
  "tutorials/hf_image_classification": [
    {
      "source": "Copyright 2024, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "conda create --name hf_image_classification python=3.10 pip\nconda activate hf_image_classification\npip install maite datasets jupyter matplotlib torch torchmetrics torchvision transformers watermark",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 43
    },
    {
      "source": "from typing import Any, Optional, Sequence\n\nimport datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torchmetrics import Accuracy, Metric\nfrom torchvision.transforms.functional import resize, to_tensor\nfrom transformers import AutoModelForImageClassification, ViTForImageClassification\n\nimport maite.protocols.image_classification as ic\nfrom maite.protocols import ArrayLike, DatasetMetadata, MetricMetadata, ModelMetadata\nfrom maite.tasks import evaluate",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "torchmetrics",
            "Accuracy"
          ],
          "code_str": "Accuracy",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_target",
          "resolved_location": "torchmetrics.classification.accuracy.Accuracy"
        },
        {
          "import_components": [
            "torchmetrics",
            "Metric"
          ],
          "code_str": "Metric",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_target",
          "resolved_location": "torchmetrics.metric.Metric"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 12,
          "end_lineno": 12,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "maite",
            "tasks",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_target",
          "resolved_location": "maite.tasks.evaluate"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 49
    },
    {
      "source": "from watermark import watermark\n\nprint(\"This notebook was executed with the following:\\n\")\nprint(\n    watermark(\n        python=True,\n        packages=\"datasets,jupyter,matplotlib,numpy,torch,torchmetrics,torchvision,transformers,watermark\",\n    )\n)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 65
    },
    {
      "source": "This notebook was executed with the following:\n\nPython implementation: CPython\nPython version       : 3.10.18\nIPython version      : 8.37.0\n\ndatasets    : 4.1.1\njupyter     : 1.1.1\nmatplotlib  : 3.10.6\nnumpy       : 2.2.6\ntorch       : 2.8.0\ntorchmetrics: 1.8.2\ntorchvision : 0.23.0\ntransformers: 4.56.2\nwatermark   : 2.5.0",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 80
    },
    {
      "source": "subset_size = 256\nhf_dataset: datasets.Dataset = datasets.load_dataset(\n    \"cifar10\", split=f\"test[:{subset_size}]\"\n)  # type: ignore",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 113
    },
    {
      "source": "README.md: 0.00B [00:00, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 124
    },
    {
      "source": "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 130
    },
    {
      "source": "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 136
    },
    {
      "source": "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 142
    },
    {
      "source": "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 148
    },
    {
      "source": "class HuggingFaceDataset:\n    def __init__(\n        self,\n        hf_dataset: datasets.Dataset,\n        id: str,\n        index2label: dict[int, str],\n        resize_shape: Optional[list[int]] = None,\n    ):\n        self.hf_dataset = hf_dataset\n        self.num_classes = hf_dataset.features[\"label\"].num_classes\n        self.resize_shape = resize_shape\n\n        # Create required dataset metadata attribute\n        self.metadata: DatasetMetadata = DatasetMetadata(id=id, index2label=index2label)\n\n    def __len__(self) -> int:\n        return len(self.hf_dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[torch.Tensor, torch.Tensor, ic.DatumMetadataType]:\n        if index < 0 or index >= len(self):\n            raise IndexError(\n                f\"Index {index} is out of range for the dataset, which has length {len(self)}.\"\n            )\n\n        # Get the PIL image and integer label from the base HF dataset element (which is a dictionary)\n        item = self.hf_dataset[index]\n        img_pil = item[\"img\"]\n        label = item[\"label\"]\n\n        # Convert the PIL image to a PyTorch tensor for compatibility with PyTorch libraries\n        img_pt = to_tensor(img_pil)\n\n        # Apply resizing if requested\n        if self.resize_shape is not None:\n            img_pt = resize(img_pt, self.resize_shape)\n\n        # Create one-hot encoded tensor with true class label for this image\n        target = torch.zeros(self.num_classes)\n        target[label] = 1\n\n        return img_pt, target, ic.DatumMetadataType(id=index)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "IndexError"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 168
    },
    {
      "source": "# Create map from integer class index to string label\nnum_classes = hf_dataset.features[\"label\"].num_classes\nindex2label = {i: hf_dataset.features[\"label\"].int2str(i) for i in range(num_classes)}\n\n# Wrap dataset\nwrapped_hf_dataset: ic.Dataset = HuggingFaceDataset(\n    hf_dataset, id=\"CIFAR-10\", index2label=index2label, resize_shape=[224, 224]\n)\n\nprint(f\"{len(wrapped_hf_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 224
    },
    {
      "source": "len(wrapped_hf_dataset) = 256",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 240
    },
    {
      "source": "ncols = 6\nfig, ax = plt.subplots(1, ncols, figsize=(6, 2))\nfor i in range(ncols):\n    # Get datum i\n    img, label_onehot, md = wrapped_hf_dataset[i]\n\n    # Convert to NumPy array in height, width, color channel (HWC) order (for display with matplotlib)\n    img_np = np.asarray(img).transpose(1, 2, 0)\n\n    # Get ground truth class index and label\n    index = torch.as_tensor(label_onehot).argmax().item()\n    label = index2label[int(index)]\n\n    # Plot image with label\n    ax[i].axis(\"off\")\n    ax[i].imshow(img_np)\n    ax[i].set_title(label)\n\nfig.tight_layout()",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 246
    },
    {
      "source": "hf_model: ViTForImageClassification = AutoModelForImageClassification.from_pretrained(\n    \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\"\n)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 283
    },
    {
      "source": "config.json: 0.00B [00:00, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 293
    },
    {
      "source": "pytorch_model.bin:   0%|          | 0.00/343M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 299
    },
    {
      "source": "class HuggingFaceModel:\n    def __init__(\n        self,\n        hf_model: ViTForImageClassification,\n        id: str,\n        index2label: dict[int, str],\n        device: str = \"cpu\",\n    ):\n        self.hf_model = hf_model\n        self.device = device\n\n        # Create required model metadata attribute\n        self.metadata: ModelMetadata = ModelMetadata(id=id, index2label=index2label)\n\n        # Move the model to requested device and set to eval mode\n        self.hf_model.to(device)  # type: ignore\n        self.hf_model.eval()\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[torch.Tensor]:\n        # Combine inputs into PyTorch tensor of shape-(N,C,H,W) (batch size, color channels, height, width)\n        batch_pt = torch.stack([torch.as_tensor(x) for x in batch])\n\n        # Move tensor to the desired device\n        batch_pt = batch_pt.to(self.device)\n\n        # Apply model to batch (NOTE: preprocessing not needed for this particular HF model)\n        output = self.hf_model(batch_pt)\n\n        # Restructure to expected output format (sequence of probability/logit vectors)\n        result = [x for x in output.logits.detach().cpu()]\n        return result",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 308
    },
    {
      "source": "wrapped_hf_model: ic.Model = HuggingFaceModel(\n    hf_model, id=\"vit-base-patch16-224-in21k-finetuned-cifar10\", index2label=index2label\n)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 342
    },
    {
      "source": "# Create batch with single image\ni = 0\nx, y, md = wrapped_hf_dataset[i]\nxb, yb, mdb = [x], [y], [md]\n\n# Apply model and get first (only) prediction of size-1 batch of results\npreds = wrapped_hf_model(xb)[0]\ny_hat = torch.as_tensor(preds).argmax().item()\n\n# Plot image with model prediction\nfig, ax = plt.subplots(figsize=(1.5, 1.5))\nimg_np = np.asarray(x).transpose(1, 2, 0)\nax.axis(\"off\")\nax.imshow(img_np)\nax.set_title(f\"pred: {index2label[int(y_hat)]}\")\nfig.tight_layout()",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 351
    },
    {
      "source": "tm_acc: Metric = Accuracy(task=\"multiclass\", num_classes=10)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 387
    },
    {
      "source": "class TorchMetricsClassificationMetric:\n    def __init__(self, tm_metric: Metric, name: str, device: str = \"cpu\"):\n        self.tm_metric = tm_metric\n        self.name = name\n        self.device = device\n\n        # Create required metric metadata attribute\n        self.metadata: MetricMetadata = MetricMetadata(id=name)\n\n    def reset(self):\n        self.tm_metric.reset()\n\n    def update(\n        self,\n        pred_batch: Sequence[ArrayLike],\n        target_batch: Sequence[ArrayLike],\n        metadata_batch: Sequence[ic.DatumMetadataType],\n    ) -> None:\n        # Convert inputs to PyTorch tensors of shape-(N, num_classes)\n        preds_pt: torch.Tensor = torch.stack(\n            [torch.as_tensor(x) for x in pred_batch]\n        ).to(self.device)\n        assert preds_pt.ndim == 2\n\n        targets_pt: torch.Tensor = torch.stack(\n            [torch.as_tensor(x) for x in target_batch]\n        ).to(self.device)\n        assert targets_pt.ndim == 2\n\n        # Convert probabilities/logits to predicted class indices and update native TorchMetrics metric\n        self.tm_metric.update(preds_pt.argmax(dim=1), targets_pt.argmax(dim=1))\n\n    def compute(self) -> dict[str, Any]:\n        result = {}\n        result[self.name] = self.tm_metric.compute()\n        return result",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 33,
          "end_lineno": 33,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 33,
          "end_lineno": 33,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 395
    },
    {
      "source": "wrapped_tm_acc: ic.Metric = TorchMetricsClassificationMetric(tm_acc, \"accuracy\")",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 434
    },
    {
      "source": "results, _, _ = evaluate(\n    dataset=wrapped_hf_dataset, model=wrapped_hf_model, metric=wrapped_tm_acc\n)\n\nresults",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "procedures",
        "headings": [
          "Hugging Face Image Classification Example",
          "Procedures"
        ]
      },
      "doc_lineno": 445
    },
    {
      "source": "0%|          | 0/256 [00:00<?, ?it/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "procedures",
        "headings": [
          "Hugging Face Image Classification Example",
          "Procedures"
        ]
      },
      "doc_lineno": 457
    },
    {
      "source": "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "procedures",
        "headings": [
          "Hugging Face Image Classification Example",
          "Procedures"
        ]
      },
      "doc_lineno": 463
    },
    {
      "source": "{'accuracy': tensor(0.9531)}",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "procedures",
        "headings": [
          "Hugging Face Image Classification Example",
          "Procedures"
        ]
      },
      "doc_lineno": 470
    }
  ],
  "tutorials/torchvision_object_detection": [
    {
      "source": "Copyright 2024, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\nSubject to FAR 52.227-11 \u2013 Patent Rights \u2013 Ownership by the Contractor (May 2014).\nSPDX-License-Identifier: MIT",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": null,
        "headings": []
      },
      "doc_lineno": 3
    },
    {
      "source": "conda create --name torchvision_obj_det python=3.10\nconda activate torchvision_obj_det\npip install maite jupyter torch torchvision torchmetrics pycocotools kornia",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "getting-started",
        "headings": [
          "Torchvision Object Detection Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 44
    },
    {
      "source": "pip install maite jupyter==1.0.0 torch==2.3.1 torchvision==0.18.1 torchmetrics==1.4.0.post0 pycocotools==2.0.7 kornia==0.7.2",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "getting-started",
        "headings": [
          "Torchvision Object Detection Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 52
    },
    {
      "source": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Sequence\n\nimport kornia.augmentation as K\nimport matplotlib.pyplot as plt\nimport requests\nimport torch\nfrom PIL import Image\nfrom torchmetrics import Metric as tmMetric\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.models.detection import (\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n    fasterrcnn_resnet50_fpn_v2,\n)\nfrom torchvision.ops.boxes import box_convert\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image\nfrom torchvision.utils import draw_bounding_boxes\n\nimport maite.protocols.object_detection as od\nfrom maite.protocols import (\n    ArrayLike,\n    AugmentationMetadata,\n    DatasetMetadata,\n    DatumMetadata,\n    MetricMetadata,\n    ModelMetadata,\n)\nfrom maite.tasks import evaluate",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "__future__",
            "annotations"
          ],
          "code_str": "annotations",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "__future__.annotations"
        },
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "pathlib"
          ],
          "code_str": "pathlib",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "pathlib"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "torchmetrics",
            "Metric"
          ],
          "code_str": "Metric",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_target",
          "resolved_location": "torchmetrics.metric.Metric"
        },
        {
          "import_components": [
            "torchmetrics",
            "detection",
            "mean_ap",
            "MeanAveragePrecision"
          ],
          "code_str": "MeanAveragePrecision",
          "lineno": 14,
          "end_lineno": 14,
          "context": "import_target",
          "resolved_location": "torchmetrics.detection.mean_ap.MeanAveragePrecision"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 25,
          "end_lineno": 32,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "maite",
            "tasks",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 33,
          "end_lineno": 33,
          "context": "import_target",
          "resolved_location": "maite.tasks.evaluate"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "getting-started",
        "headings": [
          "Torchvision Object Detection Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 57
    },
    {
      "source": "def download_images(coco_json_subset: dict[str, Any], root: str | Path):\n    root = Path(root)\n    root.mkdir(parents=True, exist_ok=True)\n\n    for image in coco_json_subset[\"images\"]:\n        url = image[\"coco_url\"]\n        filename = Path(root) / image[\"file_name\"]\n        if filename.exists():\n            print(f\"skipping {url}\")\n        else:\n            print(f\"saving {url} to {filename} ... \", end=\"\")\n            r = requests.get(url)\n            with open(filename, \"wb\") as f:\n                f.write(r.content)\n            print(\"done\")\n\n\nCOCO_ROOT = Path(\"coco_val2017_subset\")\nann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\ncoco_subset_json = json.load(open(ann_subset_file, \"r\"))\n\ndownload_images(coco_subset_json, root=COCO_ROOT)",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "open"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "open"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 116
    },
    {
      "source": "done",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 163
    },
    {
      "source": "tv_dataset = CocoDetection(\n    root=\"coco_val2017_subset\",\n    annFile=\"coco_val2017_subset/instances_val2017_first4.json\",\n)\n\nprint(f\"\\n{len(tv_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 169
    },
    {
      "source": "loading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n\nlen(tv_dataset) = 4",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 181
    },
    {
      "source": "# Get first image and its annotations\nimg, annotations = tv_dataset[0]\n\n# Explore structure\nprint(f\"{type(img) = }\")\nprint(f\"{type(annotations) = }\")\nprint(f\"{type(annotations[0]) = }\")\nprint(f\"{len(annotations[0]) = }\")\nprint(f\"{annotations[0].keys() = }\")\nprint(f\"{annotations[0]['bbox'] = }\")\nprint(f\"{annotations[0]['category_id'] = }\")\nprint(f\"{tv_dataset.coco.cats[64] = }\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 194
    },
    {
      "source": "type(img) = <class 'PIL.Image.Image'>\ntype(annotations) = <class 'list'>\ntype(annotations[0]) = <class 'dict'>\nlen(annotations[0]) = 7\nannotations[0].keys() = dict_keys(['segmentation', 'area', 'iscrowd', 'image_id', 'bbox', 'category_id', 'id'])\nannotations[0]['bbox'] = [102.49, 118.47, 7.9, 17.31]\nannotations[0]['category_id'] = 64\ntv_dataset.coco.cats[64] = {'supercategory': 'furniture', 'id': 64, 'name': 'potted plant'}",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 212
    },
    {
      "source": "@dataclass\nclass CocoDetectionTarget:\n    boxes: torch.Tensor\n    labels: torch.Tensor\n    scores: torch.Tensor\n\n\n# Get mapping from COCO category to name\nindex2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}\n\n\nclass MaiteCocoDetection:\n    metadata: DatasetMetadata = {\"id\": \"COCO Detection Dataset\"}\n    # We can optionally include index2label mapping within DatasetMetadata\n    metadata: DatasetMetadata = {**metadata, \"index2label\": index2label}\n\n    def __init__(self, dataset: CocoDetection):\n        self._dataset = dataset\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[ArrayLike, CocoDetectionTarget, DatumMetadata]:\n        try:\n            # get original data item\n            img_pil, annotations = self._dataset[index]\n        except IndexError as e:\n            # Here the underlying dataset is raising an IndexError since the index is beyond the\n            # container's bounds. When wrapping custom datasets, wrappers are responsible to for\n            # raising an IndexError in `__getitem__` when an index exceeds the container's bounds;\n            # this enables iteration on the wrapper to properly terminate.\n            print(\n                f\"The index number {index} is out of range for the dataset which has length {len(self._dataset)}\"\n            )\n            raise (e)\n\n        # format input\n        img_pt = pil_to_tensor(img_pil)\n\n        # format ground truth\n        num_boxes = len(annotations)\n        boxes = torch.zeros(num_boxes, 4)\n        for i, ann in enumerate(annotations):\n            bbox = torch.as_tensor(ann[\"bbox\"])\n            boxes[i, :] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n\n        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n        scores = torch.ones(num_boxes)\n\n        # format metadata\n        datum_metadata: DatumMetadata = {\n            \"id\": self._dataset.ids[index],\n        }\n\n        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 25,
          "end_lineno": 25,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "IndexError"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 43,
          "end_lineno": 43,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 45,
          "end_lineno": 45,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 244
    },
    {
      "source": "dataset: od.Dataset = MaiteCocoDetection(tv_dataset)\nlen(dataset)",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 314
    },
    {
      "source": "4",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 324
    },
    {
      "source": "def create_pil_image(\n    input: ArrayLike,\n    target: od.ObjectDetectionTarget,\n    index2label: dict[int, str],\n    color: str = \"red\",\n) -> Image.Image:\n    img_pt = torch.as_tensor(input)\n    boxes = torch.as_tensor(target.boxes)\n    label_ids = torch.as_tensor(target.labels)\n    label_names = [index2label[int(id.item())] for id in label_ids]\n    box = draw_bounding_boxes(\n        img_pt, boxes=boxes, labels=label_names, colors=color, width=2\n    )\n    return to_pil_image(box.detach())",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 331
    },
    {
      "source": "# Get mapping from COCO category to name\nindex2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 348
    },
    {
      "source": "# Get sample image and overlay ground truth annotations (bounding boxes)\ni = 0\ninput, target, _ = dataset[i]\nimg = create_pil_image(input, target, index2label)\n\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Ground Truth\")\nax.imshow(img);",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 353
    },
    {
      "source": "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\ntv_model = fasterrcnn_resnet50_fpn_v2(\n    weights=weights, box_score_thresh=0.9, progress=False\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 379
    },
    {
      "source": "class TorchvisionDetector:\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        metadata: ModelMetadata,\n        transforms: Any,\n        device: str,\n    ):\n        self.model = model\n        self.metadata = metadata\n        self.transforms = transforms\n        self.device = device\n\n        self.model.eval()\n        self.model.to(device)\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[CocoDetectionTarget]:\n        # convert to list of tensors, transfer to device, and apply inference transforms\n        # - https://pytorch.org/vision/stable/models.html\n        # - \"The models expect a list of Tensor[C, H, W].\"\n        tv_input = [\n            self.transforms(torch.as_tensor(b_elem)).to(self.device) for b_elem in batch\n        ]\n\n        # get predictions\n        tv_predictions = self.model(tv_input)\n\n        # reformat output\n        predictions = [\n            CocoDetectionTarget(\n                p[\"boxes\"].detach().cpu(),\n                p[\"labels\"].detach().cpu(),\n                p[\"scores\"].detach().cpu(),\n            )\n            for p in tv_predictions\n        ]\n\n        return predictions",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 390
    },
    {
      "source": "model: od.Model = TorchvisionDetector(\n    model=tv_model,\n    metadata={\"id\": \"TorchvisionDetector\", \"index2label\": index2label},\n    transforms=weights.transforms(),\n    device=\"cpu\",\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 431
    },
    {
      "source": "# Create batch with sample image\ni = 0\nx, y, md = dataset[i]\nx = torch.as_tensor(x)\nxb, yb, mdb = x.unsqueeze(0), [y], [md]\nprint(f\"{xb.shape = }\")\n\n# Get predictions for batch(which just has one image for this example)\npreds = model([xb[0]])\n\n# Overlay detections on image\nimg = create_pil_image(xb[0], preds[0], index2label)\n\n# Plot\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Prediction\")\nax.imshow(img);",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 443
    },
    {
      "source": "xb.shape = torch.Size([1, 3, 230, 352])",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 467
    },
    {
      "source": "tm_metric = MeanAveragePrecision(\n    box_format=\"xyxy\",\n    iou_type=\"bbox\",\n    iou_thresholds=[0.5],\n    rec_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    max_detection_thresholds=[1, 10, 100],\n    class_metrics=False,\n    extended_summary=False,\n    average=\"macro\",\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 489
    },
    {
      "source": "class WrappedTorchmetricsMetric:\n    def __init__(self, tm_metric: tmMetric, metadata: MetricMetadata):\n        self._tm_metric = tm_metric\n        self.metadata = metadata\n\n    # Create utility function to convert ObjectDetectionTarget_impl type to what\n    # the type expected by torchmetrics IntersectionOverUnion metric\n    @staticmethod\n    def to_tensor_dict(target: od.ObjectDetectionTarget) -> dict[str, torch.Tensor]:\n        \"\"\"\n        Convert an ObjectDetectionTarget_impl into a dictionary expected internally by\n        raw `update` method of raw torchmetrics method\n        \"\"\"\n        out = {\n            \"boxes\": torch.as_tensor(target.boxes),\n            \"scores\": torch.as_tensor(target.scores),\n            \"labels\": torch.as_tensor(target.labels),\n        }\n\n        return out\n\n    def update(\n        self,\n        pred_batch: Sequence[od.TargetType],\n        target_batch: Sequence[od.TargetType],\n        metadata_batch: Sequence[od.DatumMetadataType],\n    ) -> None:\n        # Convert to natively-typed from of preds/targets\n        preds_tm = [self.to_tensor_dict(pred) for pred in pred_batch]\n        targets_tm = [self.to_tensor_dict(tgt) for tgt in target_batch]\n        self._tm_metric.update(preds_tm, targets_tm)\n\n    def compute(self) -> dict[str, Any]:\n        return self._tm_metric.compute()\n\n    def reset(self) -> None:\n        self._tm_metric.reset()",
      "names": [
        {
          "import_components": [
            "staticmethod"
          ],
          "code_str": "staticmethod",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "staticmethod"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 33,
          "end_lineno": 33,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 33,
          "end_lineno": 33,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 505
    },
    {
      "source": "mAP_metric: od.Metric = WrappedTorchmetricsMetric(\n    tm_metric, metadata={\"id\": \"torchmetrics_map_metric\"}\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 545
    },
    {
      "source": "# Run evaluate over original (clean) dataset\nresults, _, _ = evaluate(model=model, dataset=dataset, metric=mAP_metric)\n\n# Report mAP_50 performance\nresults[\"map_50\"]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "procedures",
        "headings": [
          "Torchvision Object Detection Example",
          "Procedures"
        ]
      },
      "doc_lineno": 558
    },
    {
      "source": "0%|          | 0/4 [00:00<?, ?it/s]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "procedures",
        "headings": [
          "Torchvision Object Detection Example",
          "Procedures"
        ]
      },
      "doc_lineno": 570
    },
    {
      "source": "tensor(0.3713)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "procedures",
        "headings": [
          "Torchvision Object Detection Example",
          "Procedures"
        ]
      },
      "doc_lineno": 577
    },
    {
      "source": "kornia_noise = K.RandomGaussianNoise(\n    mean=0.0,\n    std=0.08,  # relative to [0, 1] pixel values\n    p=1.0,\n    keepdim=True,\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 594
    },
    {
      "source": "class WrappedKorniaAugmentation:\n    def __init__(self, kornia_aug: Any, metadata: AugmentationMetadata):\n        self.kornia_aug = kornia_aug\n        self.metadata = metadata\n\n    def __call__(\n        self,\n        batch: tuple[\n            Sequence[od.InputType],\n            Sequence[od.TargetType],\n            Sequence[od.DatumMetadataType],\n        ],\n    ) -> tuple[\n        Sequence[od.InputType], Sequence[od.TargetType], Sequence[od.DatumMetadataType]\n    ]:\n        # Unpack tuple\n        xb, yb, metadata = batch\n\n        # Type narrow / bridge input batch to PyTorch tensor\n        xb_pt = [torch.as_tensor(xb_i) for xb_i in xb]\n        assert xb_pt[0].ndim == 3, \"Input should be sequence of 3d ArrayLikes\"\n\n        # Apply augmentation to batch\n        # Return augmentation outputs as uint8\n        # - NOTE: assumes input batch has pixels in [0, 255]\n        xb_aug = [\n            (self.kornia_aug(xb_pti / 255.0).clamp(min=0.0, max=1.0) * 255.0).to(\n                torch.uint8\n            )\n            for xb_pti in xb_pt\n        ]\n\n        # Return augmented inputs and pass through unchanged targets and metadata\n        return xb_aug, yb, metadata",
      "names": [
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 606
    },
    {
      "source": "noise: od.Augmentation = WrappedKorniaAugmentation(\n    kornia_noise, metadata={\"id\": \"kornia_rnd_gauss_noise\"}\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 643
    },
    {
      "source": "# Create batch with sample image\ni = 0\nx, y, md = dataset[i]\nx = torch.as_tensor(x)\nxb, yb, mdb = [x], [y], [md]\n\n# Apply augmentation\nxb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))\n\n# Get predictions for augmented batch (which just has one image for this example)\npreds_aug = model(xb_aug)\n\n# Overlay detections on image\nxb_aug = torch.as_tensor(xb_aug[0])\nimg_aug = create_pil_image(xb_aug, preds_aug[0], index2label)\n\n# Show result\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Perturbed\")\nax.imshow(img_aug);",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 652
    },
    {
      "source": "# Run evaluate over perturbed dataset\nresults, _, _ = evaluate(\n    model=model,\n    dataset=dataset,\n    metric=mAP_metric,\n    augmentation=noise,\n)\n\n# Report mAP_50 performance\nresults[\"map_50\"]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 685
    },
    {
      "source": "0%|          | 0/4 [00:00<?, ?it/s]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 702
    },
    {
      "source": "tensor(0.2381)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 709
    }
  ]
}