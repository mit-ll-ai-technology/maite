
<!DOCTYPE html>

<html data-content_root="../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Overview of MAITE Protocols — maite 0.9.0 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!--
    this give us a css class that will be invisible only if js is disabled
  -->
<noscript>
<style>
      .pst-js-only { display: none !important; }

    </style>
</noscript>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet"/>
<link href="../_static/pygments.css?v=8f2a1f02" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-codeautolink.css?v=b2176991" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-design.min.css?v=87e54e7c" rel="stylesheet" type="text/css"/>
<!-- So that users can add custom icons -->
<script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" rel="preload"/>
<script src="../_static/documentation_options.js?v=39bb1c6d"></script>
<script src="../_static/doctools.js?v=9a2dae69"></script>
<script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../_static/copybutton.js?v=30646c52"></script>
<script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-115029372-2"></script>
<script src="../_static/gtag.js"></script>
<script src="../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'explanation/protocol_overview';</script>
<script src="../_static/pypi_icon.js?v=74687d30"></script>
<script src="../_static/condaforge_icon.js?v=19caf20c"></script>
<script src="../_static/anaconda_icon.js?v=e3074ad8"></script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="type_hints_for_API_design.html" rel="next" title="A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&amp;E Framework"/>
<link href="../explanation.html" rel="prev" title="Explanation"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="0.9" name="docsearch:version"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<dialog id="pst-search-dialog">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</dialog>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<p class="title logo__title">maite 0.9.0 documentation</p>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../tutorials.html">
    Tutorials
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../how_tos.html">
    How-To Guides
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../explanation.html">
    Explanation
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../api_reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../changes.html">
    Changelog
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<button aria-label="Search" class="btn search-button-field search-button__button pst-js-only" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
<span class="search-button__default-text">Search</span>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
</div>
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/mit-ll-ai-technology/maite" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/maite/" rel="noopener" target="_blank" title="PyPI"><i aria-hidden="true" class="fa-custom fa-pypi fa-lg"></i>
<span class="sr-only">PyPI</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://anaconda.org/conda-forge/maite" rel="noopener" target="_blank" title="Anaconda"><i aria-hidden="true" class="fa-custom fa-anaconda fa-lg"></i>
<span class="sr-only">Anaconda</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<button aria-label="Search" class="btn search-button-field search-button__button pst-js-only" data-bs-placement="bottom" data-bs-toggle="tooltip" title="Search">
<i class="fa-solid fa-magnifying-glass"></i>
<span class="search-button__default-text">Search</span>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<dialog id="pst-primary-sidebar-modal"></dialog>
<div class="bd-sidebar-primary bd-sidebar" id="pst-primary-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../tutorials.html">
    Tutorials
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../how_tos.html">
    How-To Guides
  </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../explanation.html">
    Explanation
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../api_reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../changes.html">
    Changelog
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<button aria-label="Color mode" class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" data-bs-placement="bottom" data-bs-title="Color mode" data-bs-toggle="tooltip">
<i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light" title="Light"></i>
<i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark" title="Dark"></i>
<i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto" title="System Settings"></i>
</button></div>
<div class="navbar-item"><ul aria-label="Icon Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/mit-ll-ai-technology/maite" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://pypi.org/project/maite/" rel="noopener" target="_blank" title="PyPI"><i aria-hidden="true" class="fa-custom fa-pypi fa-lg"></i>
<span class="sr-only">PyPI</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://anaconda.org/conda-forge/maite" rel="noopener" target="_blank" title="Anaconda"><i aria-hidden="true" class="fa-custom fa-anaconda fa-lg"></i>
<span class="sr-only">Anaconda</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Overview of MAITE Protocols</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_hints_for_API_design.html">A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&amp;E Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="maite_vision.html">MAITE’s Vision for Interoperability in AI Test and Evaluation</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
<div class="sidebar-primary-item">
<div class="flat" data-ea-manual="true" data-ea-publisher="readthedocs" data-ea-type="readthedocs-sidebar" id="ethical-ad-placement">
</div></div>
</div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../explanation.html">Explanation</a></li>
<li aria-current="page" class="breadcrumb-item active"><span class="ellipsis">Overview of MAITE Protocols</span></li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY
Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).
SPDX-License-Identifier: MIT
</pre></div>
</div>
<section id="overview-of-maite-protocols">
<h1>Overview of MAITE Protocols<a class="headerlink" href="#overview-of-maite-protocols" title="Link to this heading">#</a></h1>
<p>MAITE provides protocols for the following AI components:</p>
<ul class="simple">
<li><p>models</p></li>
<li><p>datasets</p></li>
<li><p>dataloaders</p></li>
<li><p>augmentations</p></li>
<li><p>metrics</p></li>
</ul>
<p>MAITE protocols specify expected interfaces of these components (i.e, a
minimal set of required attributes, methods, and method type signatures)
to promote interoperability in test and evaluation (T&amp;E). This enables
the creation of higher-level procedures (e.g., an <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> utility)
that can interact with any components that conform to the protocols.</p>
<section id="concept-bridging-arraylikes">
<h2>1 Concept: Bridging ArrayLikes<a class="headerlink" href="#concept-bridging-arraylikes" title="Link to this heading">#</a></h2>
<p>MAITE uses a type called <code class="docutils literal notranslate"><span class="pre">ArrayLike</span></code> (following NumPy’s
<a class="reference external" href="https://numpy.org/devdocs/user/basics.interoperability.html">interoperability
approach</a>)
that helps components that natively use different flavors of tensors
(e.g., NumPy ndarray, PyTorch Tensor, JAX ndarray) work together.</p>
<p>In this example, the functions “type narrow” from <code class="docutils literal notranslate"><span class="pre">ArrayLike</span></code> to the
type they want to work with internally. Note that this doesn’t
necessarily require a conversion depending on the actual input type.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/index.html#module-numpy" title="numpy"><span class="nn">numpy</span></a><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">maite.protocols</span><span class="w"> </span><span class="kn">import</span> <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.ArrayLike.html#maite.protocols.ArrayLike" title="maite.protocols.ArrayLike"><span class="n">ArrayLike</span></a>


<span class="k">def</span><span class="w"> </span><span class="nf">my_numpy_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.ArrayLike.html#maite.protocols.ArrayLike" title="maite.protocols.ArrayLike"><span class="n">ArrayLike</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="numpy.ndarray"><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span></a><span class="p">:</span>
    <span class="n">arr</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><span class="n">np</span><span class="o">.</span><span class="n">asarray</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">arr</span>


<span class="k">def</span><span class="w"> </span><span class="nf">my_torch_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.ArrayLike.html#maite.protocols.ArrayLike" title="maite.protocols.ArrayLike"><span class="n">ArrayLike</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">tensor</span>


<span class="c1"># can apply NumPy function to PyTorch Tensor</span>
<span class="n">np_out</span> <span class="o">=</span> <span class="n">my_numpy_fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># can apply PyTorch function to NumPy array</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">my_torch_fn</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html#numpy.random.rand" title="numpy.random.rand"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># note: no performance hit from conversion when all `ArrayLike`s are from same library</span>
<span class="c1"># or when can share the same underlying memory</span>
<span class="n">torch_out</span> <span class="o">=</span> <span class="n">my_torch_fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>By using bridging, we MAITE can permit implementers of the protocol to
internally interact with their own types while exposing a more open
interface to other MAITE-compliant components.</p>
</section>
<section id="data-types">
<h2>2 Data Types<a class="headerlink" href="#data-types" title="Link to this heading">#</a></h2>
<p>MAITE represents an <em>individual</em> data item as a tuple of:</p>
<ul class="simple">
<li><p>input (i.e., image),</p></li>
<li><p>target (i.e., label), and</p></li>
<li><p>metadata (at the datum level)</p></li>
</ul>
<p>and a <em>batch</em> of data items as a tuple of:</p>
<ul class="simple">
<li><p>input batches,</p></li>
<li><p>target batches, and</p></li>
<li><p>metadata batches.</p></li>
</ul>
<p>MAITE provides versions of <code class="docutils literal notranslate"><span class="pre">Model</span></code>, <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>,
<code class="docutils literal notranslate"><span class="pre">Augmentation</span></code>, and <code class="docutils literal notranslate"><span class="pre">Metric</span></code> protocols that correspond to different
machine learning problem types (e.g. image classification, object
detection) by parameterizing protocol interfaces on the particular
input, target, and metadata types associated with that problem type.</p>
<section id="image-classification">
<h3>2.1 Image Classification<a class="headerlink" href="#image-classification" title="Link to this heading">#</a></h3>
<p>For image classification with <code class="docutils literal notranslate"><span class="pre">Cl</span></code> image classes, we have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DatumMetadataType</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="nb">id</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/stdtypes.html#str" title="str"><span class="nb">str</span></a><span class="o">|</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#int" title="int"><span class="nb">int</span></a>

<span class="n">InputType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">ArrayLike</span>  <span class="c1"># shape-(C, H, W) tensor with single image</span>
<span class="n">TargetType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">ArrayLike</span>  <span class="c1"># shape-(Cl) tensor of one-hot encoded true class or predicted probabilities</span>
</pre></div>
</div>
<p>Notes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TargetType</span></code> is used for both ground truth (coming from a dataset)
and predictions (output from a model). So for a problem with 4
classes,</p>
<ul>
<li><p>true label of class 2 would be one-hot encoded as <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">0]</span></code></p></li>
<li><p>prediction from a model would be a vector of probabilities or
logits, e.g., <code class="docutils literal notranslate"><span class="pre">[0.1,</span> <span class="pre">0.0,</span> <span class="pre">0.7,</span> <span class="pre">0.2]</span></code></p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">InputType</span></code> and <code class="docutils literal notranslate"><span class="pre">InputBatchType</span></code> are shown with shapes following
PyTorch channels-first convention</p></li>
</ul>
<p>These type aliases along with the versions of the various component
protocols that use these types can be imported from
<code class="docutils literal notranslate"><span class="pre">maite.protocols.image_classification</span></code> (if necessary):</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fmt: off</span>

<span class="c1"># import protocol classes</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">maite.protocols.image_classification</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: I001</span>
    <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.image_classification.Dataset.html#maite.protocols.image_classification.Dataset" title="maite._internals.protocols.image_classification.Dataset"><span class="n">Dataset</span></a><span class="p">,</span>                                        <span class="c1"># noqa: F401</span>
    <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.image_classification.DataLoader.html#maite.protocols.image_classification.DataLoader" title="maite._internals.protocols.image_classification.DataLoader"><span class="n">DataLoader</span></a><span class="p">,</span>                                     <span class="c1"># noqa: F401</span>
    <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.image_classification.Model.html#maite.protocols.image_classification.Model" title="maite._internals.protocols.image_classification.Model"><span class="n">Model</span></a><span class="p">,</span>                                          <span class="c1"># noqa: F401</span>
    <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.image_classification.Augmentation.html#maite.protocols.image_classification.Augmentation" title="maite._internals.protocols.image_classification.Augmentation"><span class="n">Augmentation</span></a><span class="p">,</span>                                   <span class="c1"># noqa: F401</span>
    <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.image_classification.Metric.html#maite.protocols.image_classification.Metric" title="maite._internals.protocols.image_classification.Metric"><span class="n">Metric</span></a>                                          <span class="c1"># noqa: F401</span>
<span class="p">)</span>

<span class="c1"># import type aliases</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">maite.protocols.image_classification</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">InputType</span><span class="p">,</span>                                      <span class="c1"># noqa: F401</span>
    <span class="n">TargetType</span><span class="p">,</span>                                     <span class="c1"># noqa: F401</span>
    <span class="n">DatumMetadataType</span><span class="p">,</span>                              <span class="c1"># noqa: F401</span>
<span class="p">)</span>

<span class="c1"># fmt: on</span>
</pre></div>
</div>
<p>Alternatively, image classification components and types can be accessed
via the module directly:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">maite.protocols.image_classification</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ic</span>  <span class="c1"># noqa: I001</span>

<span class="c1"># model: ic.Model = load_model(...)</span>
</pre></div>
</div>
</section>
<section id="object-detection">
<h3>2.2 Object Detection<a class="headerlink" href="#object-detection" title="Link to this heading">#</a></h3>
<p>For object detection over <code class="docutils literal notranslate"><span class="pre">Cl</span></code> classes with <code class="docutils literal notranslate"><span class="pre">D_i</span></code> detections in an
image <code class="docutils literal notranslate"><span class="pre">i</span></code>, we have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DatumMetadataType</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="nb">id</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/stdtypes.html#str" title="str"><span class="nb">str</span></a><span class="o">|</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#int" title="int"><span class="nb">int</span></a>

<span class="k">class</span><span class="w"> </span><span class="nc">ObjectDetectionTarget</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
    <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#property" title="property"><span class="nd">@property</span></a>
    <span class="k">def</span><span class="w"> </span><span class="nf">boxes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ArrayLike</span><span class="p">:</span> <span class="o">...</span>  <span class="c1"># shape-(D_i, 4) tensor of bounding boxes w/format X0, Y0, X1, Y1</span>

    <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#property" title="property"><span class="nd">@property</span></a>
    <span class="k">def</span><span class="w"> </span><span class="nf">labels</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ArrayLike</span><span class="p">:</span> <span class="o">...</span> <span class="c1"># shape-(D_i) tensor of labels for each box</span>

    <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#property" title="property"><span class="nd">@property</span></a>
    <span class="k">def</span><span class="w"> </span><span class="nf">scores</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ArrayLike</span><span class="p">:</span> <span class="o">...</span> <span class="c1"># shape-(D_i) tensor of scores for each box or</span>
                                       <span class="c1"># shape-(D_i, Cl) tensor of scores for each box/class (scores may be either probabilities or logits)</span>

<span class="n">InputType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">ArrayLike</span>  <span class="c1"># shape-(C, H, W) tensor with single image</span>
<span class="n">TargetType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">ObjectDetectionTarget</span>
</pre></div>
</div>
<p>Notes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ObjectDetectionTarget</span></code> contains a single label and score per box</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InputType</span></code> is shown with shapes following PyTorch channels-first
convention</p></li>
</ul>
</section>
</section>
<section id="models">
<h2>3 Models<a class="headerlink" href="#models" title="Link to this heading">#</a></h2>
<p>All models implement a <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method that takes
<code class="docutils literal notranslate"><span class="pre">Sequence[InputType]</span></code> and produces <code class="docutils literal notranslate"><span class="pre">Sequence[TargetType]</span></code> (for the
<code class="docutils literal notranslate"><span class="pre">InputType</span></code> and <code class="docutils literal notranslate"><span class="pre">TargetType</span></code> appropriate to the given machine
learning problem type).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">ic</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A model protocol for the image classification AI task.

Implementers must provide a <code class="xref py py-obj docutils literal notranslate"><span class="pre">__call__</span></code> method that operates on a batch of model
inputs (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike])</span> <span class="pre">and</span> <span class="pre">returns</span> <span class="pre">a</span> <span class="pre">batch</span> <span class="pre">of</span> <span class="pre">model</span> <span class="pre">targets</span> <span class="pre">(as</span>
<span class="pre">`Sequence[ArrayLike]</span></code>)

Methods
-------

__call__(input_batch: Sequence[ArrayLike]) -&gt; Sequence[ArrayLike]
    Make a model prediction for inputs in input batch. Input batch is expected to
    be <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> with each element of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>.

Attributes
----------

metadata : ModelMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

We create a multinomial logistic regression classifier for a CIFAR-10-like dataset
with 10 classes and shape-(3, 32, 32) images.

&gt;&gt;&gt; import maite.protocols.image_classification as ic
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import numpy.typing as npt
&gt;&gt;&gt; from maite.protocols import ArrayLike, ModelMetadata
&gt;&gt;&gt; from typing import Sequence

Creating a MAITE-compliant model involves writing a <code class="xref py py-obj docutils literal notranslate"><span class="pre">__call__</span></code> method that takes a
batch of inputs and returns a batch of predictions (probabilities or logits).

&gt;&gt;&gt; class LinearClassifier:
...     def __init__(self) -&gt; None:
...         # Set up required metadata attribute using the default <code class="xref py py-obj docutils literal notranslate"><span class="pre">ModelMetadata</span></code> type,
...         # using class name for the ID
...         self.metadata: ModelMetadata = {"id": self.__class__.__name__}
...
...         # Initialize weights
...         rng = np.random.default_rng(12345678)
...         num_classes = 10
...         flattened_size = 3 * 32 * 32
...         self.weights = -0.2 + 0.4 * rng.random((flattened_size, num_classes))
...         self.bias = -0.2 + 0.4 * rng.random((1, num_classes))
...
...     def __call__(self, batch: Sequence[ArrayLike]) -&gt; Sequence[npt.NDArray]:
...         # Convert each element in batch to ndarray, flatten,
...         # then combine into 4D array of shape-(N, C, H, W)
...         batch_np = np.vstack([np.asarray(x).flatten() for x in batch])
...
...         # Send input batch through model
...         out = batch_np @ self.weights + self.bias
...         out = np.exp(out) / np.sum(
...             np.exp(out), axis=1, keepdims=True
...         )  # softmax
...
...         # Restructure to sequence of shape-(10,) probabilities
...         return [row for row in out]

We set up a test batch, instantiate the model, and apply it to the batch.

&gt;&gt;&gt; batch_size = 8
&gt;&gt;&gt; rng = np.random.default_rng(12345678)
&gt;&gt;&gt; batch: Sequence[ArrayLike] = [
...     -0.2 + 0.4 * rng.random((3, 32, 32)) for _ in range(batch_size)
... ]
&gt;&gt;&gt;
&gt;&gt;&gt; model: ic.Model = LinearClassifier()
&gt;&gt;&gt; out = model(batch)

We can now show the class probabilities returned by the model for each image in the batch.

&gt;&gt;&gt; np.set_printoptions(
...     floatmode="fixed", precision=2
... )  # for reproducible output for doctest
&gt;&gt;&gt; for probs in out:  # doctest: +NORMALIZE_WHITESPACE
...     print(np.round(probs, 2))
[0.16 0.10 0.16 0.14 0.04 0.02 0.06 0.04 0.17 0.10]
[0.21 0.16 0.04 0.07 0.08 0.05 0.09 0.03 0.18 0.09]
[0.15 0.11 0.13 0.11 0.09 0.09 0.07 0.04 0.19 0.02]
[0.04 0.08 0.14 0.07 0.12 0.20 0.11 0.06 0.14 0.04]
[0.03 0.08 0.06 0.05 0.17 0.18 0.09 0.03 0.12 0.19]
[0.09 0.04 0.10 0.03 0.32 0.05 0.07 0.04 0.15 0.09]
[0.15 0.05 0.10 0.05 0.11 0.14 0.04 0.08 0.08 0.20]
[0.11 0.11 0.08 0.11 0.08 0.05 0.24 0.03 0.08 0.12]

Note that when writing a Model implementer, return types may be narrower than the
return types promised by the protocol (npt.NDArray is a subtype of ArrayLike), but
the argument types must be at least as general as the argument types promised by the
protocol.</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">maite.protocols.object_detection</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">od</span>

<a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">od</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A model protocol for the object detection AI task.

Implementers must provide a <code class="xref py py-obj docutils literal notranslate"><span class="pre">__call__</span></code> method that operates on a batch of model inputs
(as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]`s)</span> <span class="pre">and</span> <span class="pre">returns</span> <span class="pre">a</span> <span class="pre">batch</span> <span class="pre">of</span> <span class="pre">model</span> <span class="pre">targets</span> <span class="pre">(as</span>
<span class="pre">`Sequence[ObjectDetectionTarget]</span></code>)

Methods
-------

__call__(input_batch: Sequence[ArrayLike]) -&gt; Sequence[ObjectDetectionTarget]
    Make a model prediction for inputs in input batch. Elements of input batch
    are expected in the shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>.

Attributes
----------

metadata : ModelMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

We create a simple MAITE-compliant object detection model, note it is a dummy model.

&gt;&gt;&gt; from dataclasses import dataclass
&gt;&gt;&gt; from typing import Sequence
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import maite.protocols.object_detection as od
&gt;&gt;&gt; from maite.protocols import ModelMetadata

We define an object detection target dataclass as the output of the object detection model

&gt;&gt;&gt; @dataclass
... class MyObjectDetectionTarget:
...     boxes: np.ndarray
...     labels: np.ndarray
...     scores: np.ndarray

Specify parameters that will be used to create a dummy dataset.

&gt;&gt;&gt; N_DATAPOINTS = 2  # datapoints in dataset
&gt;&gt;&gt; N_CLASSES = 5  # possible classes that can be detected
&gt;&gt;&gt; C = 3  # number of color channels
&gt;&gt;&gt; H = 32  # img height
&gt;&gt;&gt; W = 32  # img width

Now create a batch of data to form the inputs of the MAITE's object detection model.

&gt;&gt;&gt; simple_batch: list[np.ndarray] = [
...     np.random.rand(C, H, W) for _ in range(N_DATAPOINTS)
... ]

We define a simple object detection model, note here there is not an actual object detection
model. In the __call__ method, it just outputs the MyObjectDetectionTarget.

&gt;&gt;&gt; class ObjectDetectionDummyModel:
...     metadata: ModelMetadata = {"id": "ObjectDetectionDummyModel"}
...
...     def __call__(
...         self, batch: Sequence[od.InputType]
...     ) -&gt; Sequence[MyObjectDetectionTarget]:
...         # For the simplicity, we don't provide an object detection model here, but the output from a model.
...         DETECTIONS_PER_IMG = (
...             2  # number of bounding boxes detections per image/datapoints
...         )
...         all_boxes = np.array(
...             [[1, 3, 5, 9], [2, 5, 8, 12], [4, 10, 8, 20], [3, 5, 6, 15]]
...         )  # all detection boxes for N_DATAPOINTS
...         all_predictions = list()
...         for datum_idx in range(N_DATAPOINTS):
...             boxes = all_boxes[datum_idx : datum_idx + DETECTIONS_PER_IMG]
...             labels = np.random.randint(N_CLASSES, size=DETECTIONS_PER_IMG)
...             scores = np.random.rand(DETECTIONS_PER_IMG)
...             predictions = MyObjectDetectionTarget(boxes, labels, scores)
...             all_predictions.append(predictions)
...         return all_predictions

We can instantiate this class and typehint it as a maite object detection model.
By using typehinting, we permit a static typechecker to verify protocol compliance.

&gt;&gt;&gt; od_dummy_model: od.Model = ObjectDetectionDummyModel()
&gt;&gt;&gt; od_dummy_model.metadata
{'id': 'ObjectDetectionDummyModel'}
&gt;&gt;&gt; predictions = od_dummy_model(simple_batch)
&gt;&gt;&gt; predictions  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
[MyObjectDetectionTarget(boxes=array([[ 1,  3,  5,  9], [ 2,  5,  8, 12]]), labels=array([..., ...]), scores=array([..., ...])),
MyObjectDetectionTarget(boxes=array([[ 2,  5,  8, 12], [ 4, 10,  8, 20]]), labels=array([..., ...]), scores=array([..., ...]))]</pre>
</section>
<section id="datasets-and-dataloaders">
<h2>4 Datasets and DataLoaders<a class="headerlink" href="#datasets-and-dataloaders" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s provide access to single data items and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>s
provide access to batches of data with the input, target, and metadata
types corresponding to the given machine learning problem type.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">ic</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A dataset protocol for image classification AI task providing datum-level
data access.

Implementers must provide index lookup (via <code class="xref py py-obj docutils literal notranslate"><span class="pre">__getitem__(ind:</span> <span class="pre">int)</span></code> method) and
support <a class="reference external" href="https://docs.python.org/3/library/functions.html#len" title="(in Python v3.13)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">len</span></code></a> (via <code class="xref py py-obj docutils literal notranslate"><span class="pre">__len__()</span></code> method). Data elements looked up this way correspond
to individual examples (as opposed to batches).

Indexing into or iterating over an image_classification dataset returns a
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of types <code class="xref py py-obj docutils literal notranslate"><span class="pre">ArrayLike</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">ArrayLike</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">DatumMetadataType</span></code>.
These correspond to the model input type, model target type, and datum-level
metadata, respectively.

Methods
-------

__getitem__(ind: int) -&gt; tuple[ArrayLike, ArrayLike, DatumMetadataType]
    Provide map-style access to dataset elements. Returned tuple elements
    correspond to model input type, model target type, and datum-specific metadata type,
    respectively.

__len__() -&gt; int
    Return the number of data elements in the dataset.

Attributes
----------

metadata : DatasetMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

We create a dummy set of data and use it to create a class that implements
this lightweight dataset protocol:

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from typing import Any
&gt;&gt;&gt; from typing_extensions import TypedDict
&gt;&gt;&gt; from maite.protocols import ArrayLike, DatasetMetadata

Assume we have 5 classes, 10 datapoints, and 10 target labels, and that we want
to simply have an integer 'id' field in each datapoint's metadata:

&gt;&gt;&gt; N_CLASSES: int = 5
&gt;&gt;&gt; N_DATUM: int = 10
&gt;&gt;&gt; images: list[np.ndarray] = [np.random.rand(3, 32, 16) for _ in range(N_DATUM)]
&gt;&gt;&gt; targets: np.ndarray = np.eye(N_CLASSES)[np.random.choice(N_CLASSES, N_DATUM)]

We can type our datum metadata as a maite.protocols DatumMetadata, or define our
own TypedDict with additional fields

&gt;&gt;&gt; class MyDatumMetadata(DatumMetadata):
...     hour_of_day: float
&gt;&gt;&gt; datum_metadata = [
...     MyDatumMetadata(id=i, hour_of_day=np.random.rand() * 24)
...     for i in range(N_DATUM)
... ]

Constructing a compliant dataset just involves a simple wrapper that fetches
individual datapoints, where a datapoint is a single image, target, metadata 3-tuple.

&gt;&gt;&gt; class ImageDataset:
...     def __init__(
...         self,
...         dataset_name: str,
...         index2label: dict[int, str],
...         images: list[np.ndarray],
...         targets: np.ndarray,
...         datum_metadata: list[MyDatumMetadata],
...     ):
...         self.images = images
...         self.targets = targets
...         self.metadata = DatasetMetadata(
...             {"id": dataset_name, "index2label": index2label}
...         )
...         self._datum_metadata = datum_metadata
...
...     def __len__(self) -&gt; int:
...         return len(images)
...
...     def __getitem__(
...         self, ind: int
...     ) -&gt; tuple[np.ndarray, np.ndarray, MyDatumMetadata]:
...         return self.images[ind], self.targets[ind], self._datum_metadata[ind]

We can instantiate this class and typehint it as an image_classification.Dataset.
By using typehinting, we permit a static typechecker to verify protocol compliance.

&gt;&gt;&gt; from maite.protocols import image_classification as ic
&gt;&gt;&gt; dataset: ic.Dataset = ImageDataset(
...     "a_dataset",
...     {i: f"class_name_{i}" for i in range(N_CLASSES)},
...     images,
...     targets,
...     datum_metadata,
... )

Note that when writing a Dataset implementer, return types may be narrower than the
return types promised by the protocol (np.ndarray is a subtype of ArrayLike), but
the argument types must be at least as general as the argument types promised by the
protocol.</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">ic</span><span class="o">.</span><span class="n">DataLoader</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A dataloader protocol for the image classification AI task providing
batch-level data access.

Implementers must provide an iterable object (returning an iterator via the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">__iter__</span></code> method) that yields tuples containing batches of data. These tuples
contain types <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> (elements of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>),
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> (elements shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(Cl,</span> <span class="pre">)</span></code>), and <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>,
which correspond to model input batch, model target type batch, and a datum metadata batch.

Note: Unlike Dataset, this protocol does not require indexing support, only iterating.

Methods
-------

__iter__ -&gt; Iterator[tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadataType]]]
    Return an iterator over batches of data, where each batch contains a tuple of
    of model input batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>), model target batch (as
    <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>), and batched datum-level metadata
    (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>), respectively.</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">od</span><span class="o">.</span><span class="n">DataLoader</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A dataloader protocol for the object detection AI task providing
batch-level data access.

Implementers must provide an iterable object (returning an iterator via the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">__iter__</span></code> method) that yields tuples containing batches of data. These tuples
contain types <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> (elements of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>),
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ObjectDetectionTarget]</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>,
which correspond to model input batch, model target batch, and a datum metadata batch.

Note: Unlike Dataset, this protocol does not require indexing support, only iterating.


Methods
-------

__iter__ -&gt; Iterator[tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[DatumMetadataType]]]
    Return an iterator over batches of data, where each batch contains a tuple of
    of model input batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>), model target batch (as
    <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ObjectDetectionTarget]</span></code>), and batched datum-level metadata
    (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]]</span></code>), respectively.</pre>
</section>
<section id="augmentations">
<h2>5 Augmentations<a class="headerlink" href="#augmentations" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Augmentation</span></code>s take in and return a batch of data with the
<code class="docutils literal notranslate"><span class="pre">InputType</span></code>, <code class="docutils literal notranslate"><span class="pre">TargetType</span></code>, and <code class="docutils literal notranslate"><span class="pre">DatumMetadataType</span></code> types
corresponding to the given machine learning problem type.</p>
<p>Augmentations can access the datum-level metadata associated with each
data item to potentially tailor the augmentation to individual items.
Augmentations can also associate new datum-level metadata with each data
item, e.g., documenting aspects of the actual change that was applied
(e.g., the actual rotation angle sampled from a range of possible
angles).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">ic</span><span class="o">.</span><span class="n">Augmentation</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">An augmentation protocol for the image classification AI task.

An augmentation is expected to take a batch of data and return a modified version of
that batch. Implementers must provide a single method that takes and returns a
labeled data batch, where a labeled data batch is represented by a tuple of types
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> (with elements of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>), <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>
(with elements of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(Cl,</span> <span class="pre">)</span></code>), and <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>. These correspond
to the model input batch type, model target batch type, and datum-level metadata
batch type, respectively.

Methods
-------

__call__(datum: tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadataType]]) -&gt;          tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadataType]])
    Return a modified version of original data batch. A data batch is represented
    by a tuple of model input batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> with elements of shape
    <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>), model target batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code> with elements of shape
    <code class="xref py py-obj docutils literal notranslate"><span class="pre">(Cl,)</span></code>), and batch metadata (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>), respectively.

Attributes
----------

metadata : AugmentationMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

We can write an implementer of the augmentation class as either a function or a class.
The only requirement is that the object provide a __call__ method that takes objects
at least as general as the types promised in the protocol signature and return types
at least as specific.

&gt;&gt;&gt; import copy
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from typing import Any
&gt;&gt;&gt; from collections.abc import Sequence
&gt;&gt;&gt; from maite.protocols import ArrayLike, DatumMetadata, AugmentationMetadata
&gt;&gt;&gt;
&gt;&gt;&gt; class EnrichedDatumMetadata(DatumMetadata):
...     new_key: int  # add a field to those already in DatumMetadata
...
&gt;&gt;&gt; class ImageAugmentation:
...     def __init__(self, aug_name: str):
...         self.metadata: AugmentationMetadata = {'id': aug_name}
...     def __call__(
...         self,
...         data_batch: tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadata]]
...     ) -&gt; tuple[Sequence[np.ndarray], Sequence[np.ndarray], Sequence[EnrichedDatumMetadata]]:
...         inputs, targets, mds = data_batch
...         # We copy data passed into the constructor to avoid mutating original inputs
...         # By using np.ndarray constructor, the static type-checker will let us treat
...         # generic ArrayLike as a more narrow return type
...         inputs_aug = [copy.copy(np.array(input)) for input in inputs]
...         targets_aug = [copy.copy(np.array(target)) for target in targets]
...         # Modify inputs_aug, targets_aug, or mds_aug as needed
...         # In this example, we just add a new metadata field
...         mds_aug = []
...         for i, md in enumerate(mds):
...             mds_aug.append(EnrichedDatumMetadata(<a href="#id1"><span class="problematic" id="id2">**</span></a>md, new_key=i))
...         return inputs_aug, targets_aug, mds_aug

We can typehint an instance of the above class as an Augmentation in the
image_classification domain:

&gt;&gt;&gt; from maite.protocols import image_classification as ic
&gt;&gt;&gt; im_aug: ic.Augmentation = ImageAugmentation(aug_name = 'an_augmentation')</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">od</span><span class="o">.</span><span class="n">Augmentation</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">An augmentation protocol for the object detection AI task.

An augmentation is expected to take a batch of data and return a modified version of
that batch. Implementers must provide a single method that takes and returns a
labeled data batch, where a labeled data batch is represented by a tuple of types
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ObjectDetectionTarget]</span></code>, and <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>.
These correspond to the model input batch type, model target batch type, and datum-level
metadata batch type, respectively.

Methods
-------

__call__(datum: Tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[DatumMetadataType]]) -&gt;          Tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[DatumMetadataType]]
    Return a modified version of original data batch. A data batch is represented
    by a tuple of model input batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence</span> <span class="pre">ArrayLike</span></code> with elements of shape
    <code class="xref py py-obj docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>), model target batch (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[ObjectDetectionTarget]</span></code>), and
    batch metadata (as <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[DatumMetadataType]</span></code>), respectively.

Attributes
----------

metadata : AugmentationMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

We can write an implementer of the augmentation class as either a function or a class.
The only requirement is that the object provide a <code class="xref py py-obj docutils literal notranslate"><span class="pre">__call__</span></code> method that takes objects
at least as general as the types promised in the protocol signature and return types
at least as specific.

We create a dummy set of data and use it to create a class the implements this
lightweight protocol and augments the data:

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.random.seed(1)
&gt;&gt;&gt; import copy
&gt;&gt;&gt; from dataclasses import dataclass
&gt;&gt;&gt; from typing import Any, Sequence
&gt;&gt;&gt; from maite.protocols import AugmentationMetadata, object_detection as od

First, we specify parameters that will be used to create the dummy dataset.

&gt;&gt;&gt; N_DATAPOINTS = 3  # datapoints in dataset
&gt;&gt;&gt; N_CLASSES = 2  # possible classes that can be detected
&gt;&gt;&gt; C = 3  # number of color channels
&gt;&gt;&gt; H = 10  # img height
&gt;&gt;&gt; W = 10  # img width

Next, we create the input data to be used by the Augmentation. In this example, we
create the following batch of object detection data:

• <code class="xref py py-obj docutils literal notranslate"><span class="pre">xb</span></code> is the input batch data. Our batch will include <code class="xref py py-obj docutils literal notranslate"><span class="pre">N_DATAPOINTS</span></code> number of samples. Note
  that we initialize all of the data to zeros in this example to demonstrate the augmentations
  better.

• <code class="xref py py-obj docutils literal notranslate"><span class="pre">yb</span></code> is the object detection target data, which in this example represents zero object
  detections for each input datum (by having empty bounding boxes and class labels and scores).

• <code class="xref py py-obj docutils literal notranslate"><span class="pre">mdb</span></code> is the associated metadata for each input datum.

&gt;&gt;&gt; @dataclass
... class MyObjectDetectionTarget:
...     boxes: np.ndarray
...     labels: np.ndarray
...     scores: np.ndarray
&gt;&gt;&gt; xb: Sequence[od.InputType] = list(np.zeros((N_DATAPOINTS, C, H, W)))
&gt;&gt;&gt; yb: Sequence[od.TargetType] = list(
...     MyObjectDetectionTarget(boxes=np.empty(0), labels=np.empty(0), scores=np.empty(0))
...     for _ in range(N_DATAPOINTS)
... )
&gt;&gt;&gt; mdb: Sequence[od.DatumMetadataType] = list({"id": i} for i in range(N_DATAPOINTS))
&gt;&gt;&gt; # Display the first datum in batch, first color channel, and only first 5 rows and cols
&gt;&gt;&gt; np.set_printoptions(floatmode='fixed', precision=3)  # for reproducible output for doctest
&gt;&gt;&gt; np.array(xb[0])[0][:5, :5]  # doctest: +NORMALIZE_WHITESPACE
array([[0.000, 0.000, 0.000, 0.000, 0.000],
       [0.000, 0.000, 0.000, 0.000, 0.000],
       [0.000, 0.000, 0.000, 0.000, 0.000],
       [0.000, 0.000, 0.000, 0.000, 0.000],
       [0.000, 0.000, 0.000, 0.000, 0.000]])

Now we create the Augmentation, which will apply random noise (rounded to 3 decimal
places) to the input data using the <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html#numpy.random.random" title="(in NumPy v2.3)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.random.random</span></code></a> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">np.round</span></code> functions.

&gt;&gt;&gt; np_noise = lambda shape: np.round(np.random.random(shape), 3)
&gt;&gt;&gt; class ImageAugmentation:
...     def __init__(self, aug_func: Any, metadata: AugmentationMetadata):
...         self.aug_func = aug_func
...         self.metadata = metadata
...     def __call__(
...         self,
...         batch: tuple[Sequence[od.InputType], Sequence[od.TargetType], Sequence[od.DatumMetadataType]],
...     ) -&gt; tuple[Sequence[od.InputType], Sequence[od.TargetType], Sequence[od.DatumMetadataType]]:
...         xb, yb, mdb = batch
...         # Copy data passed into the constructor to avoid mutating original inputs
...         xb_aug = [copy.copy(input) for input in xb]
...         # Add random noise to the input batch data, xb
...         # (Note that all batch data dimensions (shapes) are the same in this example)
...         shape = np.array(xb[0]).shape
...         xb_aug = [x + self.aug_func(shape) for x in xb]
...         # Note that this example augmentation only affects inputs--not targets
...         return xb_aug, yb, mdb

We can typehint an instance of the above class as an Augmentation in the object
detection domain:

&gt;&gt;&gt; noise: od.Augmentation = ImageAugmentation(np_noise, metadata={"id": "np_rand_noise"})

Now we can apply the <code class="xref py py-obj docutils literal notranslate"><span class="pre">noise</span></code> augmentation to our 3-tuple batch of data. Recall that
our data was initialized to all zeros, so any non-zero values in the augmented data
is a result of the augmentation.

&gt;&gt;&gt; xb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))
&gt;&gt;&gt; # Display the first datum in batch, first color channel, and only first 5 rows and cols
&gt;&gt;&gt; np.array(xb_aug[0])[0][:5, :5]  # doctest: +NORMALIZE_WHITESPACE
array([[0.417, 0.720, 0.000, 0.302, 0.147],
       [0.419, 0.685, 0.204, 0.878, 0.027],
       [0.801, 0.968, 0.313, 0.692, 0.876],
       [0.098, 0.421, 0.958, 0.533, 0.692],
       [0.989, 0.748, 0.280, 0.789, 0.103]])</pre>
</section>
<section id="metrics">
<h2>6 Metrics<a class="headerlink" href="#metrics" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Metric</span></code> protocol is inspired by the design of existing libraries
like Torchmetrics and Torcheval. The <code class="docutils literal notranslate"><span class="pre">update</span></code> method operates on
batches of predictions and truth labels by either caching them for later
computation of the metric (via <code class="docutils literal notranslate"><span class="pre">compute</span></code>) or updating sufficient
statistics in an online fashion.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">ic</span><span class="o">.</span><span class="n">Metric</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A metric protocol for the image classification AI task.

A metric in this sense is expected to measure the level of agreement between model
predictions and ground-truth labels.

Methods
-------

update(pred_batch: Sequence[ArrayLike], target_batch: Sequence[ArrayLike], metadata_batch: Sequence[DatumMetadataType]) -&gt; None
    Add predictions and targets (and metadata if applicable) to metric's cache for later calculation. Both
    predictions and targets are expected to be sequences with elements of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(Cl,)</span></code>.

compute() -&gt; dict[str, Any]
    Compute metric value(s) for currently cached predictions and targets, returned as
    a dictionary.

reset() -&gt; None
    Clear contents of current metric's cache of predictions and targets.

Attributes
----------

metadata : MetricMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

Create a basic accuracy metric and test it on a small example dataset:

&gt;&gt;&gt; from typing import Any, Sequence
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from maite.protocols import ArrayLike
&gt;&gt;&gt; from maite.protocols import image_classification as ic

&gt;&gt;&gt; class MyAccuracy:
...     metadata: MetricMetadata = {"id": "Example Multiclass Accuracy"}
...
...     def __init__(self):
...         self._total = 0
...         self._correct = 0
...
...     def reset(self) -&gt; None:
...         self._total = 0
...         self._correct = 0
...
...     def update(
...         self,
...         pred_batch: Sequence[ArrayLike],
...         target_batch: Sequence[ArrayLike],
...         metadata_batch: Sequence[DatumMetadataType],
...     ) -&gt; None:
...         model_preds = [np.array(r) for r in pred_batch]
...         true_onehot = [np.array(r) for r in target_batch]
...
...         # Stack into single array, convert to class indices
...         model_classes = np.vstack(model_preds).argmax(axis=1)
...         truth_classes = np.vstack(true_onehot).argmax(axis=1)
...
...         # Compare classes and update running counts
...         same = model_classes == truth_classes
...         self._total += len(same)
...         self._correct += same.sum().item()
...
...     def compute(self) -&gt; dict[str, Any]:
...         if self._total &gt; 0:
...             return {"accuracy": self._correct / self._total}
...         else:
...             raise Exception("No batches processed yet.")

Instantiate this class and typehint it as an image_classification.Metric.
By using typehinting, permits a static typechecker to check protocol compliance.

&gt;&gt;&gt; accuracy: ic.Metric = MyAccuracy()

To use the metric call update() for each batch of predictions and truth values and call compute() to calculate the final metric values.

&gt;&gt;&gt; # batch 1
&gt;&gt;&gt; model_preds = [
...     np.array([0.8, 0.1, 0.0, 0.1]),
...     np.array([0.1, 0.2, 0.6, 0.1]),
... ]  # predicted classes: 0, 2
&gt;&gt;&gt; true_onehot = [
...     np.array([1.0, 0.0, 0.0, 0.0]),
...     np.array([0.0, 1.0, 0.0, 0.0]),
... ]  # true classes: 0, 1
&gt;&gt;&gt; metadatas: list[ic.DatumMetadataType] = [{"id": 1}, {"id": 2}]
&gt;&gt;&gt; accuracy.update(model_preds, true_onehot, metadatas)
&gt;&gt;&gt; print(accuracy.compute())
{'accuracy': 0.5}
&gt;&gt;&gt;
&gt;&gt;&gt; # batch 2
&gt;&gt;&gt; model_preds = [
...     np.array([0.1, 0.1, 0.7, 0.1]),
...     np.array([0.0, 0.1, 0.0, 0.9]),
... ]  # predicted classes: 2, 3
&gt;&gt;&gt; true_onehot = [
...     np.array([0.0, 0.0, 1.0, 0.0]),
...     np.array([0.0, 0.0, 0.0, 1.0]),
... ]  # true classes: 2, 3
&gt;&gt;&gt; metadatas: list[ic.DatumMetadataType] = [{"id": 3}, {"id": 4}]
&gt;&gt;&gt; accuracy.update(model_preds, true_onehot, metadatas)
&gt;&gt;&gt;
&gt;&gt;&gt; print(accuracy.compute())
{'accuracy': 0.75}</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">od</span><span class="o">.</span><span class="n">Metric</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">A metric protocol for the object detection AI task.

A metric in this sense is expected to measure the level of agreement between model
predictions and ground-truth labels.

Methods
-------

update(pred_batch: Sequence[ObjectDetectionTarget], target_batch: Sequence[ObjectDetectionTarget], metadata_batch: Sequence[DatumMetadataType]) -&gt; None
     Add predictions and targets (and metadata if applicable) to metric's cache for later calculation.

compute() -&gt; dict[str, Any]
     Compute metric value(s) for currently cached predictions and targets, returned as
     a dictionary.

reset() -&gt; None
    Clear contents of current metric's cache of predictions and targets.

Attributes
----------

metadata : MetricMetadata
    A typed dictionary containing at least an 'id' field of type str

Examples
--------

Below, we write and test a class that implements the Metric protocol for object detection.
For simplicity, the metric will compute the intersection over union (IoU) averaged over
all predicted and associated ground truth boxes for a single image, and then take the mean
over the per-image means.

Note that when writing a <code class="xref py py-obj docutils literal notranslate"><span class="pre">Metric</span></code> implementer, return types may be narrower than the
return types promised by the protocol, but the argument types must be at least as
general as the argument types promised by the protocol.

&gt;&gt;&gt; from dataclasses import dataclass
&gt;&gt;&gt; from maite.protocols import ArrayLike, MetricMetadata
&gt;&gt;&gt; from typing import Any, Sequence
&gt;&gt;&gt; import maite.protocols.object_detection as od
&gt;&gt;&gt; import numpy as np

&gt;&gt;&gt; class MyIoUMetric:
...     def __init__(self, id: str):
...         self.pred_boxes = []  # elements correspond to predicted boxes in single image
...         self.target_boxes = []  # elements correspond to ground truth boxes in single image
...         # Store provided id for this metric instance
...         self.metadata = MetricMetadata(id=id)
...
...     def reset(self) -&gt; None:
...         self.pred_boxes = []
...         self.target_boxes = []
...
...     def update(
...         self,
...         pred_batch: Sequence[od.ObjectDetectionTarget],
...         target_batch: Sequence[od.ObjectDetectionTarget],
...         metadata_batch: Sequence[od.DatumMetadataType],
...     ) -&gt; None:
...         self.pred_boxes.extend(pred_batch)
...         self.target_boxes.extend(target_batch)
...
...     @staticmethod
...     def iou_vec(boxes_a: ArrayLike, boxes_b: ArrayLike) -&gt; np.ndarray:
...         # Break up points into separate columns
...         x0a, y0a, x1a, y1a = np.split(boxes_a, 4, axis=1)
...         x0b, y0b, x1b, y1b = np.split(boxes_b, 4, axis=1)
...         # Calculate intersections
...         xi_0, yi_0 = np.split(
...             np.maximum(
...                 np.append(x0a, y0a, axis=1), np.append(x0b, y0b, axis=1)
...             ),
...             2,
...             axis=1,
...         )
...         xi_1, yi_1 = np.split(
...             np.minimum(
...                 np.append(x1a, y1a, axis=1), np.append(x1b, y1b, axis=1)
...             ),
...             2,
...             axis=1,
...         )
...         ints: np.ndarray = np.maximum(0, xi_1 - xi_0) * np.maximum(
...             0, yi_1 - yi_0
...         )
...         # Calculate unions (as sum of areas minus their intersection)
...         unions: np.ndarray = (
...             (x1a - x0a) * (y1a - y0a)
...             + (x1b - x0b) * (y1b - y0b)
...             - (xi_1 - xi_0) * (yi_1 - yi_0)
...         )
...         return ints / unions
...
...     def compute(self) -&gt; dict[str, Any]:
...         mean_iou_by_img: list[float] = []
...         for pred_box, tgt_box in zip(self.pred_boxes, self.target_boxes):
...             single_img_ious = self.iou_vec(pred_box.boxes, tgt_box.boxes)
...             mean_iou_by_img.append(float(np.mean(single_img_ious)))
...         return {"mean_iou": np.mean(np.array(mean_iou_by_img)).item()}

Now we can instantiate our IoU Metric class:

&gt;&gt;&gt; iou_metric: od.Metric = MyIoUMetric(id="IoUMetric")

To use the metric, we populate two lists that encode predicted object detections
and ground truth object detections for a single image. (Ordinarily, predictions
would be produced by a model.)

&gt;&gt;&gt; prediction_boxes: list[tuple[int, int, int, int]] = [
...     (1, 1, 12, 12),
...     (100, 100, 120, 120),
...     (180, 180, 270, 270),
... ]

&gt;&gt;&gt; target_boxes: list[tuple[int, int, int, int]] = [
...     (1, 1, 10, 10),
...     (100, 100, 120, 120),
...     (200, 200, 300, 300),
... ]

The MAITE Metric protocol requires the <code class="xref py py-obj docutils literal notranslate"><span class="pre">pred_batch</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">target_batch</span></code> arguments to the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">update</span></code> method to be assignable to Sequence[ObjectDetectionTarget] (where ObjectDetectionTarget
encodes detections in a single image). We define an implementation of ObjectDetectionTarget and use it
to pass ground truth and predicted detections.

&gt;&gt;&gt; @dataclass
... class ObjectDetectionTargetImpl:
...     boxes: np.ndarray
...     labels: np.ndarray
...     scores: np.ndarray

&gt;&gt;&gt; num_boxes = len(target_boxes)
&gt;&gt;&gt; fake_labels = np.random.randint(0, 9, num_boxes)
&gt;&gt;&gt; fake_scores = np.zeros(num_boxes)
&gt;&gt;&gt; pred_batch = [
...     ObjectDetectionTargetImpl(
...         boxes=np.array(prediction_boxes), labels=fake_labels, scores=fake_scores
...     )
... ]
&gt;&gt;&gt; target_batch: Sequence[ObjectDetectionTargetImpl] = [
...     ObjectDetectionTargetImpl(
...         boxes=np.array(target_boxes), labels=fake_labels, scores=fake_scores
...     )
... ]
&gt;&gt;&gt; metadata_batch: Sequence[od.DatumMetadataType] = [{"id": 1}]

Finally, we call <code class="xref py py-obj docutils literal notranslate"><span class="pre">update</span></code> using this one-element batch, compute the metric value, and print it:

&gt;&gt;&gt; iou_metric.update(pred_batch, target_batch, metadata_batch)
&gt;&gt;&gt; print(iou_metric.compute())
{'mean_iou': 0.6802112029384757}</pre>
</section>
<section id="procedures">
<h2>7 Procedures<a class="headerlink" href="#procedures" title="Link to this heading">#</a></h2>
<p>MAITE provides high-level utilities for common T&amp;E procedures such as
<code class="docutils literal notranslate"><span class="pre">evaluate</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code>. They can be called with either
<code class="docutils literal notranslate"><span class="pre">Dataset</span></code>s or <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>s, and with optional <code class="docutils literal notranslate"><span class="pre">Augmentation</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> function can optionally return the model predictions
and (potentially-augmented) data batches used during inference.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">predict</span></code> function returns the model predictions and
(potentially-augmented) data batches used during inference, essentially
calling <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> with a dummy metric.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">maite.tasks</span><span class="w"> </span><span class="kn">import</span> <a class="sphinx-codeautolink-a" href="../generated/maite.tasks.evaluate.html#maite.tasks.evaluate" title="maite.tasks.evaluate"><span class="n">evaluate</span></a><span class="p">,</span> <a class="sphinx-codeautolink-a" href="../generated/maite.tasks.predict.html#maite.tasks.predict" title="maite.tasks.predict"><span class="n">predict</span></a>
<span class="c1"># we can also import from object_detection module</span>
<span class="c1"># where the function call signature is the same</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">evaluate</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">Evaluate a model's performance on data according to some metric with optional augmentation.

The types handled by all passed components must be compatible to avoid static type checking
errors. For example, if the __getitem__ method of a passed dataset returns some <code class="xref py py-obj docutils literal notranslate"><span class="pre">InputType</span></code>
in the first element of the return tuple then the <code class="xref py py-obj docutils literal notranslate"><span class="pre">model.__call__</span></code> argument must be type hinted
such that <code class="xref py py-obj docutils literal notranslate"><span class="pre">Sequence[InputType]</span></code> can be assigned to it.

Parameters
----------
model : SomeModel
    Maite Model object.

metric : SomeMetric | None, (default=None)
    Compatible maite Metric.

dataloader : SomeDataloader | None, (default=None)
    Compatible maite dataloader.

dataset : SomeDataset | None, (default=None)
    Compatible maite dataset.

batch_size : int, (default=1)
    Batch size for use with dataset (ignored if dataset=None).

augmentation : SomeAugmentation | None, (default=None)
    Compatible maite augmentation.

return_augmented_data : bool, (default=False)
    Set to True to return post-augmentation data as a function output.

return_preds : bool, (default=False)
    Set to True to return raw predictions as a function output.

collate_fn : Callable[[Iterable[tuple[T_input, T_target, T_metadata]]], tuple[Sequence[T_input], Sequence[T_target], Sequence[T_metadata]] ], (default=None)
    Callable responsible for transforming an iterable of 3-tuples where each encodes a single
    datapoint in some batch into a tuple of 3 sequences that each represent a batch of collated
    inputs, collated targets, and collated metadata, respectively. Defaults to naively push
    elements from input iterable onto sequences in their order of iteration.

Returns
-------
tuple[dict[str, Any], Sequence[TargetType], Sequence[tuple[Sequence[InputType], Sequence[TargetType], Sequence[DatumMetadataType]]]]
    Tuple of returned metric value, sequence of model predictions, and
    sequence of data batch tuples fed to the model during inference. The actual
    types represented by InputType, TargetType, and DatumMetadataType will vary
    by the AI task of the components provided as input arguments (e.g., image
    classification or object detection.)
    Note that the second and third return arguments will be empty if
    return_augmented_data is False or return_preds is False, respectively.

Raises
------
ValueError
    If neither a dataloader nor a dataset is provided</pre>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span><span class="n">predict</span><span class="o">.</span><span class="vm">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Make</span> <span class="n">predictions</span> <span class="k">for</span> <span class="n">a</span> <span class="n">given</span> <span class="n">model</span> <span class="o">&amp;</span> <span class="n">data</span> <span class="n">source</span> <span class="k">with</span> <span class="n">optional</span> <span class="n">augmentation</span><span class="o">.</span>

<span class="n">Parameters</span>
<span class="o">----------</span>
<span class="n">model</span> <span class="p">:</span> <span class="n">SomeModel</span>
    <span class="n">Maite</span> <span class="n">Model</span> <span class="nb">object</span><span class="o">.</span>

<span class="n">dataloader</span> <span class="p">:</span> <span class="n">SomeDataloader</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">Compatible</span> <span class="n">maite</span> <span class="n">dataloader</span><span class="o">.</span>

<span class="n">dataset</span> <span class="p">:</span> <span class="n">SomeDataset</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">Compatible</span> <span class="n">maite</span> <span class="n">dataset</span><span class="o">.</span>

<span class="n">batch_size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">Batch</span> <span class="n">size</span> <span class="k">for</span> <span class="n">use</span> <span class="k">with</span> <span class="n">dataset</span> <span class="p">(</span><span class="n">ignored</span> <span class="k">if</span> <span class="n">dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span>

<span class="n">augmentation</span> <span class="p">:</span> <span class="n">SomeAugmentation</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">Compatible</span> <span class="n">maite</span> <span class="n">augmentation</span><span class="o">.</span>

<span class="n">return_augmented_data</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Set</span> <span class="n">to</span> <span class="kc">True</span> <span class="n">to</span> <span class="k">return</span> <span class="n">post</span><span class="o">-</span><span class="n">augmentation</span> <span class="n">data</span> <span class="k">as</span> <span class="n">a</span> <span class="n">function</span> <span class="n">output</span><span class="o">.</span>

<span class="n">Returns</span>
<span class="o">-------</span>
<span class="nb">tuple</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">SomeTargetType</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">SomeInputType</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">SomeTargetType</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">SomeMetadataType</span><span class="p">]]],</span>
    <span class="n">A</span> <span class="nb">tuple</span> <span class="n">of</span> <span class="n">the</span> <span class="n">predictions</span> <span class="p">(</span><span class="k">as</span> <span class="n">a</span> <span class="n">sequence</span> <span class="n">of</span> <span class="n">batches</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">sequence</span>
    <span class="n">of</span> <span class="n">tuples</span> <span class="n">containing</span> <span class="n">the</span> <span class="n">information</span> <span class="n">associated</span> <span class="k">with</span> <span class="n">each</span> <span class="n">batch</span><span class="o">.</span>
    <span class="n">Note</span> <span class="n">that</span> <span class="n">the</span> <span class="n">second</span> <span class="k">return</span> <span class="n">argument</span> <span class="n">will</span> <span class="n">be</span> <span class="n">empty</span> <span class="k">if</span>
    <span class="n">return_augmented_data</span> <span class="ow">is</span> <span class="kc">False</span><span class="o">.</span>

<span class="n">Raises</span>
<span class="o">------</span>
<span class="ne">ValueError</span>
    <span class="n">If</span> <span class="n">neither</span> <span class="n">a</span> <span class="n">dataloader</span> <span class="n">nor</span> <span class="n">a</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="n">provided</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>
</article>
<footer class="prev-next-footer d-print-none">
<div class="prev-next-area">
<a class="left-prev" href="../explanation.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Explanation</p>
</div>
</a>
<a class="right-next" href="type_hints_for_API_design.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&amp;E Framework</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<dialog id="pst-secondary-sidebar-modal"></dialog>
<div class="bd-sidebar-secondary bd-toc" id="pst-secondary-sidebar"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concept-bridging-arraylikes">1 Concept: Bridging ArrayLikes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-types">2 Data Types</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification">2.1 Image Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection">2.2 Object Detection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models">3 Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-and-dataloaders">4 Datasets and DataLoaders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#augmentations">5 Augmentations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics">6 Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#procedures">7 Procedures</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div aria-label="source link" role="note">
<h3>This Page</h3>
<ul class="this-page-menu">
<li><a href="../_sources/explanation/protocol_overview.rst.txt" rel="nofollow">Show Source</a></li>
</ul>
</div></div>
</div></div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script defer="" src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer="" src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024 Massachusetts Institute of Technology.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item">
<p class="theme-version">
<!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
</div>
</div>
</footer>
</body>
</html>