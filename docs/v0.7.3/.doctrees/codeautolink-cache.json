{
  "api_reference": [],
  "changes": [],
  "explanation": [],
  "explanation/protocol_overview": [
    {
      "source": "import numpy as np\nimport torch\n\nfrom maite.protocols import ArrayLike\n\ndef my_numpy_fn(x: ArrayLike) -> np.ndarray:\n    arr = np.asarray(x)\n    # ...\n    return arr\n\ndef my_torch_fn(x: ArrayLike) -> torch.Tensor:\n    tensor = torch.as_tensor(x)\n    # ...\n    return tensor\n\n# can apply NumPy function to PyTorch Tensor\nnp_out = my_numpy_fn(torch.rand(2, 3))\n\n# can apply PyTorch function to NumPy array\ntorch_out = my_torch_fn(np.random.rand(2, 3))\n\n# note: no performance hit from conversion when all `ArrayLike`s are from same library\n# or when can share the same underlying memory\ntorch_out = my_torch_fn(torch.rand(2, 3))",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "asarray"
          ],
          "code_str": "np.asarray",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "numpy.asarray"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "numpy",
            "random",
            "rand"
          ],
          "code_str": "np.random.rand",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "numpy.random.rand"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "concept-bridging-arraylikes",
        "headings": [
          "Overview of MAITE Protocols",
          "1 Concept: Bridging ArrayLikes"
        ]
      },
      "doc_lineno": 37
    },
    {
      "source": "# define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)\nDatumMetadataType(TypedDict):\n    id: str|int\n\nInputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\nTargetType: TypeAlias = ArrayLike  # shape-(Cl) tensor of one-hot encoded true class or predicted probabilities\n\nInputBatchType: TypeAlias = Sequence[ArrayLike]  # element shape-(C, H, W) tensor of N images\nTargetBatchType: TypeAlias = Sequence[ArrayLike]  # element shape-(Cl,)\nDatumMetadataBatchType: TypeAlias = Sequence[DatumMetadataType]",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 94
    },
    {
      "source": "# import protocol classes\nfrom maite.protocols.image_classification import (\n    Dataset,\n    DataLoader,\n    Model,\n    Augmentation,\n    Metric\n)\n\n# import type aliases\nfrom maite.protocols.image_classification import (\n    InputType,\n    TargetType,\n    DatumMetadataType,\n    InputBatchType,\n    TargetBatchType,\n    DatumMetadataBatchType\n)",
      "names": [
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset"
          ],
          "code_str": "Dataset",
          "lineno": 2,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "DataLoader"
          ],
          "code_str": "DataLoader",
          "lineno": 2,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.DataLoader"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Model"
          ],
          "code_str": "Model",
          "lineno": 2,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Model"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation"
          ],
          "code_str": "Augmentation",
          "lineno": 2,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Metric"
          ],
          "code_str": "Metric",
          "lineno": 2,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite._internals.protocols.image_classification.Metric"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "InputBatchType"
          ],
          "code_str": "InputBatchType",
          "lineno": 11,
          "end_lineno": 18,
          "context": "import_target",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "TargetBatchType"
          ],
          "code_str": "TargetBatchType",
          "lineno": 11,
          "end_lineno": 18,
          "context": "import_target",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "DatumMetadataBatchType"
          ],
          "code_str": "DatumMetadataBatchType",
          "lineno": 11,
          "end_lineno": 18,
          "context": "import_target",
          "resolved_location": "collections.abc.Sequence"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 124
    },
    {
      "source": "import maite.protocols.image_classification as ic\n\n# model: ic.Model = load_model(...)",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "image-classification",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.1 Image Classification"
        ]
      },
      "doc_lineno": 148
    },
    {
      "source": "# define type to store an id of each datum (additional fields can be added by defining structurally-assignable TypedDict)\nDatumMetadataType(TypedDict):\n    id: str|int\n\nclass ObjectDetectionTarget(Protocol):\n    @property\n    def boxes(self) -> ArrayLike: ...  # shape-(D_i, 4) tensor of bounding boxes w/format X0, Y0, X1, Y1\n\n    @property\n    def labels(self) -> ArrayLike: ... # shape-(D_i) tensor of labels for each box\n\n    @property\n    def scores(self) -> ArrayLike: ... # shape-(D_i) tensor of scores for each box (e.g., probabilities)\n\nInputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\nTargetType: TypeAlias = ObjectDetectionTarget\n\nInputBatchType: TypeAlias = Sequence[ArrayLike]  # sequence of N ArrayLikes each of shape (C, H, W)\nTargetBatchType: TypeAlias = Sequence[TargetType]  # sequence of object detection \"target\" objects\nDatumMetadataBatchType: TypeAlias = Sequence[DatumMetadataType]",
      "names": [],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "object-detection",
        "headings": [
          "Overview of MAITE Protocols",
          "2 Data Types",
          "2.2 Object Detection"
        ]
      },
      "doc_lineno": 159
    },
    {
      "source": "import maite.protocols.image_classification as ic\nprint(ic.Model.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "models",
        "headings": [
          "Overview of MAITE Protocols",
          "3 Models"
        ]
      },
      "doc_lineno": 195
    },
    {
      "source": "import maite.protocols.object_detection as od\nprint(od.Model.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "models",
        "headings": [
          "Overview of MAITE Protocols",
          "3 Models"
        ]
      },
      "doc_lineno": 292
    },
    {
      "source": "print(ic.Dataset.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 399
    },
    {
      "source": "print(ic.DataLoader.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 496
    },
    {
      "source": "print(od.DataLoader.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "datasets-and-dataloaders",
        "headings": [
          "Overview of MAITE Protocols",
          "4 Datasets and DataLoaders"
        ]
      },
      "doc_lineno": 527
    },
    {
      "source": "print(ic.Augmentation.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "augmentations",
        "headings": [
          "Overview of MAITE Protocols",
          "5 Augmentations"
        ]
      },
      "doc_lineno": 572
    },
    {
      "source": "print(od.Augmentation.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "augmentations",
        "headings": [
          "Overview of MAITE Protocols",
          "5 Augmentations"
        ]
      },
      "doc_lineno": 651
    },
    {
      "source": "print(ic.Metric.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "metrics",
        "headings": [
          "Overview of MAITE Protocols",
          "6 Metrics"
        ]
      },
      "doc_lineno": 791
    },
    {
      "source": "print(od.Metric.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "metrics",
        "headings": [
          "Overview of MAITE Protocols",
          "6 Metrics"
        ]
      },
      "doc_lineno": 888
    },
    {
      "source": "from maite.workflows import evaluate, predict\n# we can also import from object_detection module\n# where the function call signature is the same",
      "names": [
        {
          "import_components": [
            "maite",
            "workflows",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.workflows.evaluate"
        },
        {
          "import_components": [
            "maite",
            "workflows",
            "predict"
          ],
          "code_str": "predict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.workflows.predict"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "workflows",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Workflows"
        ]
      },
      "doc_lineno": 1060
    },
    {
      "source": "print(evaluate.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "workflows",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Workflows"
        ]
      },
      "doc_lineno": 1066
    },
    {
      "source": "print(predict.__doc__)",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "explanation/protocol_overview",
        "ref_id": "workflows",
        "headings": [
          "Overview of MAITE Protocols",
          "7 Workflows"
        ]
      },
      "doc_lineno": 1118
    }
  ],
  "explanation/type_hints_for_API_design": [
    {
      "source": "def count_vowels(x: str) -> int:  # `: str` and `-> int` are the annotations\n    return sum(1 for char in x if char in set(\"aeiouAEIOU\"))\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "set"
          ],
          "code_str": "set",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "set"
        },
        {
          "import_components": [
            "sum"
          ],
          "code_str": "sum",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "sum"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "a-quick-introduction-to-writing-statically-typed-python-code",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "A quick introduction to writing statically typed Python code"
        ]
      },
      "doc_lineno": 24
    },
    {
      "source": "from typing import Callable, List, Mapping, Optional, TypeVar, Union, Protocol, Literal\n\n# The following are type annotations that could be included, e.g., in a\n# function' signature\n\n# Either an integer or a float\nUnion[int, float]\n\n# Either a boolean or `None`\nOptional[bool]\n\n# Either the string 'cat' or the string 'dog'\nLiteral[\"cat\", \"dog\"]\n\n# Any object supporting the call syntax \u2013 f(...) \u2013 that accepts three\n# integer-values inputs and returns a string\nCallable[[int, int, int], str]\n\n# A list of an arbitrary number of strings\nList[str]\n\n# A mapping, such as a dictionary, whose keys are strings\n# and whose values are floats\nMapping[str, float]\n\n# A type named Person that has two attributes: `name` (str) and `age` (int)\n# and a \"greeting\" method, which accepts no inputs and returns a string\nclass Person:\n    name: str\n    age: int\n\n    def greeting(self) -> str:\n      ...\n\n# A function that accepts a single input and returns an output of the\n# same type\nT = TypeVar(\"T\")\ndef type_preserving_func(x: T) -> T:\n  ...\n\n# The following describes *any* type that exposes the method: \n# `<obj>.open(file_path: str)`\n# Note: this is a protocol, which enables a feature known as \"structural subtyping\".\n# This will be an important feature that we discuss later\nclass Openable(Protocol):\n    def open(self, file_path: str):\n        ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "List",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "typing",
            "Mapping"
          ],
          "code_str": "Mapping",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Mapping"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "typing",
            "TypeVar"
          ],
          "code_str": "TypeVar",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "Union"
          ],
          "code_str": "Union",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Union"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Literal"
        },
        {
          "import_components": [
            "typing",
            "Union"
          ],
          "code_str": "Union",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Union"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "bool"
          ],
          "code_str": "bool",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "bool"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "typing.Literal"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "List",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "Mapping"
          ],
          "code_str": "Mapping",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "typing.Mapping"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 32,
          "end_lineno": 32,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "TypeVar"
          ],
          "code_str": "TypeVar",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 38,
          "end_lineno": 38,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "TypeVar",
            "()"
          ],
          "code_str": "T",
          "lineno": 38,
          "end_lineno": 38,
          "context": "none",
          "resolved_location": "typing.TypeVar"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 45,
          "end_lineno": 45,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 46,
          "end_lineno": 46,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "a-quick-introduction-to-writing-statically-typed-python-code",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "A quick introduction to writing statically typed Python code"
        ]
      },
      "doc_lineno": 36
    },
    {
      "source": "# contents of example.py\nfrom typing import Iterable\n\ndef get_data_registry() -> dict[str, int]:\n    ...\n\ndef process_data(x: Iterable[int]) -> int:\n    data_total = sum(x)\n    return data_total\n\ndef run_app():\n    registry = get_data_registry()\n    process_data(registry.keys())  # <-- static type checker flags error here!\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "sum"
          ],
          "code_str": "sum",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "sum"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "static-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Static Type Checkers"
        ]
      },
      "doc_lineno": 96
    },
    {
      "source": "# Demonstrating pyright's ability to infer types through un-annotated functions\ndef make_int() -> int: ...\ndef add(x, y): return x + y  # note: not annotated!\n\nx, y = make_int(), make_int()\nz = add(x, y)\n\nreveal_type(z)  # pyright reveals: int, mypy reveals: Any\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "static-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Static Type Checkers"
        ]
      },
      "doc_lineno": 132
    },
    {
      "source": "from beartype import beartype\n\n@beartype\ndef process_age(age: int) -> int:\n    return age\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "runtime-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Runtime type checkers"
        ]
      },
      "doc_lineno": 154
    },
    {
      "source": ">>> process_age(\"hello\")\nBeartypeCallHintParamViolation: @beartyped process_age() parameter x='hello' violates type hint class 'int', as 'hello' not instance of int.\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "runtime-type-checkers",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Tools that make type annotations worthwhile",
          "Runtime type checkers"
        ]
      },
      "doc_lineno": 163
    },
    {
      "source": "# before PEP 585\nimport typing\n\ndef f(ages: typing.List[int], records: typing.Dict[str, int]): ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "List"
          ],
          "code_str": "typing.List",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "typing.List"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "typing.Dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 257
    },
    {
      "source": "# after PEP 585\nfrom __future__ import annotations  # required for Python < 3.9\n\ndef f(ages: list[int], records: dict[str, int]): ...\n",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 263
    },
    {
      "source": "# before PEP 604\ndef f(x: typing.Union[int, str]): ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 272
    },
    {
      "source": "# after PEP 604\nfrom __future__ import annotations  # required for Python < 3.10\n\ndef f(x: int | str): ...\n",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 276
    },
    {
      "source": "from typing import Sequence\nfrom typing_extensions import TypeAlias\nimport torch as tr\n\nScalars: TypeAlias = int | float | complex\n# Supports array-likes from 0D to 2D structures\nArrayLike: TypeAlias = Scalars | Sequence[Scalars] | Sequence[Sequence[Scalars]] \n\ndef to_tensor(x: ArrayLike) -> tr.Tensor:\n    ...\n\nto_tensor(0)  # static type checker: OK\nto_tensor([1, []])  # static type checker: ERROR!\nto_tensor([1, 1])  # static type checker: OK\nto_tensor([[2+1j, 3+0j], [1-8j, 2+10j]])  # static type checker: OK\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "complex"
          ],
          "code_str": "complex",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "complex"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 285
    },
    {
      "source": "from typing_extensions import TypeVarTuple, Unpack, TypeAlias\nfrom typing import Generic, Any\nimport torch\nfrom typing import NewType\n\nShape = TypeVarTuple(\"Shape\")\n\n# A PyTorch tensor with additional shape type information\n# This is a so-called \"variadic generic\": the Shape type variable can vary in length/contents\nclass Tensor(Generic[Unpack[Shape]], torch.Tensor):\n    ...\n\n# Declaring descriptive aliases for common array dimensions\nHeight: TypeAlias = int\nWidth: TypeAlias = int\nChannel: TypeAlias = int\nTime: TypeAlias = int\nBatch: TypeAlias = int\n\n# Some representative utility functions for loading tensor data\ndef load_time_series(path: str) -> Tensor[Time]: ...\ndef load_image(path: str) -> Tensor[Channel, Height, Width]: ...\ndef load_video(path: str) -> Tensor[Time, Channel, Height, Width]: ...\n\n# Some functions working with tensors..\n# Stack multiple Tensors along a leading \"Batch\" dimension\ndef stack(*arrs: Tensor[Unpack[Shape]]) -> Tensor[Batch, Unpack[Shape]]: ...\n\n# Get the resolution, HxW, from any shape-(..., H, W) tensor\ndef get_img_resolution(img: Tensor[Unpack[tuple[Any, ...]], Height, Width]) -> tuple[Height, Width]: ...\n\nlist_of_images = [load_image(p) for p in [\"a.png\", \"b.png\"]]  # list[Tensor[Channel, Height, Width]]\nimg_tensor = stack(*list_of_images)  # Tensor[Batch, Channel, Height, Width]\nimg_res = get_img_resolution(img_tensor)  # Tuple[Height, Width]\n\nlist_of_videos = [load_video(p) for p in [\"a.mp4\", \"b.mp4\"]]  # list[Tensor[Time, Channel, Height, Width]]\nvideo_tensor = stack(*list_of_videos)  # Tensor[Batch, Time, Channel, Height, Width]\nvideo_res = get_img_resolution(video_tensor)  # Tuple[Height, Width]\n\ntime_series = load_time_series(\"data.pkl\")  # Tensor[Time]\n# attempting to get the resolution of a shape-(Time,) tensor...\nget_img_resolution(time_series)  # static type check: error!\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Generic"
          ],
          "code_str": "Generic",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Generic"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "NewType"
          ],
          "code_str": "NewType",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.NewType"
        },
        {
          "import_components": [
            "typing",
            "Generic"
          ],
          "code_str": "Generic",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "typing.Generic"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "typing.Any"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 311
    },
    {
      "source": "img_tensor = load_image(\"img.png\")  # type-checker sees: Tensor[Channel, Height, Width]\nimg_tensor = img_tensor * 2  # type-checker sees: Tensor\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "on-using-annotations-to-write-legible-documentation",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "On using annotations to write legible documentation"
        ]
      },
      "doc_lineno": 360
    },
    {
      "source": "import abc\nfrom typing import Sequence, Any, Dict, Tuple\nfrom typing_extensions import TypeAlias\nfrom torch import Tensor\n\nfrom our_library import BoundingBox\n\nClassScores: TypeAlias = Dict[Any, float]\n\nclass OurDetectorAPI(abc.ABC):\n    @abc.abstractmethod\n    def detect(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]: \n        raise NotImplemented() \n",
      "names": [
        {
          "import_components": [
            "abc"
          ],
          "code_str": "abc",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "abc"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "abc",
            "ABC"
          ],
          "code_str": "abc.ABC",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "abc.ABC"
        },
        {
          "import_components": [
            "abc",
            "abstractmethod"
          ],
          "code_str": "abc.abstractmethod",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "abc.abstractmethod"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "NotImplemented"
          ],
          "code_str": "NotImplemented",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "NotImplemented"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 380
    },
    {
      "source": "class BoundingBox:\n    def __init__(self, left: float, top: float, right: float, bottom: float):\n        # check that bbox coords satisfy, e.g., left <= right\n        # use bbox coords to construct vertices\n        ...\n    def compute_box_area(self) -> float: ...\n    def get_intersection(self, other_box: \"BoundingBox\") -> \"BoundingBox\": ...\n",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "float"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 398
    },
    {
      "source": "def measure_detector_precision_and_recall(model: OurDetectorAPI) -> float:\n    if not isinstance(model, OurDetectorAPI):\n        raise TypeError(\"You've gotta be one of us!\")\n    \n    data = load_data()\n    detections = model.detect(data)\n    ...\n",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 410
    },
    {
      "source": "from our_library import BoundingBox, OurDetectorAPI\nfrom their_library import TheirDetector\n\nclass SadCompatShim(OurDetectorAPI):\n    def __init__(self, actual_detector: TheirDetector):\n        self.det = actual_detector\n\n    # this is the best-case scenario\n    def detect(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]:\n        their_bboxes, their_scores = self.det.their_detection_method(img)\n        our_bboxes = [BoundingBox(*bbox) for bbox in their_bboxes]\n        return list(zip(our_bboxes, their_scores))\n",
      "names": [
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 431
    },
    {
      "source": "from typing import Any, Dict, Sequence, Tuple, Protocol, runtime_checkable\n\nfrom torch import Tensor\nfrom typing_extensions import TypeAlias, runtime_checkable\n\nClassScores: TypeAlias = Dict[Any, float]\n\n@runtime_checkable\nclass BoundingBox(Protocol):\n    left: float\n    top: float\n    right: float\n    bottom: float\n\n@runtime_checkable  # <-- enables `isinstance` checks to look for necessary structure [1]\nclass OurDetectorAPI(Protocol):\n    def __call__(self, img: Tensor) -> Sequence[Tuple[BoundingBox, ClassScores]]:\n        ...\n\ndef measure_detector_precision_and_recall(model: OurDetectorAPI) -> float:\n    if not isinstance(model, OurDetectorAPI):  # <-- [1]: I.e, this still works!\n        raise TypeError(\"You've gotta be one of us!\")\n    \n    data = load_data()\n    detections = model(data)\n    ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "runtime_checkable"
          ],
          "code_str": "runtime_checkable",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.runtime_checkable"
        },
        {
          "import_components": [
            "typing",
            "Dict"
          ],
          "code_str": "Dict",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "typing.Dict"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "typing",
            "Protocol"
          ],
          "code_str": "Protocol",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "typing.Protocol"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "typing",
            "Tuple"
          ],
          "code_str": "Tuple",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "typing.Tuple"
        },
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 459
    },
    {
      "source": "# Implementing an \"empty\" detector in the old API\nfrom our_library import OurDetectorAPI  # <- our library must be installed\n\nclass EmptyDetector(OurDetectorAPI):\n    def detect(self, img):\n        return []\n\nempty_detector = EmptyDetector()\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 497
    },
    {
      "source": "# Implementing an \"empty\" detector in the new API\nempty_detector = lambda img: []\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 510
    },
    {
      "source": "@runtime_checkable\nclass Configurable(Protocol):\n    def ___special_config_interface__(self) -> dict[str, Any]: ...\n\nclass ConfigurableDetector(OurDetectorAPI, Configurable, Protocol): ...\n\ndef orchestrate_detector(model: ConfigurableDetector): ...\n",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 522
    },
    {
      "source": "empty_detector = lambda x: []\norchestrate_detector(empty_detector)  # static type checker: error\n",
      "names": [],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "typed-interfaces-should-be-informative-inspire-good-design-and-be-easy-to-satisfy",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Typed interfaces should be informative, inspire good design, and be easy to satisfy"
        ]
      },
      "doc_lineno": 534
    },
    {
      "source": "from torch import Tensor\nimport torch.nn as nn\nfrom typing import Iterable\n\ndef load_data() -> Tensor: ...\ndef load_model() -> nn.Module: ...\n\ndef measure_data_distr(img_batch: Tensor):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"Not image batch-like\")\n    # <actual functionality here>\n\ndef compute_accuracy(img_batch: Tensor, model: nn.Module):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"not batch-like\")\n    # <actual functionality here>\n\ndef compute_calibration(img_batch: Tensor, models: Iterable[nn.Module]):\n    if not batch.ndim == 4 or not batch.shape[1] == 3:\n        raise TypeError(\"Not image batch-like\")\n    # <actual functionality here>\n\nif __name__ == \"__main__\":\n    tensor = load_data()\n    model = load_model()\n\n    measure_data_distr(tensor)\n    compute_accuracy(tensor, model)\n    compute_calibration(tensor, [model])\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "validate-early-in-your-program-and-use-narrow-types-to-prove-that-you-did-so",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so"
        ]
      },
      "doc_lineno": 548
    },
    {
      "source": "from typing import cast\n\n# 0. Define types that describe specific validated states that your\n#    library depends on across multiple interfaces\nfrom our_library.narrow_types import NonEmpty\n\n# Returns unstructured/unvalidated data\ndef stream_data() -> tuple[str, ...]: ...\n\n# Create functions that can validate that the data satisfies\n# specific properties and ascribe to the validated data a new \n# type, which serves as proof of validation\ndef parse_stream(stream: tuple[str, ...]) -> NonEmpty[tuple[str, ...]]:\n    if not isinstance(stream, tuple): raise TypeError(\"not tuple\")\n    if not stream: raise TypeError(\"is empty\")\n    if not all(isinstance(item, str) for item in stream): raise TypeError(\"not strings\")\n    \n    # The sole purpose of this line is for infroming the static type checker.\n    # We don't actually use the `NonEmpty` type to change our data\n    # at all!\n    proven_data = cast(NonEmpty[tuple[str, ...]], stream)  \n    return proven_data\n\n# Design downstream functions to require this narrowed type, which can only\n# be obtained by going through the parsing process. Now functions operate\n# safely without having to re-validate the data at every turn, and their\n# requirements are now explicitly documented via annotations\ndef consumer1(data: NonEmpty[tuple[str, ...]]): ...\ndef consumer2(data: NonEmpty[tuple[str, ...]]): ...\ndef consumer3(data: NonEmpty[tuple[str, ...]]): ...\n\nif __name__ == \"__main__\":\n    # 1. Start with unstructured data\n    data = stream_data()  # type checker sees: tuple[str, ...]\n    \n    # Attempting to pass `data` to, e.g., `consumer1` would produce\n    # a static type checking error.\n\n    # 2. Enter \"parsing\" phase of program, where we validate the data\n    #    and ascribe a narrowed type to the data.\n    #    This is where we handle and log errors.\n    try:\n        # input: tuple[str, ...]  (less-structured)\n        parsed_data = parse_stream(data)\n        # output: NonEmpty[tuple[str, ...]]  (more-structured)\n    except TypeError:\n        # log error\n        # cue graceful recovery\n        ...\n\n    # 3. Enter \"execution\" phase of program: the 'illegal state' of having empty data\n    #    here is impossible, assuming we rely faithfully on our type checker, because\n    #    we are working with data that has been \"proven\" to be valid\n    consumer1(parsed_data)  # type checker: OK\n    consumer2(parsed_data)  # type checker: OK\n    consumer3(parsed_data)  # type checker: OK\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "all"
          ],
          "code_str": "all",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "all"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "TypeError"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 28,
          "end_lineno": 28,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 28,
          "end_lineno": 28,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 46,
          "end_lineno": 46,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "validate-early-in-your-program-and-use-narrow-types-to-prove-that-you-did-so",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so"
        ]
      },
      "doc_lineno": 592
    },
    {
      "source": "x: Any   # starting with: x can be Any type\n\n# narrow x via isinstance:\nif isinstance(x, int):\n    # type checker narrows x to `int` here\n    ...\nelif isinstance(x, str):\n    # type checker narrows x to `str` here\n    ...\n\ny: int | list[int]  # starting with: y can be an int or list of ints\n\n# narrow y via assert\nassert isinstance(y, list)\ny  # type checker narows y to list[int]  (it is impossible for y to be an int here at runtime)\n\n# via casting\nfrom typing import cast\n\nz: Any  # starting with: x can be Any type\n\n# Warning: you can lie to the type checker using `cast`.\n# `cast` doesn't do any processing at runtime\nout = cast(tuple[int, int], z)  # type checker sees `out` as `tuple[int, int]`\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 18,
          "end_lineno": 18,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 18,
          "end_lineno": 18,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "typing.cast"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 659
    },
    {
      "source": "from typing import Any\n\nfrom typing_extensions import TypeGuard\n\nclass NonNegativeInt(int):\n    ...\n\n# this is our type-guard, which can narrow int -> NonNegativeInt\ndef is_non_negative_int(x: int) -> TypeGuard[NonNegativeInt]: \n    return 0 < x\n\ndef process_age(x: NonNegativeInt): ...\n\ndef main(x: int):\n    if is_non_negative_int(x):\n        # x is narrowed to NonNegativeInt\n        process_age(x)\n    else:\n        # x is an int here\n        # log error\n        ...\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 688
    },
    {
      "source": "from torch import Tensor\nimport torch.nn as nn\nfrom typing import cast, Iterable\nfrom typing_extensions import TypeGuard\n\n\nclass TensorBCHW(Tensor):\n    \"\"\"Signals that a PyTorch tensor has been validated to\n    be shaped like a batch of images: (B, C, H, W)\"\"\"\n    ...\n\ndef load_data() -> Tensor: ...\ndef load_model() -> nn.Module: ...\n\n\ndef is_batch_of_images(\n    x: Tensor, expected_channel_size: int\n) -> TypeGuard[TensorBCHW]:\n    return isinstance(x, Tensor) and x.ndim == 4 and x.shape[1] == expected_channel_size\n        \n\ndef measure_data_distr(batch: TensorBCHW): ...\n\ndef compute_accuracy(batch: TensorBCHW, model: nn.Module): ...\n\ndef compute_calibration(batch: TensorBCHW, models: Iterable[nn.Module]): ...\n\n\nif __name__ == \"__main__\":\n    model = load_model()\n    tensor = load_data()\n    \n    # type checker sees tensor as: Tensor\n    if not is_batch_of_images(tensor, expected_channel_size=3):\n        raise TypeError(\"not a batch!\")\n    # type checker sees tensor as: TensorBCHW\n\n\n    # static type-checker ensures input is `TensorBCHW`\n    measure_data_distr(tensor)\n    compute_accuracy(tensor, model)\n    compute_calibration(tensor, [model])\n",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "cast"
          ],
          "code_str": "cast",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.cast"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "isinstance"
          ],
          "code_str": "isinstance",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "isinstance"
        },
        {
          "import_components": [
            "typing",
            "Iterable"
          ],
          "code_str": "Iterable",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "typing.Iterable"
        },
        {
          "import_components": [
            "TypeError"
          ],
          "code_str": "TypeError",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "TypeError"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "type-narrowing",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Motivating the Adoption of Specific Typing Features and Methods in the T&E Framework",
          "Validate early in your program and use narrow types to prove that you did so",
          "Type narrowing"
        ]
      },
      "doc_lineno": 719
    },
    {
      "source": "def count_vowels(text: str) -> int: ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "additional-resources",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Additional resources"
        ]
      },
      "doc_lineno": 234
    },
    {
      "source": "def count_vowels(text: Iterable[Hashable]) -> int: ...\n",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "explanation/type_hints_for_API_design",
        "ref_id": "additional-resources",
        "headings": [
          "A Primer on Python Typing: Relevant Language Features, Methods, and Tools for the T&E Framework",
          "Additional resources"
        ]
      },
      "doc_lineno": 239
    }
  ],
  "generated/maite.errors.InvalidArgument": [],
  "generated/maite.errors.MaiteException": [],
  "generated/maite.protocols.ArrayLike": [],
  "generated/maite.protocols.image_classification.Augmentation": [
    {
      "source": ">>> import copy\n>>> import numpy as np\n>>> from typing import Any\n>>> from collections.abc import Sequence\n>>> from maite.protocols import ArrayLike, DatumMetadata, AugmentationMetadata\n>>>\n>>> class EnrichedDatumMetadata(DatumMetadata):\n...     new_key: int  # add a field to those already in DatumMetadata\n...\n>>> class ImageAugmentation:\n...     def __init__(self, aug_name: str):\n...         self.metadata: AugmentationMetadata = {'id': aug_name}\n...     def __call__(\n...         self,\n...         data_batch: tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[DatumMetadata]]\n...     ) -> tuple[Sequence[np.ndarray], Sequence[np.ndarray], Sequence[EnrichedDatumMetadata]]:\n...         inputs, targets, mds = data_batch\n...         # We copy data passed into the constructor to avoid mutating original inputs\n...         # By using np.ndarray constructor, the static type-checker will let us treat\n...         # generic ArrayLike as a more narrow return type\n...         inputs_aug = [copy.copy(np.array(input)) for input in inputs]\n...         targets_aug = [copy.copy(np.array(target)) for target in targets]\n...         # Modify inputs_aug, targets_aug, or mds_aug as needed\n...         # In this example, we just add a new metadata field\n...         mds_aug = []\n...         for i, md in enumerate(mds):\n...             mds_aug.append(EnrichedDatumMetadata(**md, new_key=i))\n...         return inputs_aug, targets_aug, mds_aug",
      "names": [
        {
          "import_components": [
            "copy"
          ],
          "code_str": "copy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "copy"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "collections",
            "abc"
          ],
          "code_str": "collections.abc",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "collections.abc"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "numpy",
            "ndarray"
          ],
          "code_str": "np.ndarray",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "numpy.ndarray"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "collections",
            "abc",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "collections.abc.Sequence"
        },
        {
          "import_components": [
            "numpy",
            "array"
          ],
          "code_str": "np.array",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "numpy.array"
        },
        {
          "import_components": [
            "copy",
            "copy"
          ],
          "code_str": "copy.copy",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "copy.copy"
        },
        {
          "import_components": [
            "numpy",
            "array"
          ],
          "code_str": "np.array",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "numpy.array"
        },
        {
          "import_components": [
            "copy",
            "copy"
          ],
          "code_str": "copy.copy",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "copy.copy"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Augmentation",
        "ref_id": "maite-protocols-image-classification-augmentation",
        "headings": [
          "maite.protocols.image_classification.Augmentation"
        ]
      },
      "doc_lineno": 57
    },
    {
      "source": ">>> from maite.protocols import image_classification as ic\n>>> im_aug: ic.Augmentation = ImageAugmentation(aug_name = 'an_augmentation')",
      "names": [
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation"
          ],
          "code_str": "ic.Augmentation",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Augmentation",
            "()"
          ],
          "code_str": "im_aug",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Augmentation"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Augmentation",
        "ref_id": "maite-protocols-image-classification-augmentation",
        "headings": [
          "maite.protocols.image_classification.Augmentation"
        ]
      },
      "doc_lineno": 63
    }
  ],
  "generated/maite.protocols.image_classification.DataLoader": [],
  "generated/maite.protocols.image_classification.Dataset": [
    {
      "source": ">>> import numpy as np\n>>> from typing import Any, TypedDict\n>>> from maite.protocols import ArrayLike, DatasetMetadata",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "TypedDict"
          ],
          "code_str": "TypedDict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.TypedDict"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "maite-protocols-image-classification-dataset",
        "headings": [
          "maite.protocols.image_classification.Dataset"
        ]
      },
      "doc_lineno": 32
    },
    {
      "source": ">>> N_CLASSES: int = 5\n>>> N_DATUM: int = 10\n>>> images: list[np.ndarray] = [np.random.rand(3, 32, 16) for _ in range(N_DATUM)]\n>>> targets: np.ndarray = np.eye(N_CLASSES)[np.random.choice(N_CLASSES, N_DATUM)]",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "maite-protocols-image-classification-dataset",
        "headings": [
          "maite.protocols.image_classification.Dataset"
        ]
      },
      "doc_lineno": 40
    },
    {
      "source": ">>> class MyDatumMetadata(DatumMetadata):\n...     hour_of_day: float\n...\n>>> datum_metadata = [ MyDatumMetadata(id = i, hour_of_day=np.random.rand()*24) for i in range(N_DATUM) ]",
      "names": [
        {
          "import_components": [
            "float"
          ],
          "code_str": "float",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "float"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "maite-protocols-image-classification-dataset",
        "headings": [
          "maite.protocols.image_classification.Dataset"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": ">>> class ImageDataset:\n...     def __init__(self,\n...                  dataset_name: str,\n...                  index2label: dict[int,str],\n...                  images: list[np.ndarray],\n...                  targets: np.ndarray,\n...                  datum_metadata: list[MyDatumMetadata]):\n...         self.images = images\n...         self.targets = targets\n...         self.metadata = DatasetMetadata({'id': dataset_name, 'index2label': index2label})\n...         self._datum_metadata = datum_metadata\n...     def __len__(self) -> int:\n...         return len(images)\n...     def __getitem__(self, ind: int) -> tuple[np.ndarray, np.ndarray, MyDatumMetadata]:\n...         return self.images[ind], self.targets[ind], self._datum_metadata[ind]",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "maite-protocols-image-classification-dataset",
        "headings": [
          "maite.protocols.image_classification.Dataset"
        ]
      },
      "doc_lineno": 67
    },
    {
      "source": ">>> from maite.protocols import image_classification as ic\n>>> dataset: ic.Dataset = ImageDataset('a_dataset',\n...                                    {i: f\"class_name_{i}\" for i in range(N_CLASSES)},\n...                                    images, targets, datum_metadata)",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset"
          ],
          "code_str": "ic.Dataset",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "image_classification",
            "Dataset",
            "()"
          ],
          "code_str": "dataset",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.protocols.image_classification.Dataset"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Dataset",
        "ref_id": "maite-protocols-image-classification-dataset",
        "headings": [
          "maite.protocols.image_classification.Dataset"
        ]
      },
      "doc_lineno": 75
    }
  ],
  "generated/maite.protocols.image_classification.Metric": [
    {
      "source": ">>> from typing import Any, Sequence\n>>> import numpy as np\n>>> from maite.protocols import ArrayLike\n>>> from maite.protocols import image_classification as ic",
      "names": [
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "maite-protocols-image-classification-metric",
        "headings": [
          "maite.protocols.image_classification.Metric"
        ]
      },
      "doc_lineno": 25
    },
    {
      "source": ">>> class MyAccuracy:\n...    metadata: MetricMetadata = {'id': 'Example Multiclass Accuracy'}\n...\n...    def __init__(self):\n...        self._total = 0\n...        self._correct = 0\n...\n...    def reset(self) -> None:\n...        self._total = 0\n...        self._correct = 0\n...\n...    def update(self, preds: Sequence[ArrayLike], targets: Sequence[ArrayLike]) -> None:\n...        model_probs = [np.array(r) for r in preds]\n...        true_onehot = [np.array(r) for r in targets]\n...\n...        # Stack into single array, convert to class indices\n...        model_classes = np.vstack(model_probs).argmax(axis=1)\n...        truth_classes = np.vstack(true_onehot).argmax(axis=1)\n...\n...        # Compare classes and update running counts\n...        same = (model_classes == truth_classes)\n...        self._total += len(same)\n...        self._correct += same.sum()\n...\n...    def compute(self) -> dict[str, Any]:\n...        if self._total > 0:\n...            return {\"accuracy\": self._correct / self._total}\n...        else:\n...            raise Exception(\"No batches processed yet.\")",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "maite-protocols-image-classification-metric",
        "headings": [
          "maite.protocols.image_classification.Metric"
        ]
      },
      "doc_lineno": 55
    },
    {
      "source": ">>> accuracy: ic.Metric = MyAccuracy()",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "maite-protocols-image-classification-metric",
        "headings": [
          "maite.protocols.image_classification.Metric"
        ]
      },
      "doc_lineno": 60
    },
    {
      "source": ">>> # batch 1\n>>> model_probs = [np.array([0.8, 0.1, 0.0, 0.1]), np.array([0.1, 0.2, 0.6, 0.1])] # predicted classes: 0, 2\n>>> true_onehot = [np.array([1.0, 0.0, 0.0, 0.0]), np.array([0.0, 1.0, 0.0, 0.0])] # true classes: 0, 1\n>>> accuracy.update(model_probs, true_onehot)\n>>> print(accuracy.compute())\n{'accuracy': 0.5}\n>>>\n>>> # batch 2\n>>> model_probs = [np.array([0.1, 0.1, 0.7, 0.1]), np.array([0.0, 0.1, 0.0, 0.9])] # predicted classes: 2, 3\n>>> true_onehot = [np.array([0.0, 0.0, 1.0, 0.0]), np.array([0.0, 0.0, 0.0, 1.0])] # true classes: 2, 3\n>>> accuracy.update(model_probs, true_onehot)\n>>>\n>>> print(accuracy.compute())\n{'accuracy': 0.75}",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Metric",
        "ref_id": "maite-protocols-image-classification-metric",
        "headings": [
          "maite.protocols.image_classification.Metric"
        ]
      },
      "doc_lineno": 77
    }
  ],
  "generated/maite.protocols.image_classification.Model": [
    {
      "source": ">>> import maite.protocols.image_classification as ic\n>>> import numpy as np\n>>> import numpy.typing as npt\n>>> from maite.protocols import ArrayLike, ModelMetadata\n>>> from typing import Sequence",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "typing"
          ],
          "code_str": "numpy.typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy.typing"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "maite-protocols-image-classification-model",
        "headings": [
          "maite.protocols.image_classification.Model"
        ]
      },
      "doc_lineno": 28
    },
    {
      "source": ">>> class LinearClassifier:\n...     def __init__(self) -> None:\n...         # Set up required metadata attribute using the default `ModelMetadata` type,\n...         # using class name for the ID\n...         self.metadata: ModelMetadata = {\"id\": self.__class__.__name__}\n...\n...         # Initialize weights\n...         rng = np.random.default_rng(12345678)\n...         num_classes = 10\n...         flattened_size = 3 * 32 * 32\n...         self.weights = -0.2 + 0.4 * rng.random((flattened_size, num_classes))\n...         self.bias = -0.2 + 0.4 * rng.random((1, num_classes))\n...\n...     def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[npt.NDArray]:\n...         # Convert each element in batch to ndarray, flatten,\n...         # then combine into 4D array of shape-(N, C, H, W)\n...         batch_np = np.vstack([np.asarray(x).flatten() for x in batch])\n...\n...         # Send input batch through model\n...         out = batch_np @ self.weights + self.bias\n...         out = np.exp(out) / np.sum(np.exp(out), axis=1, keepdims=True) # softmax\n...\n...         # Restructure to sequence of shape-(10,) probabilities\n...         return [row for row in out]",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "maite-protocols-image-classification-model",
        "headings": [
          "maite.protocols.image_classification.Model"
        ]
      },
      "doc_lineno": 56
    },
    {
      "source": ">>> batch_size = 8\n>>> rng = np.random.default_rng(12345678)\n>>> batch: Sequence[ArrayLike] = [-0.2 + 0.4 * rng.random((3, 32, 32)) for _ in range(batch_size)]\n>>>\n>>> model: ic.Model = LinearClassifier()\n>>> out = model(batch)",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "maite-protocols-image-classification-model",
        "headings": [
          "maite.protocols.image_classification.Model"
        ]
      },
      "doc_lineno": 65
    },
    {
      "source": ">>> for probs in out:\n...     print(np.round(probs, 2))\n[0.16 0.1  0.16 0.14 0.04 0.02 0.06 0.04 0.17 0.1 ]\n[0.21 0.16 0.04 0.07 0.08 0.05 0.09 0.03 0.18 0.09]\n[0.15 0.11 0.13 0.11 0.09 0.09 0.07 0.04 0.19 0.02]\n[0.04 0.08 0.14 0.07 0.12 0.2  0.11 0.06 0.14 0.04]\n[0.03 0.08 0.06 0.05 0.17 0.18 0.09 0.03 0.12 0.19]\n[0.09 0.04 0.1  0.03 0.32 0.05 0.07 0.04 0.15 0.09]\n[0.15 0.05 0.1  0.05 0.11 0.14 0.04 0.08 0.08 0.2 ]\n[0.11 0.11 0.08 0.11 0.08 0.05 0.24 0.03 0.08 0.12]",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.image_classification.Model",
        "ref_id": "maite-protocols-image-classification-model",
        "headings": [
          "maite.protocols.image_classification.Model"
        ]
      },
      "doc_lineno": 78
    }
  ],
  "generated/maite.protocols.object_detection.Augmentation": [
    {
      "source": ">>> import numpy as np\n>>> np.random.seed(1)\n>>> import copy\n>>> from dataclasses import dataclass\n>>> from typing import Any\n>>> from maite.protocols import AugmentationMetadata, object_detection as od",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "random",
            "seed"
          ],
          "code_str": "np.random.seed",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "numpy.random.seed"
        },
        {
          "import_components": [
            "copy"
          ],
          "code_str": "copy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "copy"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "typing.Any"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 37
    },
    {
      "source": ">>> N_DATAPOINTS = 3  # datapoints in dataset\n>>> N_CLASSES = 2  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 10  # img height\n>>> W = 10  # img width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 45
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray\n>>> xb: od.InputBatchType = list(np.zeros((N_DATAPOINTS, C, H, W)))\n>>> yb: od.TargetBatchType = list(\n...     MyObjectDetectionTarget(boxes=np.empty(0), labels=np.empty(0), scores=np.empty(0))\n...     for _ in range(N_DATAPOINTS)\n... )\n>>> mdb: od.DatumMetadataBatchType = list({\"id\": i} for i in range(N_DATAPOINTS))\n>>> # Display the first datum in batch, first color channel, and only first 5 rows and cols\n>>> np.array(xb[0])[0][:5, :5]\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 76
    },
    {
      "source": ">>> np_noise = lambda shape: np.round(np.random.random(shape), 3)\n>>> class ImageAugmentation:\n...     def __init__(self, aug_func: Any, metadata: AugmentationMetadata):\n...         self.aug_func = aug_func\n...         self.metadata = metadata\n...     def __call__(\n...         self,\n...         batch: tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType],\n...     ) -> tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType]:\n...         xb, yb, mdb = batch\n...         # Copy data passed into the constructor to avoid mutating original inputs\n...         xb_aug = [copy.copy(input) for input in xb]\n...         # Add random noise to the input batch data, xb\n...         # (Note that all batch data dimensions (shapes) are the same in this example)\n...         shape = np.array(xb[0]).shape\n...         xb_aug = [x + self.aug_func(shape) for x in xb]\n...         # Note that this example augmentation only affects inputs--not targets\n...         return xb_aug, yb, mdb",
      "names": [
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 98
    },
    {
      "source": ">>> noise: od.Augmentation = ImageAugmentation(np_noise, metadata={\"id\": \"np_rand_noise\"})",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 103
    },
    {
      "source": ">>> xb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))\n>>> # Display the first datum in batch, first color channel, and only first 5 rows and cols\n>>> np.array(xb_aug[0])[0][:5, :5]\narray([[0.417, 0.72 , 0.   , 0.302, 0.147],\n       [0.419, 0.685, 0.204, 0.878, 0.027],\n       [0.801, 0.968, 0.313, 0.692, 0.876],\n       [0.098, 0.421, 0.958, 0.533, 0.692],\n       [0.989, 0.748, 0.28 , 0.789, 0.103]])",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Augmentation",
        "ref_id": "maite-protocols-object-detection-augmentation",
        "headings": [
          "maite.protocols.object_detection.Augmentation"
        ]
      },
      "doc_lineno": 116
    }
  ],
  "generated/maite.protocols.object_detection.DataLoader": [],
  "generated/maite.protocols.object_detection.Dataset": [
    {
      "source": ">>> import numpy as np\n>>> from dataclasses import dataclass\n>>> from maite.protocols import (\n...     DatumMetadata,\n...     DatasetMetadata,\n...     object_detection as od,\n... )",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 36
    },
    {
      "source": ">>> N_DATUM = 5  # data points in dataset\n>>> N_CLASSES = 2  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 10  # image height\n>>> W = 10  # image width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 44
    },
    {
      "source": ">>> def generate_random_bbox(\n...     n_classes: int, min_size: int = 2, max_size: int = 4\n... ) -> np.ndarray:\n...     # Generate random coordinates for top-left corner of bbox\n...     x1 = np.random.randint(0, W - min_size)\n...     y1 = np.random.randint(0, H - min_size)\n...     # Generate random width and height, ensuring bounding box stays within image boundaries\n...     bbox_width = np.random.randint(min_size, min(max_size, W - x1))\n...     bbox_height = np.random.randint(min_size, min(max_size, H - y1))\n...     # Set coordinates for bottom-right corner of bbox\n...     x2 = x1 + bbox_width\n...     y2 = y1 + bbox_height\n...     # Pick random class label\n...     label = np.random.choice(n_classes)\n...     return np.array([x1, y1, x2, y2, label])",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "min"
          ],
          "code_str": "min",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "min"
        },
        {
          "import_components": [
            "min"
          ],
          "code_str": "min",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "min"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 67
    },
    {
      "source": ">>> def generate_random_annotation(max_num_detections: int = 2) -> np.ndarray:\n...     num_detections = np.random.choice(max_num_detections + 1)\n...     annotation = [generate_random_bbox(N_CLASSES) for _ in range(num_detections)]\n...     return np.vstack(annotation) if num_detections > 0 else np.empty(0)",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 72
    },
    {
      "source": ">>> images: list[np.ndarray] = list(np.random.rand(N_DATUM, C, H, W))\n>>> annotations: list[np.ndarray] = [\n...     generate_random_annotation() for _ in range(N_DATUM)\n... ]\n>>> hour_of_day: list[int] = [np.random.choice(24) for _ in range(N_DATUM)]\n>>> dataset: list[tuple] = list(zip(images, annotations, hour_of_day))",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 81
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 90
    },
    {
      "source": ">>> class MyDatumMetadata(DatumMetadata):\n...     hour_of_day: int",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 96
    },
    {
      "source": ">>> class ImageDataset:\n...     # Set up required dataset-level metadata\n...     metadata: DatasetMetadata = {\n...         \"id\": \"Dummy Dataset\",\n...         \"index2label\": {i: f\"class_name_{i}\" for i in range(N_CLASSES)}\n...     }\n...     def __init__(self, dataset: list[tuple[np.ndarray, np.ndarray, int]]):\n...         self.dataset = dataset\n...     def __len__(self) -> int:\n...         return len(self.dataset)\n...     def __getitem__(\n...         self, index: int\n...     ) -> tuple[np.ndarray, od.ObjectDetectionTarget, od.DatumMetadataType]:\n...         if index < 0 or index >= len(self):\n...             raise IndexError(f\"Index {index} is out of range for the dataset, which has length {len(self)}.\")\n...         image, annotations, hour_of_day = self.dataset[index]\n...         # Structure ground truth target\n...         boxes, labels = [], []\n...         for _, ann in enumerate(annotations):\n...             bbox = ann[:-1]\n...             label = ann[-1:]\n...             if len(bbox) != 0:\n...                 boxes.append(bbox)\n...                 labels.append(label)\n...         od_target = MyObjectDetectionTarget(\n...             boxes=np.array(boxes), labels=np.array(labels), scores=np.ones(len(boxes))\n...         )\n...         # Structure datum-level metadata\n...         datum_metadata: MyDatumMetadata = {\"id\": str(index), \"hour_of_day\": hour_of_day}\n...         return image, od_target, datum_metadata",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "IndexError"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 26,
          "end_lineno": 26,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 29,
          "end_lineno": 29,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 131
    },
    {
      "source": ">>> maite_od_dataset: od.Dataset = ImageDataset(dataset)",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Dataset",
        "ref_id": "maite-protocols-object-detection-dataset",
        "headings": [
          "maite.protocols.object_detection.Dataset"
        ]
      },
      "doc_lineno": 136
    }
  ],
  "generated/maite.protocols.object_detection.Metric": [
    {
      "source": ">>> from dataclasses import dataclass\n>>> from maite.protocols import ArrayLike, MetricMetadata\n>>> from typing import Any, Sequence\n>>> import maite.protocols.object_detection as od\n>>> import numpy as np",
      "names": [
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 5,
          "end_lineno": 5,
          "context": "import_target",
          "resolved_location": "numpy"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 33
    },
    {
      "source": ">>> class MyIoUMetric:\n...\n...     def __init__(self, id: str):\n...         self.pred_boxes = []  # elements correspond to predicted boxes in single image\n...         self.target_boxes = []  # elements correspond to ground truth boxes in single image\n...         # Store provided id for this metric instance\n...         self.metadata = MetricMetadata(\n...             id=id\n...         )\n...\n...     def reset(self) -> None:\n...         self.pred_boxes = []\n...         self.target_boxes = []\n...\n...     def update(\n...         self,\n...         pred_batch: Sequence[od.ObjectDetectionTarget],\n...         target_batch: Sequence[od.ObjectDetectionTarget],\n...     ) -> None:\n...         self.pred_boxes.extend(pred_batch)\n...         self.target_boxes.extend(target_batch)\n...\n...     @staticmethod\n...     def iou_vec(boxes_a: ArrayLike, boxes_b: ArrayLike) -> np.ndarray:\n...         # Break up points into separate columns\n...         x0a, y0a, x1a, y1a = np.split(boxes_a, 4, axis=1)\n...         x0b, y0b, x1b, y1b = np.split(boxes_b, 4, axis=1)\n...         # Calculate intersections\n...         xi_0, yi_0 = np.split(\n...             np.maximum(np.append(x0a, y0a, axis=1), np.append(x0b, y0b, axis=1)),\n...             2,\n...             axis=1,\n...         )\n...         xi_1, yi_1 = np.split(\n...             np.minimum(np.append(x1a, y1a, axis=1), np.append(x1b, y1b, axis=1)),\n...             2,\n...             axis=1,\n...         )\n...         ints: np.ndarray = np.maximum(0, xi_1 - xi_0) * np.maximum(0, yi_1 - yi_0)\n...         # Calculate unions (as sum of areas minus their intersection)\n...         unions: np.ndarray = (\n...             (x1a - x0a) * (y1a - y0a)\n...             + (x1b - x0b) * (y1b - y0b)\n...             - (xi_1 - xi_0) * (yi_1 - yi_0)\n...         )\n...         return ints / unions\n...\n...     def compute(self) -> dict[str, Any]:\n...         mean_iou_by_img: list[float] = []\n...         for pred_box, tgt_box in zip(self.pred_boxes, self.target_boxes):\n...             single_img_ious = self.iou_vec(pred_box.boxes, tgt_box.boxes)\n...             mean_iou_by_img.append(float(np.mean(single_img_ious)))\n...         return {\"mean_iou\": np.mean(np.array(mean_iou_by_img))}\n...",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 88
    },
    {
      "source": ">>> iou_metric: od.Metric = MyIoUMetric(id=\"IoUMetric\")",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 92
    },
    {
      "source": ">>> prediction_boxes: list[tuple[int, int, int, int]] = [\n...     (1, 1, 12, 12),\n...     (100, 100, 120, 120),\n...     (180, 180, 270, 270),\n... ]",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 102
    },
    {
      "source": ">>> target_boxes: list[tuple[int, int, int, int]] = [\n...     (1, 1, 10, 10),\n...     (100, 100, 120, 120),\n...     (200, 200, 300, 300),\n... ]",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 108
    },
    {
      "source": ">>> @dataclass\n... class ObjectDetectionTargetImpl:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 119
    },
    {
      "source": ">>> num_boxes = len(target_boxes)\n>>> fake_labels = np.random.randint(0, 9, num_boxes)\n>>> fake_scores = np.zeros(num_boxes)\n>>> pred_batch = [\n...     ObjectDetectionTargetImpl(\n...         boxes=np.array(prediction_boxes), labels=fake_labels, scores=fake_scores\n...     )\n... ]\n>>> target_batch: Sequence[ObjectDetectionTargetImpl] = [\n...     ObjectDetectionTargetImpl(\n...         boxes=np.array(target_boxes), labels=fake_labels, scores=fake_scores\n...     )\n... ]",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 133
    },
    {
      "source": ">>> iou_metric.update(pred_batch, target_batch)\n>>> print(iou_metric.compute())\n{'mean_iou': 0.6802112029384757}",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Metric",
        "ref_id": "maite-protocols-object-detection-metric",
        "headings": [
          "maite.protocols.object_detection.Metric"
        ]
      },
      "doc_lineno": 139
    }
  ],
  "generated/maite.protocols.object_detection.Model": [
    {
      "source": ">>> from dataclasses import dataclass\n>>> from typing import Sequence\n>>> import numpy as np\n>>> import maite.protocols.object_detection as od\n>>> from maite.protocols import ModelMetadata",
      "names": [
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 27
    },
    {
      "source": ">>> @dataclass\n... class MyObjectDetectionTarget:\n...     boxes: np.ndarray\n...     labels: np.ndarray\n...     scores: np.ndarray\n...",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 36
    },
    {
      "source": ">>> N_DATAPOINTS = 2  # datapoints in dataset\n>>> N_CLASSES = 5  # possible classes that can be detected\n>>> C = 3  # number of color channels\n>>> H = 32  # img height\n>>> W = 32  # img width",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 44
    },
    {
      "source": ">>> simple_batch: list[np.ndarray] = [\n...     np.random.rand(C, H, W) for _ in range(N_DATAPOINTS)\n... ]",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        }
      ],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 50
    },
    {
      "source": ">>> class ObjectDetectionDummyModel:\n...     metadata: ModelMetadata = {\"id\": \"ObjectDetectionDummyModel\"}\n...\n...     def __call__(\n...         self, batch: od.InputBatchType\n...     ) -> Sequence[MyObjectDetectionTarget]:\n...         # For the simplicity, we don't provide an object detection model here, but the output from a model.\n...         DETECTIONS_PER_IMG = (\n...             2  # number of bounding boxes detections per image/datapoints\n...         )\n...         all_boxes = np.array(\n...             [[1, 3, 5, 9], [2, 5, 8, 12], [4, 10, 8, 20], [3, 5, 6, 15]]\n...         )  # all detection boxes for N_DATAPOINTS\n...         all_predictions = list()\n...         for datum_idx in range(N_DATAPOINTS):\n...             boxes = all_boxes[datum_idx : datum_idx + DETECTIONS_PER_IMG]\n...             labels = np.random.randint(N_CLASSES, size=DETECTIONS_PER_IMG)\n...             scores = np.random.rand(DETECTIONS_PER_IMG)\n...             predictions = MyObjectDetectionTarget(boxes, labels, scores)\n...             all_predictions.append(predictions)\n...         return all_predictions\n...",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 76
    },
    {
      "source": ">>> od_dummy_model: od.Model = ObjectDetectionDummyModel()\n>>> od_dummy_model.metadata\n{'id': 'ObjectDetectionDummyModel'}\n>>> predictions = od_dummy_model(simple_batch)\n>>> predictions  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n[MyObjectDetectionTarget(boxes=array([[ 1,  3,  5,  9], [ 2,  5,  8, 12]]), labels=array([..., ...]), scores=array([..., ...])),\nMyObjectDetectionTarget(boxes=array([[ 2,  5,  8, 12], [ 4, 10,  8, 20]]), labels=array([..., ...]), scores=array([..., ...]))]",
      "names": [],
      "example": {
        "document": "generated/maite.protocols.object_detection.Model",
        "ref_id": "maite-protocols-object-detection-model",
        "headings": [
          "maite.protocols.object_detection.Model"
        ]
      },
      "doc_lineno": 87
    }
  ],
  "generated/maite.protocols.object_detection.ObjectDetectionTarget": [],
  "generated/maite.testing.docs.NumPyDocResults": [],
  "generated/maite.testing.docs.NumpyDocErrorCode": [],
  "generated/maite.testing.docs.validate_docstring": [
    {
      "source": ">>> from maite.testing.docs import validate_docstring\n>>> from maite.testing.documentation.documentation_dependencies import person",
      "names": [
        {
          "import_components": [
            "maite",
            "testing",
            "docs",
            "validate_docstring"
          ],
          "code_str": "validate_docstring",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.testing.docs.validate_docstring"
        }
      ],
      "example": {
        "document": "generated/maite.testing.docs.validate_docstring",
        "ref_id": "maite-testing-docs-validate-docstring",
        "headings": [
          "maite.testing.docs.validate_docstring"
        ]
      },
      "doc_lineno": 117
    },
    {
      "source": ">>> validate_docstring(person, ignore=('ES01', 'SA01'), include_ignored_errors=True)\n{'error_count': 0, 'errors': {}, ...'ignored_errors': {'ES01': ['No extended summary found'], 'SA01': ['See Also section not found']}...",
      "names": [],
      "example": {
        "document": "generated/maite.testing.docs.validate_docstring",
        "ref_id": "maite-testing-docs-validate-docstring",
        "headings": [
          "maite.testing.docs.validate_docstring"
        ]
      },
      "doc_lineno": 122
    },
    {
      "source": ">>> def f():\n...     # doc-ignore: GL08\n...     return\n>>> validate_docstring(f)   # doctest: +ELLIPSIS\n{'error_count': 0, 'errors': {}, 'file': ..., 'file_line': 1}\n>>> validate_docstring(f, ignore_via_comments_allowed=False)   # doctest: +ELLIPSIS\n{'error_count': 1, 'errors': {'GL08': ['The object does not have a docstring']}, 'file': ..., 'file_line': 1}",
      "names": [],
      "example": {
        "document": "generated/maite.testing.docs.validate_docstring",
        "ref_id": "maite-testing-docs-validate-docstring",
        "headings": [
          "maite.testing.docs.validate_docstring"
        ]
      },
      "doc_lineno": 132
    }
  ],
  "generated/maite.testing.project.ModuleScan": [
    {
      "source": ">>> from maite.testing.project import ModuleScan\n>>> scanner = ModuleScan()\n>>> results = scanner(\"maite\")\n>>> results[\"summary\"]\n{'filesAnalyzed': ..., 'errorCount': ..., 'warningCount': ..., 'informationCount': ..., 'timeInSec': ...}\n>>> results[\"typeCompleteness\"][\"packageName\"]\n'maite'\n>>> results[\"typeCompleteness\"][\"symbols\"]     # will change as MAITE changes --> # doctest: +SKIP\n[{'category': 'class',\n  'name': 'maite.errors.MaiteException',\n  'referenceCount': 3,\n  'isExported': True,\n  'isTypeKnown': True,\n  'isTypeAmbiguous': False,\n  'diagnostics': []},\n {'category': 'class',\n  'name': 'maite.errors.InternalError',\n  'referenceCount': 1,\n  'isExported': True,\n  'isTypeKnown': True,\n  'isTypeAmbiguous': False,\n  'diagnostics': []},\n ...]",
      "names": [
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan"
          ],
          "code_str": "ModuleScan",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan"
          ],
          "code_str": "ModuleScan",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan",
            "()"
          ],
          "code_str": "scanner",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan",
            "()"
          ],
          "code_str": "scanner",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        }
      ],
      "example": {
        "document": "generated/maite.testing.project.ModuleScan",
        "ref_id": "maite-testing-project-modulescan",
        "headings": [
          "maite.testing.project.ModuleScan"
        ]
      },
      "doc_lineno": 43
    }
  ],
  "generated/maite.testing.project.get_public_symbols": [
    {
      "source": ">>> from maite.testing.project import get_public_symbols, ModuleScan\n>>> scanner = ModuleScan()\n>>> results = scanner(\"maite\")\n>>> get_public_symbols(results)    # will change as MAITE changes --> # doctest: +SKIP\n[{'category': 'class',\n  'name': 'maite.errors.MaiteException',\n  'referenceCount': 3,\n  'isExported': True,\n  'isTypeKnown': True,\n  'isTypeAmbiguous': False,\n  'diagnostics': []},\n {'category': 'class',\n  'name': 'maite.errors.InternalError',\n  'referenceCount': 1,\n  'isExported': True,\n  'isTypeKnown': True,\n  'isTypeAmbiguous': False,\n  'diagnostics': []},\n ...]",
      "names": [
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "get_public_symbols"
          ],
          "code_str": "get_public_symbols",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.testing.project.get_public_symbols"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan"
          ],
          "code_str": "ModuleScan",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan"
          ],
          "code_str": "ModuleScan",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan",
            "()"
          ],
          "code_str": "scanner",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "ModuleScan",
            "()"
          ],
          "code_str": "scanner",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "maite._internals.testing.project.ModuleScan"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "project",
            "get_public_symbols"
          ],
          "code_str": "get_public_symbols",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "maite.testing.project.get_public_symbols"
        }
      ],
      "example": {
        "document": "generated/maite.testing.project.get_public_symbols",
        "ref_id": "maite-testing-project-get-public-symbols",
        "headings": [
          "maite.testing.project.get_public_symbols"
        ]
      },
      "doc_lineno": 65
    },
    {
      "source": ">>> get_public_symbols(results, submodule=\"maite.testing.docs\")\n[{'category': 'type alias', 'name': 'maite.testing.docs.NumpyDocErrorCode', 'referenceCount': 1, 'isExported': True, 'isTypeKnown': True, 'isTypeAmbiguous': False, 'diagnostics': []}, {'category': 'class', 'name': 'maite.testing.docs.NumPyDocResults', 'referenceCount': 1, 'isExported': True, 'isTypeKnown': True, 'isTypeAmbiguous': False, 'diagnostics'...",
      "names": [],
      "example": {
        "document": "generated/maite.testing.project.get_public_symbols",
        "ref_id": "maite-testing-project-get-public-symbols",
        "headings": [
          "maite.testing.project.get_public_symbols"
        ]
      },
      "doc_lineno": 70
    }
  ],
  "generated/maite.testing.pyright.PyrightOutput": [],
  "generated/maite.testing.pyright.list_error_messages": [],
  "generated/maite.testing.pyright.pyright_analyze": [
    {
      "source": ">>> from maite.testing.pyright import pyright_analyze\n>>> def f(x: str):\n...     return 1 + x\n>>> pyright_analyze(f)[0]\n{'version': ..., 'time': ..., 'generalDiagnostics': [{'file': ..., 'severity': ..., 'message': 'Operator \"+\" not supported for types \"Literal[1]\" and \"str\"', 'range': {'start': {'line': ..., 'character': ...}, 'end': {'line': ..., 'character': ...}}, 'rule': ...}], 'summary': {'filesAnalyzed': ..., 'errorCount': 1, 'warningCount': 0, 'informationCount': 0, 'timeInSec': ...}}",
      "names": [
        {
          "import_components": [
            "maite",
            "testing",
            "pyright",
            "pyright_analyze"
          ],
          "code_str": "pyright_analyze",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.testing.pyright.pyright_analyze"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "maite",
            "testing",
            "pyright",
            "pyright_analyze"
          ],
          "code_str": "pyright_analyze",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "maite.testing.pyright.pyright_analyze"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 110
    },
    {
      "source": ">>> def g(x: int) -> int:\n...     return 1 + x\n>>> pyright_analyze(g)[0]\n{'version': ..., 'time': ..., 'generalDiagnostics': ..., 'summary': {'filesAnalyzed': ..., 'errorCount': 0, 'warningCount': 0, 'informationCount': 0, 'timeInSec': ...}}",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 117
    },
    {
      "source": ">>> import math  # import statement is not be in scope of `f`\n>>> def f():\n...     math.acos(1)\n>>> pyright_analyze(f)[0][\"summary\"][\"errorCount\"]\n1",
      "names": [
        {
          "import_components": [
            "math"
          ],
          "code_str": "math",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "math"
        },
        {
          "import_components": [
            "math",
            "acos"
          ],
          "code_str": "math.acos",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "math.acos"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 126
    },
    {
      "source": ">>> pyright_analyze(f, preamble=\"import math\")[0][\"summary\"][\"errorCount\"]\n0",
      "names": [],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 131
    },
    {
      "source": ">>> def plus_1(x: int):\n...     '''\n...     Examples\n...     --------\n...     >>> from mylib import plus_1\n...     >>> plus_1('2')  # <- pyright_analyze will catch typo (str instead of int)\n...     3\n...     '''\n...     return x + 1\n>>> pyright_analyze(plus_1, scan_docstring=True)[0][\"summary\"][\"errorCount\"]    # nested notional example has fake import --> # doctest: +SKIP\n1",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 145
    },
    {
      "source": ">>> def plus_1(x: int):\n...     '''\n...     Examples\n...     --------\n...     >>> from mylib import plus_1\n...     >>> plus_1(2)\n...     3\n...     '''\n...     return x + 1\n>>> pyright_analyze(plus_1, scan_docstring=True)[0][\"summary\"][\"errorCount\"]    # nested notional example has fake import --> # doctest: +SKIP\n0",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pyright.pyright_analyze",
        "ref_id": "maite-testing-pyright-pyright-analyze",
        "headings": [
          "maite.testing.pyright.pyright_analyze"
        ]
      },
      "doc_lineno": 159
    }
  ],
  "generated/maite.testing.pytest.cleandir": [
    {
      "source": ">>> import pytest\n>>> @pytest.mark.usefixtures(\"cleandir\")\n... def test_writes_some_file():\n...     from pathlib import Path\n...     Path(\"dummy.txt\").touch()  # file will be written to a tmp dir",
      "names": [
        {
          "import_components": [
            "pathlib"
          ],
          "code_str": "pathlib",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "pathlib"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "pathlib",
            "Path",
            "()",
            "touch"
          ],
          "code_str": "touch",
          "lineno": 5,
          "end_lineno": 5,
          "context": "after_call",
          "resolved_location": "pathlib.Path.touch"
        }
      ],
      "example": {
        "document": "generated/maite.testing.pytest.cleandir",
        "ref_id": "maite-testing-pytest-cleandir",
        "headings": [
          "maite.testing.pytest.cleandir"
        ]
      },
      "doc_lineno": 31
    }
  ],
  "generated/maite.utils.validation.chain_validators": [
    {
      "source": ">>> from maite.utils.validation import check_type, check_domain, chain_validators\n>>> from functools import partial\n>>> is_int = partial(check_type, type_=int)\n>>> is_pos = partial(check_domain, lower=0)\n>>> check_pos_int = chain_validators(is_int, is_pos)\n>>> check_pos_int(\"foo\", 10)\n10\n>>> try:\n...     check_pos_int(\"foo\", [\"a\"])\n... except:\n...     print(\"maite.errors.InvalidArgument: Expected `foo` to be of type `int`. Got `['a']` (type: `list`).\")\nmaite.errors.InvalidArgument: Expected `foo` to be of type `int`. Got `['a']` (type: `list`).\n>>> try:\n...     check_pos_int(\"foo\", -1)\n... except:\n...     print(\"maite.errors.InvalidArgument: `foo` must satisfy `0 <= foo`.  Got: `-1`.\")\nmaite.errors.InvalidArgument: `foo` must satisfy `0 <= foo`.  Got: `-1`.",
      "names": [
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_type"
          ],
          "code_str": "check_type",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.check_type"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_domain"
          ],
          "code_str": "check_domain",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.check_domain"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "chain_validators"
          ],
          "code_str": "chain_validators",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.chain_validators"
        },
        {
          "import_components": [
            "functools"
          ],
          "code_str": "functools",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_from",
          "resolved_location": "functools"
        },
        {
          "import_components": [
            "functools",
            "partial"
          ],
          "code_str": "partial",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_type"
          ],
          "code_str": "check_type",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "maite.utils.validation.check_type"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "functools",
            "partial"
          ],
          "code_str": "partial",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "functools",
            "partial",
            "()"
          ],
          "code_str": "is_int",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_domain"
          ],
          "code_str": "check_domain",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "maite.utils.validation.check_domain"
        },
        {
          "import_components": [
            "functools",
            "partial"
          ],
          "code_str": "partial",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "functools",
            "partial",
            "()"
          ],
          "code_str": "is_pos",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "functools",
            "partial",
            "()"
          ],
          "code_str": "is_int",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "functools",
            "partial",
            "()"
          ],
          "code_str": "is_pos",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "functools.partial"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "chain_validators"
          ],
          "code_str": "chain_validators",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "maite.utils.validation.chain_validators"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.chain_validators",
        "ref_id": "maite-utils-validation-chain-validators",
        "headings": [
          "maite.utils.validation.chain_validators"
        ]
      },
      "doc_lineno": 47
    }
  ],
  "generated/maite.utils.validation.check_domain": [
    {
      "source": ">>> from maite.utils.validation import check_domain\n>>> try:\n...     check_domain(\"x\", 1, lower=20)\n... except:\n...     print(\"maite.errors.InvalidArgument: `x` must satisfy `20 <= x`.  Got: `1`.\")\nmaite.errors.InvalidArgument: `x` must satisfy `20 <= x`.  Got: `1`.",
      "names": [
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_domain"
          ],
          "code_str": "check_domain",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.check_domain"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_domain"
          ],
          "code_str": "check_domain",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "maite.utils.validation.check_domain"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_domain",
        "ref_id": "maite-utils-validation-check-domain",
        "headings": [
          "maite.utils.validation.check_domain"
        ]
      },
      "doc_lineno": 64
    },
    {
      "source": ">>> try:\n...     check_domain(\"x\", 1, lower=1, incl_low=False)\n... except:\n...     print(\"maite.errors.InvalidArgument: `x` must satisfy `1 < x`.  Got: `1`.\")\nmaite.errors.InvalidArgument: `x` must satisfy `1 < x`.  Got: `1`.",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_domain",
        "ref_id": "maite-utils-validation-check-domain",
        "headings": [
          "maite.utils.validation.check_domain"
        ]
      },
      "doc_lineno": 70
    },
    {
      "source": ">>> check_domain(\"x\", 1, lower=1, incl_low=True) # ok\n1",
      "names": [],
      "example": {
        "document": "generated/maite.utils.validation.check_domain",
        "ref_id": "maite-utils-validation-check-domain",
        "headings": [
          "maite.utils.validation.check_domain"
        ]
      },
      "doc_lineno": 73
    },
    {
      "source": ">>> check_domain(\"x\", 0.0, lower=-10, upper=10)  # ok\n0.0",
      "names": [],
      "example": {
        "document": "generated/maite.utils.validation.check_domain",
        "ref_id": "maite-utils-validation-check-domain",
        "headings": [
          "maite.utils.validation.check_domain"
        ]
      },
      "doc_lineno": 76
    }
  ],
  "generated/maite.utils.validation.check_one_of": [
    {
      "source": ">>> from maite.utils.validation import check_one_of\n>>> try:\n...     check_one_of(\"foo\", None, [1, 2])\n... except:\n...     print(\"maite.errors.InvalidArgument: Expected `foo` to be one of: 1, 2. Got `None`.\")\nmaite.errors.InvalidArgument: Expected `foo` to be one of: 1, 2. Got `None`.",
      "names": [
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_one_of"
          ],
          "code_str": "check_one_of",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.check_one_of"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_one_of"
          ],
          "code_str": "check_one_of",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "maite.utils.validation.check_one_of"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 54
    },
    {
      "source": ">>> print(check_one_of(\"foo\", None, [1, 2], None))\nNone",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 59
    },
    {
      "source": ">>> check_one_of(\"foo\", 1, [True])  # `1` == `True`\n1",
      "names": [],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 64
    },
    {
      "source": ">>> try:\n...     check_one_of(\"foo\", 1, [True], requires_identity=True)\n... except:\n...     print(\"maite.errors.InvalidArgument: Expected `foo` to be: True. Got `1`.\")\nmaite.errors.InvalidArgument: Expected `foo` to be: True. Got `1`.",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 70
    },
    {
      "source": ">>> from enum import Enum\n>>> class Pet(Enum):\n...     cat = 1\n...     dog = 2",
      "names": [
        {
          "import_components": [
            "enum"
          ],
          "code_str": "enum",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "enum"
        },
        {
          "import_components": [
            "enum",
            "Enum"
          ],
          "code_str": "Enum",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "enum.Enum"
        },
        {
          "import_components": [
            "enum",
            "Enum"
          ],
          "code_str": "Enum",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "enum.Enum"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 77
    },
    {
      "source": ">>> try:\n...     check_one_of(\"bar\", 88, Pet)\n... except:\n...     print(\"maite.errors.InvalidArgument: Expected `bar` to be one of: Pet.cat, Pet.dog. Got `88`.\")\nmaite.errors.InvalidArgument: Expected `bar` to be one of: Pet.cat, Pet.dog. Got `88`.\n>>> check_one_of(\"bar\", Pet.cat, Pet)\n<Pet.cat: 1>",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_one_of",
        "ref_id": "maite-utils-validation-check-one-of",
        "headings": [
          "maite.utils.validation.check_one_of"
        ]
      },
      "doc_lineno": 85
    }
  ],
  "generated/maite.utils.validation.check_type": [
    {
      "source": ">>> from maite.utils.validation import check_type\n>>> check_type('apple', 1, int)\n1",
      "names": [
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_type"
          ],
          "code_str": "check_type",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "maite.utils.validation.check_type"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "maite",
            "utils",
            "validation",
            "check_type"
          ],
          "code_str": "check_type",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "maite.utils.validation.check_type"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_type",
        "ref_id": "maite-utils-validation-check-type",
        "headings": [
          "maite.utils.validation.check_type"
        ]
      },
      "doc_lineno": 42
    },
    {
      "source": ">>> try:\n...     check_type('apple', 1, bool)\n... except:\n...     print(\"maite.errors.InvalidArgument: Expected `apple` to be of type `bool`. Got `1` (type: `int`).\")\nmaite.errors.InvalidArgument: Expected `apple` to be of type `bool`. Got `1` (type: `int`).",
      "names": [
        {
          "import_components": [
            "bool"
          ],
          "code_str": "bool",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "bool"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_type",
        "ref_id": "maite-utils-validation-check-type",
        "headings": [
          "maite.utils.validation.check_type"
        ]
      },
      "doc_lineno": 48
    },
    {
      "source": ">>> check_type('apple', 1, (int, bool))\n1",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "bool"
          ],
          "code_str": "bool",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "bool"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_type",
        "ref_id": "maite-utils-validation-check-type",
        "headings": [
          "maite.utils.validation.check_type"
        ]
      },
      "doc_lineno": 51
    },
    {
      "source": ">>> print(check_type('apple', None, (int, bool), optional=True))\nNone",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "bool"
          ],
          "code_str": "bool",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "bool"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "generated/maite.utils.validation.check_type",
        "ref_id": "maite-utils-validation-check-type",
        "headings": [
          "maite.utils.validation.check_type"
        ]
      },
      "doc_lineno": 54
    }
  ],
  "generated/maite.workflows.evaluate": [],
  "generated/maite.workflows.predict": [],
  "how_to/static_typing": [],
  "how_to/wrap_image_classification_dataset": [
    {
      "source": "import PIL.Image\nimport datasets\nimport numpy as np\nimport numpy.typing as npt\nfrom IPython.display import display\n\nfrom typing import Literal\n\nimport maite.protocols.image_classification as ic\nfrom maite.protocols import DatasetMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "numpy",
            "typing"
          ],
          "code_str": "numpy.typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "numpy.typing"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Literal"
          ],
          "code_str": "Literal",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_target",
          "resolved_location": "typing.Literal"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 29
    },
    {
      "source": "cifar10_dataset_dict = datasets.load_dataset(\"uoft-cs/cifar10\")\ncifar10_dataset_dict",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 62
    },
    {
      "source": "cifar10_train: datasets.Dataset = cifar10_dataset_dict[\"train\"] # type: ignore\ncifar10_test: datasets.Dataset = cifar10_dataset_dict[\"test\"] # type: ignore",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "load-the-cifar-10-dataset-from-hugging-face-hub",
        "headings": [
          "Wrap an Image Classification Dataset",
          "1 Load the CIFAR-10 dataset from Hugging Face Hub"
        ]
      },
      "doc_lineno": 120
    },
    {
      "source": "for label_number, label_name in enumerate(cifar10_train.features[\"label\"].names):\n    print(f\"Label {label_number}: {label_name}\")",
      "names": [
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 133
    },
    {
      "source": "for i in range(0, len(cifar10_train), 10000):\n    item = cifar10_train[i]\n    label, img = item[\"label\"], item[\"img\"]\n    label_name = cifar10_train.features[\"label\"].names[label]\n    print(f\"CIFAR-10 Train {i}\")\n    print(f\"Label: {label} {label_name}\")\n    display(img)",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 155
    },
    {
      "source": "# Extend DatasetMetadata to record whether dataset is a train or test split\nclass CustomDatasetMetadata(DatasetMetadata):\n    split: str\n\nclass Cifar10Dataset:\n    def __init__(\n        self,\n        cifar10_dataset_split: datasets.Dataset,\n        split: Literal[\"train\", \"test\"]\n    ):\n        # Save the CIFAR-10 dataset given by the user. This is helpful if you want to\n        # sample the dataset using the Hugging Face API prior to using it.\n        self.dataset = cifar10_dataset_split\n\n        # Create a dictionary mapping label number to label name from the label metadata\n        # in the underlying dataset.\n        index2label = {\n            i: label for i, label in enumerate(self.dataset.features[\"label\"].names)\n        }\n\n        # Create required metadata attribute (with custom split key)\n        self.metadata: DatasetMetadata = CustomDatasetMetadata(\n            id=\"CIFAR-10\",\n            index2label=index2label,\n            split=split\n        )\n\n        # Get the number of classes used in the dataset\n        num_classes = self.dataset.features[\"label\"].num_classes\n\n        # Create a collection of target vectors to be used for the one-hot encoding of labels\n        self.targets = np.eye(num_classes)\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[npt.NDArray, npt.NDArray, ic.DatumMetadataType]:\n        # Look up item in the dataset, which returns a dictionary with two keys:\n        # - \"img\": PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n        # - \"label\": int\n        item = self.dataset[index]\n        img_pil = item[\"img\"]\n        label = item[\"label\"]\n\n        # Convert the PIL image to a NumPy array\n        img_hwc = np.array(img_pil)  # shape (H, W, C)\n\n        # Use MAITE array index convention for representing images: shape (C, H, W)\n        img_chw = img_hwc.transpose(2, 0, 1)\n\n        # Get one-hot encoded tensor indicating the class label for this image\n        target = self.targets[label, :].copy()\n\n        # CIFAR-10 does not have any extra metadata, so we record only the index of this datum\n        metadata: ic.DatumMetadataType = {\"id\": index}\n\n        return img_chw, target, metadata",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "enumerate"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 39,
          "end_lineno": 39,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 38,
          "end_lineno": 38,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 249
    },
    {
      "source": "train_dataset: ic.Dataset = Cifar10Dataset(cifar10_dataset_split=cifar10_train, split=\"train\")\ntrain_dataset.metadata",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 316
    },
    {
      "source": "test_dataset: ic.Dataset = Cifar10Dataset(cifar10_dataset_split=cifar10_test, split=\"test\")\ntest_dataset.metadata",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 343
    },
    {
      "source": "print(f\"CIFAR-10 size: train={len(train_dataset)}, test={len(test_dataset)}\")",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 370
    },
    {
      "source": "def print_datum(dataset, index):\n    img_arr_chw, target, datum_metadata = dataset[index]\n    print(f\"Datum {datum_metadata['id']}\")\n    print(f\"  Input Image Array: {str(img_arr_chw)[:30]}...\")\n    print(f\"    shape={img_arr_chw.shape}\")\n    print(f\"    dtype={img_arr_chw.dtype}\")\n    display(PIL.Image.fromarray(img_arr_chw.transpose(1, 2, 0)))\n    print(f\"  Target: {target}\")\n    label_index = np.argmax(target)\n    print(f\"    target index: {np.argmax(target)}\")\n    print(f\"    target label: {dataset.metadata['index2label'][label_index]}\")\n    print(\"  Metadata:\")\n    print(f\"    {datum_metadata}\")",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 382
    },
    {
      "source": "for i in [0, 3000, 6000]:\n    print_datum(test_dataset, i)\n    print()",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Image Classification Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 398
    }
  ],
  "how_to/wrap_image_classification_model": [
    {
      "source": "import io\nimport json\nimport urllib.request\nfrom typing import Callable, Sequence\n\nimport numpy as np\nimport PIL.Image\nimport torch as pt\nimport torchvision\nfrom IPython.display import display\n\nimport maite.protocols.image_classification as ic\nfrom maite.protocols import ArrayLike, ModelMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "io"
          ],
          "code_str": "io",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "io"
        },
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "urllib",
            "request"
          ],
          "code_str": "urllib.request",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "urllib.request"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 28
    },
    {
      "source": "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n\nmodel = torchvision.models.resnet50(\n    weights=model_weights\n)  # weights will download automatically\n\nmodel = model.eval()  # set the ResNet50 model to eval mode",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "load-the-pretrained-resnet50-model",
        "headings": [
          "Wrap an Image Classification Model",
          "1 Load the Pretrained ResNet50 Model"
        ]
      },
      "doc_lineno": 68
    },
    {
      "source": "labels_url = \"https://raw.githubusercontent.com/raghakot/keras-vis/refs/heads/master/resources/imagenet_class_index.json\"\nresponse = urllib.request.urlopen(labels_url).read()\nlabels = json.loads(response.decode(\"utf-8\"))\n\nimg_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01491361_tiger_shark.JPEG\"\nimage_data = urllib.request.urlopen(img_url).read()\nexample_img_1 = PIL.Image.open(io.BytesIO(image_data))\n\nimg_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01695060_Komodo_dragon.JPEG\"\nimage_data = urllib.request.urlopen(img_url).read()\nexample_img_2 = PIL.Image.open(io.BytesIO(image_data))\n\nexample_imgs = [example_img_1, example_img_2]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 129
    },
    {
      "source": "dict(list(labels.items())[:4])",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 150
    },
    {
      "source": "def prediction_label(logits):\n    logits = logits.unsqueeze(0)\n    label_pred = logits.argmax().item()\n    return f\"{label_pred} {labels[str(label_pred)]}\"\n\npreprocess = model_weights.transforms()\n\nfor example_img in example_imgs:\n    input = preprocess(example_img)\n    logits = model(input.unsqueeze(0)) # use unsqueeze to add batch dimension\n\n    print(\n        f\"\"\"\n    ResNet50 Outputs\n    ================\n    Result Type: {type(logits)}\n    Result Shape: {logits.shape}\n    Sample Prediction: {prediction_label(logits)}\n    \"\"\"\n    )\n\n    display(example_img)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Image Classification Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 174
    },
    {
      "source": "class TorchvisionResNetModel():\n    def __init__(\n        self,\n        model: torchvision.models.ResNet,\n        preprocess: Callable[[pt.Tensor], pt.Tensor],\n    ) -> None:\n        self.metadata: ModelMetadata = {\n            \"id\": \"Torchvision ResNet ImageNet 1k\",\n            \"index2label\": {\n                int(idx): label for idx, [_wordnetid, label] in labels.items()\n            },\n        }\n        self.model = model\n        self.preprocess = preprocess\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[pt.Tensor]:\n        # Preprocess the inputs to ensure they match the model's input format\n        imgs_chw = []\n        for img_chw in batch:\n            imgs_chw.append(self.preprocess(pt.as_tensor(img_chw)))\n\n        # Create a shape-(N,C,H,W) tensor from the list of (C,H,W) tensors\n        # Note: Images have been preprocessed to all have the same shape\n        img_nchw = pt.stack(imgs_chw)\n\n        # Call the model\n        logits = self.model(img_nchw)\n\n        # Convert the shape-(N,num_classes) logits tensor into a list of shape-(num_classes,) tensors\n        return [t for t in logits]\n\n# Wrap the Torchvision ResNet model\nwrapped_model: ic.Model = TorchvisionResNetModel(model, preprocess)",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "create-the-maite-model-wrapper",
        "headings": [
          "Wrap an Image Classification Model",
          "3 Create the MAITE Model Wrapper"
        ]
      },
      "doc_lineno": 260
    },
    {
      "source": "def pil_image_to_maite(img_pil):\n    # Convert the PIL image to a Numpy array\n    img_hwc = np.array(img_pil)  # shape (H, W, C)\n\n    # Use MAITE array index convention for representing images: (C, H, W)\n    img_chw = img_hwc.transpose(2, 0, 1)\n\n    return img_chw\n\nbatch = [pil_image_to_maite(i) for i in example_imgs]",
      "names": [],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 302
    },
    {
      "source": "predictions = wrapped_model(batch)\n\nprint(\n    f\"\"\"\nResNet50 Outputs\n================\nResult Type: {type(predictions)}\nIndividual Prediction Type: {type(predictions[0])}\nIndividual Prediction Shape: {pt.as_tensor(predictions[0]).shape}\nSample Predictions:\"\"\")\nfor prediction, example_img in zip(predictions, example_imgs):\n    print(f\"    Predicted label = {prediction_label(prediction)}\")\n    display(example_img)",
      "names": [
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "zip"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_image_classification_model",
        "ref_id": "examine-the-maite-wrapped-model-output",
        "headings": [
          "Wrap an Image Classification Model",
          "4 Examine the MAITE-wrapped Model Output"
        ]
      },
      "doc_lineno": 315
    }
  ],
  "how_to/wrap_object_detection_dataset": [
    {
      "source": "import json\nimport matplotlib.pyplot as plt\nimport pprint\nimport requests\nimport torch\n\nfrom dataclasses import dataclass\nfrom maite.protocols import DatasetMetadata, DatumMetadata, object_detection as od\nfrom pathlib import Path\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.ops.boxes import box_convert\nfrom torchvision.transforms.functional import pil_to_tensor\nfrom typing import Any\n\n%matplotlib inline\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "pprint"
          ],
          "code_str": "pprint",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "pprint"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "pathlib"
          ],
          "code_str": "pathlib",
          "lineno": 9,
          "end_lineno": 9,
          "context": "import_from",
          "resolved_location": "pathlib"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 9,
          "end_lineno": 9,
          "context": "import_target",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 13,
          "end_lineno": 13,
          "context": "import_target",
          "resolved_location": "typing.Any"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 43
    },
    {
      "source": "def download_images(coco_json_subset: dict[str, Any], root: Path):\n    \"\"\"Download a subset of COCO images.\n\n    Parameters\n    ----------\n    coco_json_subset : dict[str, Any]\n        COCO val2017_first4 JSON file.\n    root : Path\n        Location of COCO data.\n    \"\"\"\n    root.mkdir(parents=True, exist_ok=True)\n\n    for image in coco_json_subset[\"images\"]:\n        url = image[\"coco_url\"]\n        filename = Path(root) / image[\"file_name\"]\n        if filename.exists():\n            print(f\"skipping {url}\")\n        else:\n            print(f\"saving {url} to {filename} ... \", end=\"\")\n            r = requests.get(url)\n            with open(filename, \"wb\") as f:\n                f.write(r.content)\n            print(f\"done\")",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "open"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 23,
          "end_lineno": 23,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 94
    },
    {
      "source": "COCO_ROOT = Path(\"../sample_data/coco/coco_val2017_subset\")\ncoco_subset_json = dict()\nann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\ncoco_subset_json = json.load(open(ann_subset_file, \"r\"))\ndownload_images(coco_subset_json, root=COCO_ROOT)",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "open"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 120
    },
    {
      "source": "tv_dataset = CocoDetection(\n    root=str(COCO_ROOT),\n    annFile=str(ann_subset_file),\n)\n\nprint(f\"\\n{len(tv_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "load-a-subset-of-the-coco-dataset-with-torchvision",
        "headings": [
          "Wrap an Object Detection Dataset",
          "1 Load a subset of the COCO dataset with Torchvision"
        ]
      },
      "doc_lineno": 166
    },
    {
      "source": "img, annotations = tv_dataset[0]\n\n# `img` is a PIL Image\nplt.imshow(img)\nplt.axis(\"off\")  # Optional: Hide the axes\nplt.show()\nprint(f\"{type(img) = }\")\n\n# `annotations` is a list of dictionaries (corresponding to 14 objects in this case)\nprint(f\"{len(annotations) = }\")\n\n# Each annotation dictionary contains the object's bounding box (bbox) plus other info\nprint(f\"{annotations[0].keys() = }\")\n\n# Note that the COCO bounding box format is [x_min, y_min, width, height]\nprint(f\"{annotations[0]['bbox'] = }\")\n\n# Class/category labels and ids are available via the `cats` attribute on the dataset itself\nprint(f\"{tv_dataset.coco.cats[1] = }\")",
      "names": [
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "2 Examine the source dataset"
        ]
      },
      "doc_lineno": 192
    },
    {
      "source": "@dataclass\nclass CocoDetectionTarget:\n    boxes: torch.Tensor\n    labels: torch.Tensor\n    scores: torch.Tensor\n\nclass CocoDetectionDataset:\n    def __init__(self, dataset: CocoDetection, id: str):\n        self._dataset = dataset\n\n        # Get mapping from COCO category to name\n        index2label = {k: v[\"name\"] for k, v in dataset.coco.cats.items()}\n\n        # Add dataset-level metadata attribute, with required id and optional index2label mapping\n        self.metadata: DatasetMetadata = {\n            \"id\": id,\n            \"index2label\": index2label,\n        }\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n\n    def __getitem__(\n        self, index: int\n    ) -> tuple[torch.Tensor, CocoDetectionTarget, DatumMetadata]:\n\n        # Get original data item\n        img_pil, annotations = self._dataset[index]\n\n        # Format input\n        img_pt = pil_to_tensor(img_pil)\n\n        # Format ground truth\n        num_boxes = len(annotations)\n        boxes = torch.zeros(num_boxes, 4)\n        for i, ann in enumerate(annotations):\n            bbox = torch.as_tensor(ann[\"bbox\"])\n            boxes[i, :] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n\n        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n        scores = torch.ones(num_boxes)\n\n        # Format metadata\n        datum_metadata: DatumMetadata = {\n            \"id\": self._dataset.ids[index],\n        }\n\n        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 20,
          "end_lineno": 20,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 25,
          "end_lineno": 25,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 34,
          "end_lineno": 34,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 36,
          "end_lineno": 36,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 281
    },
    {
      "source": "coco_dataset: od.Dataset = CocoDetectionDataset(tv_dataset, \"COCO Detection Subset\")",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "create-a-maite-wrapper-for-the-source-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "3 Create a MAITE wrapper for the source dataset"
        ]
      },
      "doc_lineno": 332
    },
    {
      "source": "print(f\"{len(coco_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 354
    },
    {
      "source": "pprint.pp(coco_dataset.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 366
    },
    {
      "source": "# Get first datum\nimage, target, datum_metadata = coco_dataset[0]\n\n# Display image\nplt.imshow(img)\nplt.axis(\"off\")  # Optional: Hide the axes\nplt.show()\n\n# Bridge/convert ArrayLike's to PyTorch tensors\nimage = torch.as_tensor(image)\nboxes = torch.as_tensor(target.boxes)\nlabels = torch.as_tensor(target.labels)\nscores = torch.as_tensor(target.scores)\n\n# Print shapes\nprint(f\"{image.shape = }\")  # image has height 230 and weight 352\nprint(f\"{boxes.shape = }\")  # there are 14 bounding boxes\nprint(f\"{labels.shape = }\")\nprint(f\"{scores.shape = }\")\n\n# Print datum-level metadata\nprint(f\"{datum_metadata = }\")  # this datum corresponds to image file 000000037777.jpg",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 22,
          "end_lineno": 22,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_dataset",
        "ref_id": "examine-the-maite-wrapped-dataset",
        "headings": [
          "Wrap an Object Detection Dataset",
          "4 Examine the MAITE-wrapped dataset"
        ]
      },
      "doc_lineno": 458
    }
  ],
  "how_to/wrap_object_detection_model": [
    {
      "source": "import io\nimport urllib.request\nfrom dataclasses import asdict, dataclass\nfrom typing import Sequence\n\nimport numpy as np\nimport PIL.Image\nimport torch\nfrom IPython.display import display\nfrom torchvision.models.detection import (\n    FasterRCNN,\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n    fasterrcnn_resnet50_fpn_v2,\n)\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image\nfrom torchvision.utils import draw_bounding_boxes\nfrom ultralytics import YOLO\n\nimport maite.protocols.object_detection as od\nfrom maite.protocols import ArrayLike, ModelMetadata\n\n%load_ext watermark\n%watermark -iv -v",
      "names": [
        {
          "import_components": [
            "io"
          ],
          "code_str": "io",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_target",
          "resolved_location": "io"
        },
        {
          "import_components": [
            "urllib",
            "request"
          ],
          "code_str": "urllib.request",
          "lineno": 2,
          "end_lineno": 2,
          "context": "import_target",
          "resolved_location": "urllib.request"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "asdict"
          ],
          "code_str": "asdict",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "dataclasses.asdict"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 6,
          "end_lineno": 6,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 20,
          "end_lineno": 20,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 32
    },
    {
      "source": "# Load R-CNN model\nrcnn_weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\nrcnn_model = fasterrcnn_resnet50_fpn_v2(weights=rcnn_weights, box_score_thresh=0.9)\nrcnn_model.eval()  # set the RCNN to eval mode (defaults to training)\n\n# Load YOLOv8 Nano model\nyolov8_model = YOLO(\"yolov8n.pt\")  # weights will download automatically",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "load-the-pretrained-faster-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 90
    },
    {
      "source": "img_url = \"https://github.com/pytorch/vision/blob/main/test/assets/encode_jpeg/grace_hopper_517x606.jpg?raw=true\"\nimage_data = urllib.request.urlopen(img_url).read()\npil_img_1 = PIL.Image.open(io.BytesIO(image_data))\n\nimg_url = \"https://www.ultralytics.com/images/bus.jpg\"\nimage_data = urllib.request.urlopen(img_url).read()\npil_img_2 = PIL.Image.open(io.BytesIO(image_data))",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 198
    },
    {
      "source": "# Convert to PyTorch tensors\ntensor_img_1 = pil_to_tensor(pil_img_1)\ntensor_img_2 = pil_to_tensor(pil_img_2)\nrcnn_imgs = [tensor_img_1, tensor_img_2]\n\n# Get the inference transforms assocated with these pretrained weights\npreprocess = rcnn_weights.transforms()\n\n# Apply inference preprocessing transforms\nbatch = [preprocess(img) for img in rcnn_imgs]\n\nrcnn_preds = rcnn_model(batch)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 210
    },
    {
      "source": "yolo_imgs = [pil_img_1, pil_img_2]\n\nyolo_preds = yolov8_model(yolo_imgs, verbose=False)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 227
    },
    {
      "source": "rcnn_pred = rcnn_preds[0]\nyolo_pred = yolo_preds[0]\n\nrcnn_boxes = rcnn_pred[\"boxes\"]\nyolo_boxes = yolo_pred.boxes\n\nprint(\n    f\"\"\"\nR-CNN Outputs\n=============\nResult Type: {type(rcnn_pred)}\nResult Attributes: {rcnn_pred.keys()}\nBox Types: {type(rcnn_boxes)}\n\nYOLO Outputs\n============\nResult Type: {type(yolo_pred)}\nResult Attributes: {yolo_pred._keys}\nBox Types: {type(yolo_boxes)}\n\"\"\"\n)",
      "names": [
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "perform-model-inference-on-sample-images",
        "headings": [
          "Wrap an Object Detection Model",
          "2 Perform Model Inference on Sample Images"
        ]
      },
      "doc_lineno": 235
    },
    {
      "source": "imgs: od.InputBatchType = [tensor_img_1, tensor_img_2]\n\n@dataclass\nclass ObjectDetectionTargetImpl:\n    boxes: np.ndarray\n    labels: np.ndarray\n    scores: np.ndarray\n\ndef render_wrapped_results(imgs, preds, model_metadata):\n    names = model_metadata[\"index2label\"]\n    for img, pred in zip(imgs, preds):\n        pred_labels = [names[label] for label in pred.labels]\n        box = draw_bounding_boxes(\n            img,\n            boxes=torch.as_tensor(pred.boxes),\n            labels=pred_labels,\n            colors=\"red\",\n            width=4,\n            font=\"DejaVuSans\", # if necessary, change to TrueType font available on your system\n            font_size=30,\n        )\n        im = to_pil_image(box.detach())\n        h, w = im.size\n        im = im.resize((h // 2, w // 2))\n        display(im)",
      "names": [
        {
          "import_components": [
            "zip"
          ],
          "code_str": "zip",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "zip"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "create-maite-wrappers-for-the-r-cnn-and-yolov8-models",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models"
        ]
      },
      "doc_lineno": 309
    },
    {
      "source": "class WrappedRCNN:\n    def __init__(\n        self, model: FasterRCNN, weights: FasterRCNN_ResNet50_FPN_V2_Weights, id: str, **kwargs\n    ):\n        self.model = model\n        self.weights = weights\n        self.kwargs = kwargs\n\n        # Add required model metadata attribute\n        index2label = {i: category for i, category in enumerate(weights.meta[\"categories\"])}\n        self.metadata: ModelMetadata = {\n            \"id\": id,\n            \"index2label\": index2label\n        }\n\n    def __call__(self, batch: od.InputBatchType) -> Sequence[ObjectDetectionTargetImpl]:\n        # Get MAITE inputs ready for native model\n        preprocess = self.weights.transforms()\n        batch = [preprocess(img) for img in batch]\n\n        # Perform inference\n        results = self.model(batch, **self.kwargs)\n\n        # Restructure results to conform to MAITE\n        all_detections = []\n        for result in results:\n            boxes = result[\"boxes\"].detach().numpy()\n            labels = result[\"labels\"].detach().numpy()\n            scores = result[\"scores\"].detach().numpy()\n            all_detections.append(\n                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n            )\n\n        return all_detections\n\nwrapped_rcnn_model: od.Model = WrappedRCNN(rcnn_model, rcnn_weights, \"TorchVision.FasterRCNN_ResNet50_FPN_V2\")",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 347
    },
    {
      "source": "wrapped_rcnn_preds = wrapped_rcnn_model(imgs)\nwrapped_rcnn_preds",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 386
    },
    {
      "source": "render_wrapped_results(imgs, wrapped_rcnn_preds, wrapped_rcnn_model.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-r-cnn-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.1 Wrap the R-CNN Model"
        ]
      },
      "doc_lineno": 406
    },
    {
      "source": "class WrappedYOLO:\n    def __init__(self, model: YOLO, id: str, **kwargs):\n        self.model = model\n        self.kwargs = kwargs\n\n        # Add required model metadata attribute\n        self.metadata: ModelMetadata = {\n            \"id\": id,\n            \"index2label\": model.names # already a mapping from integer class index to string name\n        }\n\n    def __call__(self, batch: od.InputBatchType) -> Sequence[ObjectDetectionTargetImpl]:\n        # Get MAITE inputs ready for native model\n        # Bridge/convert input to np.ndarray and tranpose (C, H, W) -> (H, W, C)\n        batch = [np.asarray(x).transpose((1, 2, 0)) for x in batch]\n\n        # Perform inference\n        results = self.model(batch, **self.kwargs)\n\n        # Restructure results to conform to MAITE\n        all_detections = []\n        for result in results:\n            detections = result.boxes\n            if detections is None:\n                continue\n            detections = detections.cpu().numpy()\n            boxes = np.asarray(detections.xyxy)\n            labels = np.asarray(detections.cls, dtype=np.uint8)\n            scores = np.asarray(detections.conf)\n            all_detections.append(\n                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n            )\n\n        return all_detections\n\nwrapped_yolov8_model: od.Model = WrappedYOLO(yolov8_model, id=\"Ultralytics.YOLOv8\", verbose=False)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 439
    },
    {
      "source": "wrapped_yolo_preds = wrapped_yolov8_model(imgs)\nwrapped_yolo_preds",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 480
    },
    {
      "source": "render_wrapped_results(imgs, wrapped_yolo_preds, wrapped_yolov8_model.metadata)",
      "names": [],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 501
    },
    {
      "source": "# Get predictions from both models for first image in batch\nwrapped_rcnn_pred = wrapped_rcnn_preds[0]\nwrapped_yolo_pred = wrapped_yolo_preds[0]\n\nwrapped_rcnn_fields = vars(wrapped_rcnn_pred).keys()\nwrapped_yolo_fields = vars(wrapped_yolo_pred).keys()\n\nprint(\n    f\"\"\"\nWrapped R-CNN Outputs\n=====================\nResult Type: {type(wrapped_rcnn_pred)}\nResult Attributes: {wrapped_rcnn_fields}\n\nYOLO Outputs\n============\nResult Type: {type(wrapped_yolo_pred)}\nResult Attributes: {wrapped_yolo_fields}\n\"\"\"\n)",
      "names": [
        {
          "import_components": [
            "vars"
          ],
          "code_str": "vars",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "vars"
        },
        {
          "import_components": [
            "vars"
          ],
          "code_str": "vars",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "vars"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 17,
          "end_lineno": 17,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "how_to/wrap_object_detection_model",
        "ref_id": "wrap-the-yolo-model",
        "headings": [
          "Wrap an Object Detection Model",
          "3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models",
          "3.2 Wrap the YOLO Model"
        ]
      },
      "doc_lineno": 514
    }
  ],
  "how_tos": [],
  "index": [],
  "tutorials": [],
  "tutorials/hf_image_classification": [
    {
      "source": "import datasets\nimport maite.protocols.image_classification as ic\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom maite.protocols import ArrayLike, DatasetMetadata, MetricMetadata, ModelMetadata\nfrom maite.workflows import evaluate\nfrom torchmetrics import Accuracy, Metric\nfrom torchvision.transforms.functional import to_tensor, resize\nfrom transformers import AutoModelForImageClassification, ViTForImageClassification\nfrom typing import Any, Optional, Sequence",
      "names": [
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 4,
          "end_lineno": 4,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 7,
          "end_lineno": 7,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "maite",
            "workflows",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 8,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "maite.workflows.evaluate"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 12,
          "end_lineno": 12,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 12,
          "end_lineno": 12,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Optional"
          ],
          "code_str": "Optional",
          "lineno": 12,
          "end_lineno": 12,
          "context": "import_target",
          "resolved_location": "typing.Optional"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 12,
          "end_lineno": 12,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 47
    },
    {
      "source": "from watermark import watermark\nprint(\"This notebook was executed with the following:\\n\")\nprint(watermark(python=True, packages=\"datasets,jupyter,matplotlib,numpy,torch,torchmetrics,torchvision,transformers,watermark\"))",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "getting-started",
        "headings": [
          "Hugging Face Image Classification Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 62
    },
    {
      "source": "subset_size = 256\nhf_dataset: datasets.Dataset = datasets.load_dataset(\"cifar10\", split=f\"test[:{subset_size}]\") # type: ignore",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 104
    },
    {
      "source": "class HuggingFaceDataset:\n    def __init__(self, hf_dataset: datasets.Dataset, id: str, index2label: dict[int, str], resize_shape: Optional[list[int]] = None):\n        self.hf_dataset = hf_dataset\n        self.num_classes = hf_dataset.features[\"label\"].num_classes\n        self.resize_shape = resize_shape\n\n        # Create required dataset metadata attribute\n        self.metadata: DatasetMetadata = DatasetMetadata(id=id, index2label=index2label)\n\n    def __len__(self) -> int:\n        return len(self.hf_dataset)\n\n    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor, ic.DatumMetadataType]:\n        if index < 0 or index >= len(self):\n            raise IndexError(f\"Index {index} is out of range for the dataset, which has length {len(self)}.\")\n\n        # Get the PIL image and integer label from the base HF dataset element (which is a dictionary)\n        item = self.hf_dataset[index]\n        img_pil = item[\"img\"]\n        label = item[\"label\"]\n\n        # Convert the PIL image to a PyTorch tensor for compatibility with PyTorch libraries\n        img_pt = to_tensor(img_pil)\n\n        # Apply resizing if requested\n        if self.resize_shape is not None:\n            img_pt = resize(img_pt, self.resize_shape)\n\n        # Create one-hot encoded tensor with true class label for this image\n        target = torch.zeros(self.num_classes)\n        target[label] = 1\n\n        return img_pt, target, ic.DatumMetadataType(id=index)",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 14,
          "end_lineno": 14,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "IndexError"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 157
    },
    {
      "source": "# Create map from integer class index to string label\nnum_classes = hf_dataset.features[\"label\"].num_classes\nindex2label = {i: hf_dataset.features[\"label\"].int2str(i) for i in range(num_classes)}\n\n# Wrap dataset\nwrapped_hf_dataset: ic.Dataset = HuggingFaceDataset(\n    hf_dataset,\n    id=\"CIFAR-10\",\n    index2label=index2label,\n    resize_shape=[224, 224]\n)\n\nprint(f\"{len(wrapped_hf_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 203
    },
    {
      "source": "ncols = 6\nfig, ax = plt.subplots(1, ncols, figsize=(6, 2))\nfor i in range(ncols):\n    # Get datum i\n    img, label_onehot, md = wrapped_hf_dataset[i]\n\n    # Convert to NumPy array in height, width, color channel (HWC) order (for display with matplotlib)\n    img_np = np.asarray(img).transpose(1, 2, 0)\n\n    # Get ground truth class index and label\n    index = torch.as_tensor(label_onehot).argmax().item()\n    label = index2label[int(index)]\n\n    # Plot image with label\n    ax[i].axis(\"off\")\n    ax[i].imshow(img_np)\n    ax[i].set_title(label)\n\nfig.tight_layout()",
      "names": [
        {
          "import_components": [
            "range"
          ],
          "code_str": "range",
          "lineno": 3,
          "end_lineno": 3,
          "context": "none",
          "resolved_location": "range"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-dataset",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Dataset"
        ]
      },
      "doc_lineno": 228
    },
    {
      "source": "hf_model: ViTForImageClassification = AutoModelForImageClassification.from_pretrained(\n    \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\"\n)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 265
    },
    {
      "source": "class HuggingFaceModel:\n    def __init__(self, hf_model: ViTForImageClassification, id: str, index2label: dict[int, str], device: str = \"cpu\"):\n        self.hf_model = hf_model\n        self.device = device\n\n        # Create required model metadata attribute\n        self.metadata: ModelMetadata = ModelMetadata(id=id, index2label=index2label)\n\n        # Move the model to requested device and set to eval mode\n        self.hf_model.to(device) # type: ignore\n        self.hf_model.eval()\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[torch.Tensor]:\n        # Combine inputs into PyTorch tensor of shape-(N,C,H,W) (batch size, color channels, height, width)\n        batch_pt = torch.stack([torch.as_tensor(x) for x in batch])\n\n        # Move tensor to the desired device\n        batch_pt = batch_pt.to(self.device)\n\n        # Apply model to batch (NOTE: preprocessing not needed for this particular HF model)\n        output = self.hf_model(batch_pt)\n\n        # Restructure to expected output format (sequence of probability/logit vectors)\n        result = [x for x in output.logits.detach().cpu()]\n        return result",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 290
    },
    {
      "source": "wrapped_hf_model: ic.Model = HuggingFaceModel(\n    hf_model,\n    id=\"vit-base-patch16-224-in21k-finetuned-cifar10\",\n    index2label=index2label\n)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 318
    },
    {
      "source": "# Create batch with single image\ni = 0\nx, y, md = wrapped_hf_dataset[i]\nxb, yb, mdb = [x], [y], [md]\n\n# Apply model and get first (only) prediction of size-1 batch of results\npreds = wrapped_hf_model(xb)[0]\ny_hat = torch.as_tensor(preds).argmax().item()\n\n# Plot image with model prediction\nfig, ax = plt.subplots(figsize=(1.5, 1.5))\nimg_np = np.asarray(x).transpose(1, 2, 0)\nax.axis(\"off\")\nax.imshow(img_np)\nax.set_title(f\"pred: {index2label[int(y_hat)]}\")\nfig.tight_layout()",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "wrapping-a-hugging-face-model",
        "headings": [
          "Hugging Face Image Classification Example",
          "Wrapping a Hugging Face Model"
        ]
      },
      "doc_lineno": 329
    },
    {
      "source": "tm_acc: Metric = Accuracy(task=\"multiclass\", num_classes=10)",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 365
    },
    {
      "source": "class TorchMetricsClassificationMetric:\n    def __init__(self, tm_metric: Metric, name: str, device: str = \"cpu\"):\n        self.tm_metric = tm_metric\n        self.name = name\n        self.device = device\n\n        # Create required metric metadata attribute\n        self.metadata: MetricMetadata = MetricMetadata(id=name)\n\n    def reset(self):\n        self.tm_metric.reset()\n\n    def update(self, preds: Sequence[ArrayLike], targets: Sequence[ArrayLike]) -> None:\n        # Convert inputs to PyTorch tensors of shape-(N, num_classes)\n        preds_pt: torch.Tensor = torch.stack([torch.as_tensor(x) for x in preds]).to(self.device)\n        assert preds_pt.ndim == 2\n\n        targets_pt: torch.Tensor = torch.stack([torch.as_tensor(x) for x in targets]).to(self.device)\n        assert targets_pt.ndim == 2\n\n        # Convert probabilities/logits to predicted class indices and update native TorchMetrics metric\n        self.tm_metric.update(preds_pt.argmax(dim=1), targets_pt.argmax(dim=1))\n\n    def compute(self) -> dict[str, Any]:\n        result = {}\n        result[self.name] = self.tm_metric.compute()\n        return result",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 24,
          "end_lineno": 24,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 373
    },
    {
      "source": "wrapped_tm_acc: ic.Metric = TorchMetricsClassificationMetric(tm_acc, \"accuracy\")",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "metrics",
        "headings": [
          "Hugging Face Image Classification Example",
          "Metrics"
        ]
      },
      "doc_lineno": 403
    },
    {
      "source": "results, _, _ = evaluate(\n    dataset=wrapped_hf_dataset,\n    model=wrapped_hf_model,\n    metric=wrapped_tm_acc\n)\n\nresults",
      "names": [],
      "example": {
        "document": "tutorials/hf_image_classification",
        "ref_id": "workflows",
        "headings": [
          "Hugging Face Image Classification Example",
          "Workflows"
        ]
      },
      "doc_lineno": 414
    }
  ],
  "tutorials/torchvision_object_detection": [
    {
      "source": "from __future__ import annotations\n\nimport json\nimport kornia.augmentation as K\nimport matplotlib.pyplot as plt\nimport requests\nimport torch\nimport numpy as np\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom PIL import Image\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.models.detection import (\n    fasterrcnn_resnet50_fpn_v2,\n    FasterRCNN_ResNet50_FPN_V2_Weights,\n)\nfrom torchvision.ops.boxes import box_convert\nfrom torchvision.transforms.functional import to_pil_image, pil_to_tensor\nfrom torchvision.utils import draw_bounding_boxes\nfrom typing import Any, Callable, Sequence\n\nimport maite.protocols.object_detection as od\n\nfrom maite.protocols import (\n    ArrayLike,\n    DatasetMetadata,\n    ModelMetadata,\n    MetricMetadata,\n    AugmentationMetadata,\n    DatumMetadata\n)\nfrom maite.workflows import evaluate",
      "names": [
        {
          "import_components": [
            "__future__"
          ],
          "code_str": "__future__",
          "lineno": 1,
          "end_lineno": 1,
          "context": "import_from",
          "resolved_location": "__future__"
        },
        {
          "import_components": [
            "json"
          ],
          "code_str": "json",
          "lineno": 3,
          "end_lineno": 3,
          "context": "import_target",
          "resolved_location": "json"
        },
        {
          "import_components": [
            "numpy"
          ],
          "code_str": "numpy",
          "lineno": 8,
          "end_lineno": 8,
          "context": "import_target",
          "resolved_location": "numpy"
        },
        {
          "import_components": [
            "dataclasses"
          ],
          "code_str": "dataclasses",
          "lineno": 10,
          "end_lineno": 10,
          "context": "import_from",
          "resolved_location": "dataclasses"
        },
        {
          "import_components": [
            "dataclasses",
            "dataclass"
          ],
          "code_str": "dataclass",
          "lineno": 10,
          "end_lineno": 10,
          "context": "import_target",
          "resolved_location": "dataclasses.dataclass"
        },
        {
          "import_components": [
            "pathlib"
          ],
          "code_str": "pathlib",
          "lineno": 11,
          "end_lineno": 11,
          "context": "import_from",
          "resolved_location": "pathlib"
        },
        {
          "import_components": [
            "pathlib",
            "Path"
          ],
          "code_str": "Path",
          "lineno": 11,
          "end_lineno": 11,
          "context": "import_target",
          "resolved_location": "pathlib.Path"
        },
        {
          "import_components": [
            "typing"
          ],
          "code_str": "typing",
          "lineno": 22,
          "end_lineno": 22,
          "context": "import_from",
          "resolved_location": "typing"
        },
        {
          "import_components": [
            "typing",
            "Any"
          ],
          "code_str": "Any",
          "lineno": 22,
          "end_lineno": 22,
          "context": "import_target",
          "resolved_location": "typing.Any"
        },
        {
          "import_components": [
            "typing",
            "Callable"
          ],
          "code_str": "Callable",
          "lineno": 22,
          "end_lineno": 22,
          "context": "import_target",
          "resolved_location": "typing.Callable"
        },
        {
          "import_components": [
            "typing",
            "Sequence"
          ],
          "code_str": "Sequence",
          "lineno": 22,
          "end_lineno": 22,
          "context": "import_target",
          "resolved_location": "typing.Sequence"
        },
        {
          "import_components": [
            "maite",
            "protocols",
            "ArrayLike"
          ],
          "code_str": "ArrayLike",
          "lineno": 26,
          "end_lineno": 33,
          "context": "import_target",
          "resolved_location": "maite.protocols.ArrayLike"
        },
        {
          "import_components": [
            "maite",
            "workflows",
            "evaluate"
          ],
          "code_str": "evaluate",
          "lineno": 34,
          "end_lineno": 34,
          "context": "import_target",
          "resolved_location": "maite.workflows.evaluate"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "getting-started",
        "headings": [
          "Torchvision Object Detection Example",
          "Getting Started"
        ]
      },
      "doc_lineno": 57
    },
    {
      "source": "def download_images(coco_json_subset: dict[str, Any], root: str | Path):\n    root = Path(root)\n    root.mkdir(parents=True, exist_ok=True)\n\n    for image in coco_json_subset[\"images\"]:\n        url = image[\"coco_url\"]\n        filename = Path(root) / image[\"file_name\"]\n        if filename.exists():\n            print(f\"skipping {url}\")\n        else:\n            print(f\"saving {url} to {filename} ... \", end=\"\")\n            r = requests.get(url)\n            with open(filename, \"wb\") as f:\n                f.write(r.content)\n            print(f\"done\")\n\nCOCO_ROOT = Path(\"coco_val2017_subset\")\nann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\ncoco_subset_json = json.load(open(ann_subset_file, \"r\"))\n\ndownload_images(coco_subset_json, root=COCO_ROOT)",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 1,
          "end_lineno": 1,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 13,
          "end_lineno": 13,
          "context": "none",
          "resolved_location": "open"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "open"
          ],
          "code_str": "open",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "open"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 117
    },
    {
      "source": "tv_dataset = CocoDetection(\n    root=\"coco_val2017_subset\",\n    annFile=\"coco_val2017_subset/instances_val2017_first4.json\"\n)\n\nprint(f\"\\n{len(tv_dataset) = }\")",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 169
    },
    {
      "source": "# Get first image and its annotations\nimg, annotations = tv_dataset[0]\n\n# Explore structure\nprint(f\"{type(img) = }\")\nprint(f\"{type(annotations) = }\")\nprint(f\"{type(annotations[0]) = }\")\nprint(f\"{len(annotations[0]) = }\")\nprint(f\"{annotations[0].keys() = }\")\nprint(f\"{annotations[0]['bbox'] = }\")\nprint(f\"{annotations[0]['category_id'] = }\")\nprint(f\"{tv_dataset.coco.cats[64] = }\")",
      "names": [
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "type"
          ],
          "code_str": "type",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "type"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 7,
          "end_lineno": 7,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 11,
          "end_lineno": 11,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 12,
          "end_lineno": 12,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "native-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Native Dataset"
        ]
      },
      "doc_lineno": 194
    },
    {
      "source": "@dataclass\nclass CocoDetectionTarget:\n    boxes: torch.Tensor\n    labels: torch.Tensor\n    scores: torch.Tensor\n\n# Get mapping from COCO category to name\nindex2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}\n\nclass MaiteCocoDetection:\n    metadata: DatasetMetadata = {'id': 'COCO Detection Dataset'}\n    # We can optionally include index2label mapping within DatasetMetadata\n    metadata: DatasetMetadata = {**metadata, 'index2label': index2label}\n\n    def __init__(self, dataset: CocoDetection):\n        self._dataset = dataset\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n\n    def __getitem__(self, index: int) -> tuple[ArrayLike, CocoDetectionTarget, DatumMetadata]:\n        try:\n            # get original data item\n            img_pil, annotations = self._dataset[index]\n        except IndexError as e:\n            # Here the underlying dataset is raising an IndexError since the index is beyond the\n            # container's bounds. When wrapping custom datasets, wrappers are responsible to for\n            # raising an IndexError in `__getitem__` when an index exceeds the container's bounds;\n            # this enables iteration on the wrapper to properly terminate.\n            print(f\"The index number {index} is out of range for the dataset which has length {len(self._dataset)}\")\n            raise(e)\n\n        # format input\n        img_pt = pil_to_tensor(img_pil)\n\n        # format ground truth\n        num_boxes = len(annotations)\n        boxes = torch.zeros(num_boxes, 4)\n        for i, ann in enumerate(annotations):\n            bbox = torch.as_tensor(ann[\"bbox\"])\n            boxes[i,:] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n\n        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n        scores = torch.ones(num_boxes)\n\n        # format metadata\n        datum_metadata: DatumMetadata = {\n            \"id\": self._dataset.ids[index],\n        }\n\n        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata",
      "names": [
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 18,
          "end_lineno": 18,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 19,
          "end_lineno": 19,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 21,
          "end_lineno": 21,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "IndexError"
          ],
          "code_str": "IndexError",
          "lineno": 25,
          "end_lineno": 25,
          "context": "none",
          "resolved_location": "IndexError"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 30,
          "end_lineno": 30,
          "context": "none",
          "resolved_location": "print"
        },
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 37,
          "end_lineno": 37,
          "context": "none",
          "resolved_location": "len"
        },
        {
          "import_components": [
            "enumerate"
          ],
          "code_str": "enumerate",
          "lineno": 39,
          "end_lineno": 39,
          "context": "none",
          "resolved_location": "enumerate"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 244
    },
    {
      "source": "dataset: od.Dataset = MaiteCocoDetection(tv_dataset)\nlen(dataset)",
      "names": [
        {
          "import_components": [
            "len"
          ],
          "code_str": "len",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "len"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 308
    },
    {
      "source": "def create_pil_image(\n    input: ArrayLike,\n    target: od.ObjectDetectionTarget,\n    index2label: dict[int, str],\n    color: str = \"red\"\n) -> Image.Image:\n    img_pt = torch.as_tensor(input)\n    boxes = torch.as_tensor(target.boxes)\n    label_ids = torch.as_tensor(target.labels)\n    label_names = [index2label[int(id.item())] for id in label_ids]\n    box = draw_bounding_boxes(img_pt, boxes=boxes, labels=label_names, colors=color, width=2)\n    return to_pil_image(box.detach())",
      "names": [
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "int"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 4,
          "end_lineno": 4,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "int"
          ],
          "code_str": "int",
          "lineno": 10,
          "end_lineno": 10,
          "context": "none",
          "resolved_location": "int"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 325
    },
    {
      "source": "# Get mapping from COCO category to name\nindex2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 340
    },
    {
      "source": "# Get sample image and overlay ground truth annotations (bounding boxes)\ni = 0\ninput, target, _ = dataset[i]\nimg = create_pil_image(input, target, index2label)\n\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Ground Truth\")\nax.imshow(img);",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapped-dataset",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Dataset",
          "Wrapped Dataset"
        ]
      },
      "doc_lineno": 345
    },
    {
      "source": "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\ntv_model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9, progress=False)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 371
    },
    {
      "source": "class TorchvisionDetector:\n    def __init__(self, model: torch.nn.Module, metadata: ModelMetadata, transforms: Any, device: str):\n        self.model = model\n        self.metadata = metadata\n        self.transforms = transforms\n        self.device = device\n\n        self.model.eval()\n        self.model.to(device)\n\n    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[CocoDetectionTarget]:\n\n        # convert to list of tensors, transfer to device, and apply inference transforms\n        # - https://pytorch.org/vision/stable/models.html\n        # - \"The models expect a list of Tensor[C, H, W].\"\n        tv_input = [self.transforms(torch.as_tensor(b_elem)).to(self.device) for b_elem in batch]\n\n        # get predictions\n        tv_predictions = self.model(tv_input)\n\n        # reformat output\n        predictions = [\n            CocoDetectionTarget(\n                p[\"boxes\"].detach().cpu(),\n                p[\"labels\"].detach().cpu(),\n                p[\"scores\"].detach().cpu()\n            )\n            for p in tv_predictions\n        ]\n\n        return predictions",
      "names": [
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 2,
          "end_lineno": 2,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 380
    },
    {
      "source": "model: od.Model = TorchvisionDetector(\n    model=tv_model,\n    metadata={'id': 'TorchvisionDetector', 'index2label': index2label},\n    transforms=weights.transforms(),\n    device=\"cpu\"\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 414
    },
    {
      "source": "# Create batch with sample image\ni = 0\nx, y, md = dataset[i]\nx = torch.as_tensor(x)\nxb, yb, mdb = x.unsqueeze(0), [y], [md]\nprint(f\"{xb.shape = }\")\n\n# Get predictions for batch(which just has one image for this example)\npreds = model([xb[0]])\n\n# Overlay detections on image\nimg = create_pil_image(xb[0], preds[0], index2label)\n\n# Plot\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Prediction\")\nax.imshow(img);",
      "names": [
        {
          "import_components": [
            "print"
          ],
          "code_str": "print",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "print"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "wrapping-a-torchvision-model",
        "headings": [
          "Torchvision Object Detection Example",
          "Wrapping a Torchvision Model"
        ]
      },
      "doc_lineno": 426
    },
    {
      "source": "tm_metric = MeanAveragePrecision(\n    box_format=\"xyxy\",\n    iou_type=\"bbox\",\n    iou_thresholds=[0.5],\n    rec_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    max_detection_thresholds=[1, 10, 100],\n    class_metrics=False,\n    extended_summary=False,\n    average=\"macro\"\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 472
    },
    {
      "source": "class WrappedTorchmetricsMetric:\n    def __init__(\n        self,\n        tm_metric: Callable[\n            [list[dict[str, torch.Tensor]], list[dict[str, torch.Tensor]]],\n            dict[str, Any],\n        ],\n        metadata: MetricMetadata\n    ):\n        self._tm_metric = tm_metric\n        self.metadata = metadata\n\n    # Create utility function to convert ObjectDetectionTarget_impl type to what\n    # the type expected by torchmetrics IntersectionOverUnion metric\n    @staticmethod\n    def to_tensor_dict(target: od.ObjectDetectionTarget) -> dict[str, torch.Tensor]:\n        \"\"\"\n        Convert an ObjectDetectionTarget_impl into a dictionary expected internally by\n        raw `update` method of raw torchmetrics method\n        \"\"\"\n        out = {\n            \"boxes\": torch.as_tensor(target.boxes),\n            \"scores\": torch.as_tensor(target.scores),\n            \"labels\": torch.as_tensor(target.labels),\n        }\n\n        return out\n\n    def update(self, preds: od.TargetBatchType, targets: od.TargetBatchType) -> None:\n        # Convert to natively-typed from of preds/targets\n        preds_tm = [self.to_tensor_dict(pred) for pred in preds]\n        targets_tm = [self.to_tensor_dict(tgt) for tgt in targets]\n        self._tm_metric.update(preds_tm, targets_tm)\n\n    def compute(self) -> dict[str, Any]:\n        return self._tm_metric.compute()\n\n    def reset(self) -> None:\n        self._tm_metric.reset()",
      "names": [
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "list"
          ],
          "code_str": "list",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "list"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 5,
          "end_lineno": 5,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 6,
          "end_lineno": 6,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "staticmethod"
          ],
          "code_str": "staticmethod",
          "lineno": 15,
          "end_lineno": 15,
          "context": "none",
          "resolved_location": "staticmethod"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 16,
          "end_lineno": 16,
          "context": "none",
          "resolved_location": "str"
        },
        {
          "import_components": [
            "dict"
          ],
          "code_str": "dict",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "dict"
        },
        {
          "import_components": [
            "str"
          ],
          "code_str": "str",
          "lineno": 35,
          "end_lineno": 35,
          "context": "none",
          "resolved_location": "str"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 488
    },
    {
      "source": "mAP_metric: od.Metric = WrappedTorchmetricsMetric(tm_metric, metadata={'id': 'torchmetrics_map_metric'})",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "metrics",
        "headings": [
          "Torchvision Object Detection Example",
          "Metrics"
        ]
      },
      "doc_lineno": 530
    },
    {
      "source": "# Run evaluate over original (clean) dataset\nresults, _, _ = evaluate(\n    model=model,\n    dataset=dataset,\n    metric=mAP_metric\n)\n\n# Report mAP_50 performance\nresults[\"map_50\"]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "workflows",
        "headings": [
          "Torchvision Object Detection Example",
          "Workflows"
        ]
      },
      "doc_lineno": 541
    },
    {
      "source": "kornia_noise = K.RandomGaussianNoise(\n    mean=0.0,\n    std=0.08, # relative to [0, 1] pixel values\n    p=1.0,\n    keepdim=True\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 581
    },
    {
      "source": "class WrappedKorniaAugmentation:\n    def __init__(self, kornia_aug: Any, metadata: AugmentationMetadata):\n        self.kornia_aug = kornia_aug\n        self.metadata = metadata\n\n    def __call__(\n        self,\n        batch: tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType],\n    ) -> tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType]:\n        # Unpack tuple\n        xb, yb, metadata = batch\n\n        # Type narrow / bridge input batch to PyTorch tensor\n        xb_pt = [torch.as_tensor(xb_i) for xb_i in xb]\n        assert xb_pt[0].ndim == 3, 'Input should be sequence of 3d ArrayLikes'\n\n        # Apply augmentation to batch\n        # Return augmentation outputs as uint8\n        # - NOTE: assumes input batch has pixels in [0, 255]\n        xb_aug = [(self.kornia_aug(xb_pti / 255.0).clamp(min=0.0, max=1.0) * 255.0).to(torch.uint8) for xb_pti in xb_pt]\n\n        # Return augmented inputs and pass through unchanged targets and metadata\n        return xb_aug, yb, metadata",
      "names": [
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 9,
          "end_lineno": 9,
          "context": "none",
          "resolved_location": "tuple"
        },
        {
          "import_components": [
            "tuple"
          ],
          "code_str": "tuple",
          "lineno": 8,
          "end_lineno": 8,
          "context": "none",
          "resolved_location": "tuple"
        }
      ],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 593
    },
    {
      "source": "noise: od.Augmentation = WrappedKorniaAugmentation(\n    kornia_noise, metadata={\"id\": \"kornia_rnd_gauss_noise\"}\n)",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 619
    },
    {
      "source": "# Create batch with sample image\ni = 0\nx, y, md = dataset[i]\nx = torch.as_tensor(x)\nxb, yb, mdb = [x], [y], [md]\n\n# Apply augmentation\nxb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))\n\n# Get predictions for augmented batch (which just has one image for this example)\npreds_aug = model(xb_aug)\n\n# Overlay detections on image\nxb_aug = torch.as_tensor(xb_aug[0])\nimg_aug = create_pil_image(xb_aug, preds_aug[0], index2label)\n\n# Show result\nfig, ax = plt.subplots()\nax.axis(\"off\")\nax.set_title(\"Perturbed\")\nax.imshow(img_aug);",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 628
    },
    {
      "source": "# Run evaluate over perturbed dataset\nresults, _, _ = evaluate(\n    model=model,\n    dataset=dataset,\n    metric=mAP_metric,\n    augmentation=noise,\n)\n\n# Report mAP_50 performance\nresults[\"map_50\"]",
      "names": [],
      "example": {
        "document": "tutorials/torchvision_object_detection",
        "ref_id": "augmentations",
        "headings": [
          "Torchvision Object Detection Example",
          "Augmentations"
        ]
      },
      "doc_lineno": 661
    }
  ]
}