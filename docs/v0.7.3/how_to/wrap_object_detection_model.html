
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.19: https://docutils.sourceforge.io/" name="generator"/>
<title>Wrap an Object Detection Model — maite 0.7.3 documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-codeautolink.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/sphinx_highlight.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-115029372-2"></script>
<script src="../_static/gtag.js"></script>
<script src="../_static/design-tabs.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'how_to/wrap_object_detection_model';</script>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../explanation.html" rel="next" title="Explanation"/>
<link href="wrap_object_detection_dataset.html" rel="prev" title="Wrap an Object Detection Dataset"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<a class="skip-link" href="#main-content">Skip to main content</a>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search the docs ..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search the docs ..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
<label class="sidebar-toggle primary-toggle" for="__primary">
<span class="fa-solid fa-bars"></span>
</label>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../index.html">
<p class="title logo__title">maite 0.7.3 documentation</p>
</a></div>
</div>
<div class="col-lg-9 navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item"><nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
    Site Navigation
  </p>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../how_tos.html">
                        How-To Guides
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../explanation.html">
                        Explanation
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../api_reference.html">
                        Reference
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../changes.html">
                        Changelog
                      </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
</div>
<div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
</div>
<label class="sidebar-toggle secondary-toggle" for="__secondary">
<span class="fa-solid fa-outdent"></span>
</label>
</div>
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item"><nav class="navbar-nav">
<p aria-label="Site Navigation" aria-level="1" class="sidebar-header-items__title" role="heading">
    Site Navigation
  </p>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../tutorials.html">
                        Tutorials
                      </a>
</li>
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../how_tos.html">
                        How-To Guides
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../explanation.html">
                        Explanation
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../api_reference.html">
                        Reference
                      </a>
</li>
<li class="nav-item">
<a class="nav-link nav-internal" href="../changes.html">
                        Changelog
                      </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item"><nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="static_typing.html">Enable Static Type Checking</a></li>
<li class="toctree-l1"><a class="reference internal" href="wrap_image_classification_dataset.html">Wrap an Image Classification Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="wrap_image_classification_model.html">Wrap an Image Classification Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="wrap_object_detection_dataset.html">Wrap an Object Detection Dataset</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Wrap an Object Detection Model</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumbs">
<ul aria-label="Breadcrumb" class="bd-breadcrumbs" role="navigation">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../how_tos.html">How-To Guides</a></li>
<li aria-current="page" class="breadcrumb-item active">Wrap an Object Detection Model</li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" role="main">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY
Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).
SPDX-License-Identifier: MIT
</pre></div>
</div>
<section id="wrap-an-object-detection-model">
<h1>Wrap an Object Detection Model<a class="headerlink" href="#wrap-an-object-detection-model" title="Permalink to this heading">#</a></h1>
<p>This guide explains how to wrap common object detection models, such as
the <a class="reference external" href="https://pytorch.org/vision/stable/models.html#object-detection">Torchvision Faster
R-CNN</a>
and <a class="reference external" href="https://docs.ultralytics.com/models/yolov8/">Ultralytics
YOLOv8</a>, to create
models that conform to MAITE’s
<a class="reference external" href="../generated/maite.protocols.object_detection.Model.html">maite.protocols.object_detection.Model</a>
protocol.</p>
<p>The general steps for wrapping a model are:</p>
<ul class="simple">
<li><p>Understand the source (native) model</p></li>
<li><p>Create a wrapper that makes the source model conform to the MAITE
model protocol (interface)</p></li>
<li><p>Verify that the wrapped model works correctly and has no static type
checking errors</p></li>
</ul>
<section id="load-the-pretrained-faster-r-cnn-and-yolov8-models">
<h2>1 Load the Pretrained Faster R-CNN and YOLOv8 Models<a class="headerlink" href="#load-the-pretrained-faster-r-cnn-and-yolov8-models" title="Permalink to this heading">#</a></h2>
<p>First we load the required Python libraries:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict" title="dataclasses.asdict"><span class="n">asdict</span></a><span class="p">,</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass" title="dataclasses.dataclass"><span class="n">dataclass</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/typing.html#typing.Sequence" title="typing.Sequence"><span class="n">Sequence</span></a>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">PIL.Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">FasterRCNN</span><span class="p">,</span>
    <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span><span class="p">,</span>
    <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.transforms.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">pil_to_tensor</span><span class="p">,</span> <span class="n">to_pil_image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">draw_bounding_boxes</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">maite.protocols.object_detection</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">od</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">maite.protocols</span><span class="w"> </span><span class="kn">import</span> <a class="sphinx-codeautolink-a" href="../generated/maite.protocols.ArrayLike.html#maite.protocols.ArrayLike" title="maite.protocols.ArrayLike"><span class="n">ArrayLike</span></a><span class="p">,</span> <span class="n">ModelMetadata</span>

<span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -iv -v
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Creating new Ultralytics Settings v0.0.6 file ✅
View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'
Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Python</span> <span class="n">implementation</span><span class="p">:</span> <span class="n">CPython</span>
<span class="n">Python</span> <span class="n">version</span>       <span class="p">:</span> <span class="mf">3.9.21</span>
<span class="n">IPython</span> <span class="n">version</span>      <span class="p">:</span> <span class="mf">8.18.1</span>

<span class="n">maite</span>      <span class="p">:</span> <span class="mf">0.7.3</span>
<span class="n">IPython</span>    <span class="p">:</span> <span class="mf">8.18.1</span>
<span class="n">torchvision</span><span class="p">:</span> <span class="mf">0.21.0</span>
<span class="n">PIL</span>        <span class="p">:</span> <span class="mf">11.1.0</span>
<span class="n">torch</span>      <span class="p">:</span> <span class="mf">2.6.0</span>
<span class="n">ultralytics</span><span class="p">:</span> <span class="mf">8.3.77</span>
<span class="n">numpy</span>      <span class="p">:</span> <span class="mf">1.26.4</span>
</pre></div>
</div>
<p>Next we instantiate the (native) Faster R-CNN and YOLOv8 models using
the pretrained weights provided by their respective libraries. The
models were trained on the <a class="reference external" href="https://cocodataset.org/#home">COCO
dataset</a>.</p>
<p>Note that there are additional parameters you could pass into each
model’s initialization function, which can affect its performance.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load R-CNN model</span>
<span class="n">rcnn_weights</span> <span class="o">=</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">rcnn_model</span> <span class="o">=</span> <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">rcnn_weights</span><span class="p">,</span> <span class="n">box_score_thresh</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">rcnn_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># set the RCNN to eval mode (defaults to training)</span>

<span class="c1"># Load YOLOv8 Nano model</span>
<span class="n">yolov8_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">"yolov8n.pt"</span><span class="p">)</span>  <span class="c1"># weights will download automatically</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Downloading</span><span class="p">:</span> <span class="s2">"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth"</span> <span class="n">to</span> <span class="o">/</span><span class="n">root</span><span class="o">/.</span><span class="n">cache</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">hub</span><span class="o">/</span><span class="n">checkpoints</span><span class="o">/</span><span class="n">fasterrcnn_resnet50_fpn_v2_coco</span><span class="o">-</span><span class="n">dd69338a</span><span class="o">.</span><span class="n">pth</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>0%|          | 0.00/167M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>8%|▊         | 14.1M/167M [00:00&lt;00:01, 148MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>20%|█▉        | 33.2M/167M [00:00&lt;00:00, 178MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>30%|███       | 50.2M/167M [00:00&lt;00:00, 176MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>40%|████      | 67.1M/167M [00:00&lt;00:00, 172MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>50%|█████     | 83.6M/167M [00:00&lt;00:00, 163MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>59%|█████▉    | 99.4M/167M [00:00&lt;00:00, 152MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>69%|██████▊   | 115M/167M [00:00&lt;00:00, 155MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>80%|███████▉  | 133M/167M [00:00&lt;00:00, 167MB/s]
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>91%|█████████ | 152M/167M [00:00&lt;00:00, 175MB/s]
</pre></div>
</div>
<p>100%|██████████| 167M/167M [00:01&lt;00:00, 169MB/s]</p>
<pre class="literal-block">Downloading <a class="github reference external" href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt">ultralytics/assets</a> to 'yolov8n.pt'...</pre>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>0%|          | 0.00/6.25M [00:00&lt;?, ?B/s]
</pre></div>
</div>
<p>100%|██████████| 6.25M/6.25M [00:00&lt;00:00, 117MB/s]</p>
</section>
<section id="perform-model-inference-on-sample-images">
<h2>2 Perform Model Inference on Sample Images<a class="headerlink" href="#perform-model-inference-on-sample-images" title="Permalink to this heading">#</a></h2>
<p>Object detection models vary in how they handle input data
preprocessing. For example, the R-CNN model requires manual input
transformations (e.g., resizing and normalization) and conversion to
tensors prior to conducting inference. In contrast, the YOLOv8 model
automatically resizes and normalizes inputs, which can be in a variety
of formats including PIL.</p>
<p>We will first download two sample images.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img_url</span> <span class="o">=</span> <span class="s2">"https://github.com/pytorch/vision/blob/main/test/assets/encode_jpeg/grace_hopper_517x606.jpg?raw=true"</span>
<span class="n">image_data</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">img_url</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">pil_img_1</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">image_data</span><span class="p">))</span>

<span class="n">img_url</span> <span class="o">=</span> <span class="s2">"https://www.ultralytics.com/images/bus.jpg"</span>
<span class="n">image_data</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">img_url</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">pil_img_2</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">image_data</span><span class="p">))</span>
</pre></div>
</div>
<p>Next we preprocess the images for the R-CNN model and run inference:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to PyTorch tensors</span>
<span class="n">tensor_img_1</span> <span class="o">=</span> <span class="n">pil_to_tensor</span><span class="p">(</span><span class="n">pil_img_1</span><span class="p">)</span>
<span class="n">tensor_img_2</span> <span class="o">=</span> <span class="n">pil_to_tensor</span><span class="p">(</span><span class="n">pil_img_2</span><span class="p">)</span>
<span class="n">rcnn_imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor_img_1</span><span class="p">,</span> <span class="n">tensor_img_2</span><span class="p">]</span>

<span class="c1"># Get the inference transforms assocated with these pretrained weights</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">rcnn_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">rcnn_imgs</span><span class="p">]</span>

<span class="n">rcnn_preds</span> <span class="o">=</span> <span class="n">rcnn_model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally we run inference with the YOLO model on the PIL images:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yolo_imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pil_img_1</span><span class="p">,</span> <span class="n">pil_img_2</span><span class="p">]</span>

<span class="n">yolo_preds</span> <span class="o">=</span> <span class="n">yolov8_model</span><span class="p">(</span><span class="n">yolo_imgs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Similar to the input differences, notice the differences in output:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rcnn_pred</span> <span class="o">=</span> <span class="n">rcnn_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">yolo_pred</span> <span class="o">=</span> <span class="n">yolo_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">rcnn_boxes</span> <span class="o">=</span> <span class="n">rcnn_pred</span><span class="p">[</span><span class="s2">"boxes"</span><span class="p">]</span>
<span class="n">yolo_boxes</span> <span class="o">=</span> <span class="n">yolo_pred</span><span class="o">.</span><span class="n">boxes</span>

<a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"""</span>
<span class="s2">R-CNN Outputs</span>
<span class="s2">=============</span>
<span class="s2">Result Type: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">rcnn_pred</span><span class="p">)</span><span class="si">}</span>
<span class="s2">Result Attributes: </span><span class="si">{</span><span class="n">rcnn_pred</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span>
<span class="s2">Box Types: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">rcnn_boxes</span><span class="p">)</span><span class="si">}</span>

<span class="s2">YOLO Outputs</span>
<span class="s2">============</span>
<span class="s2">Result Type: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">yolo_pred</span><span class="p">)</span><span class="si">}</span>
<span class="s2">Result Attributes: </span><span class="si">{</span><span class="n">yolo_pred</span><span class="o">.</span><span class="n">_keys</span><span class="si">}</span>
<span class="s2">Box Types: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">yolo_boxes</span><span class="p">)</span><span class="si">}</span>
<span class="s2">"""</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>R-CNN Outputs
=============
Result Type: &lt;class 'dict'&gt;
Result Attributes: dict_keys(['boxes', 'labels', 'scores'])
Box Types: &lt;class 'torch.Tensor'&gt;

YOLO Outputs
============
Result Type: &lt;class 'ultralytics.engine.results.Results'&gt;
Result Attributes: ('boxes', 'masks', 'probs', 'keypoints', 'obb')
Box Types: &lt;class 'ultralytics.engine.results.Boxes'&gt;
</pre></div>
</div>
<p>The R-CNN model returns a dictionary with certain keys, while the YOLO
model returns a custom Results class. We now proceed to wrap both models
with MAITE to get the benefits of standarizing model inputs and outputs.</p>
</section>
<section id="create-maite-wrappers-for-the-r-cnn-and-yolov8-models">
<h2>3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models<a class="headerlink" href="#create-maite-wrappers-for-the-r-cnn-and-yolov8-models" title="Permalink to this heading">#</a></h2>
<p>We create two separate classes that implement the
<a class="reference external" href="../generated/maite.protocols.object_detection.Model.html">maite.protocols.object_detection.Model</a>
protocol. A MAITE object detection model only needs to have the
following method:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__call__(input_batch:</span> <span class="pre">Sequence[ArrayLike])</span></code> to make model
predictions for inputs in input batch.</p></li>
</ul>
<p>and an attribute:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">metadata</span></code>, which is a typed dictionary containing an <code class="docutils literal notranslate"><span class="pre">id</span></code> field
for the model plus an optional (but highly recommended) map from
class indexes to labels called <code class="docutils literal notranslate"><span class="pre">index2label</span></code>.</p></li>
</ul>
<p>Wrapping the models with MAITE provides a consistent interface for
handling inputs and outputs (e.g., boxes and labels); this
interoperabilty simplifies integration across diverse workflows and
tools, e.g., downstream test &amp; evaluation pipelines.</p>
<p>We begin by creating common input and output types to be used by <em>both</em>
models, as well as an image rendering utility function.</p>
<p>MAITE object detection models must output a type compatible with
<code class="docutils literal notranslate"><span class="pre">maite.protocols.object_detection.ObjectDetectionTarget</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imgs</span><span class="p">:</span> <span class="n">od</span><span class="o">.</span><span class="n">InputBatchType</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor_img_1</span><span class="p">,</span> <span class="n">tensor_img_2</span><span class="p">]</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ObjectDetectionTargetImpl</span><span class="p">:</span>
    <span class="n">boxes</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">scores</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>

<span class="k">def</span><span class="w"> </span><span class="nf">render_wrapped_results</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">model_metadata</span><span class="p">):</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">model_metadata</span><span class="p">[</span><span class="s2">"index2label"</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#zip" title="zip"><span class="nb">zip</span></a><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
        <span class="n">pred_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">names</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">pred</span><span class="o">.</span><span class="n">labels</span><span class="p">]</span>
        <span class="n">box</span> <span class="o">=</span> <span class="n">draw_bounding_boxes</span><span class="p">(</span>
            <span class="n">img</span><span class="p">,</span>
            <span class="n">boxes</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">boxes</span><span class="p">),</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">pred_labels</span><span class="p">,</span>
            <span class="n">colors</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span>
            <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">font</span><span class="o">=</span><span class="s2">"DejaVuSans"</span><span class="p">,</span> <span class="c1"># if necessary, change to TrueType font available on your system</span>
            <span class="n">font_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">to_pil_image</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">size</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">h</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">w</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">display</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</pre></div>
</div>
<section id="wrap-the-r-cnn-model">
<h3>3.1 Wrap the R-CNN Model<a class="headerlink" href="#wrap-the-r-cnn-model" title="Permalink to this heading">#</a></h3>
<p>As mentioned in Section 2, the R-CNN model requires manual preprocessing
of our current input data.</p>
<p>Since the input expected by the native model already conforms to
<code class="docutils literal notranslate"><span class="pre">maite.protocols.object_detection.InputBatchType</span></code> as-is, we can
perform the required preprocessing inside of our wrapper.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">WrappedRCNN</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">FasterRCNN</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span><span class="p">,</span> <span class="nb">id</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/stdtypes.html#str" title="str"><span class="nb">str</span></a><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Add required model metadata attribute</span>
        <span class="n">index2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">category</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#enumerate" title="enumerate"><span class="nb">enumerate</span></a><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">"categories"</span><span class="p">])}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="p">:</span> <span class="n">ModelMetadata</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"id"</span><span class="p">:</span> <span class="nb">id</span><span class="p">,</span>
            <span class="s2">"index2label"</span><span class="p">:</span> <span class="n">index2label</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">od</span><span class="o">.</span><span class="n">InputBatchType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ObjectDetectionTargetImpl</span><span class="p">]:</span>
        <span class="c1"># Get MAITE inputs ready for native model</span>
        <span class="n">preprocess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

        <span class="c1"># Perform inference</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Restructure results to conform to MAITE</span>
        <span class="n">all_detections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">boxes</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"boxes"</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"scores"</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">all_detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">boxes</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">all_detections</span>

<span class="n">wrapped_rcnn_model</span><span class="p">:</span> <span class="n">od</span><span class="o">.</span><span class="n">Model</span> <span class="o">=</span> <span class="n">WrappedRCNN</span><span class="p">(</span><span class="n">rcnn_model</span><span class="p">,</span> <span class="n">rcnn_weights</span><span class="p">,</span> <span class="s2">"TorchVision.FasterRCNN_ResNet50_FPN_V2"</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wrapped_rcnn_preds</span> <span class="o">=</span> <span class="n">wrapped_rcnn_model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
<span class="n">wrapped_rcnn_preds</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">array</span><span class="p">([[</span>     <span class="mf">5.1904</span><span class="p">,</span>       <span class="mf">40.48</span><span class="p">,</span>      <span class="mf">513.56</span><span class="p">,</span>      <span class="mf">601.49</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">215.86</span><span class="p">,</span>      <span class="mf">414.02</span><span class="p">,</span>      <span class="mf">297.24</span><span class="p">,</span>      <span class="mf">482.01</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">array</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">scores</span><span class="o">=</span><span class="n">array</span><span class="p">([</span>    <span class="mf">0.99896</span><span class="p">,</span>     <span class="mf">0.97159</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)),</span>
 <span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">array</span><span class="p">([[</span>     <span class="mf">53.438</span><span class="p">,</span>      <span class="mf">401.78</span><span class="p">,</span>      <span class="mf">235.58</span><span class="p">,</span>      <span class="mf">907.62</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">21.099</span><span class="p">,</span>       <span class="mf">224.5</span><span class="p">,</span>       <span class="mf">802.2</span><span class="p">,</span>      <span class="mf">756.62</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">220.23</span><span class="p">,</span>      <span class="mf">404.51</span><span class="p">,</span>      <span class="mf">347.92</span><span class="p">,</span>      <span class="mf">860.75</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">668.02</span><span class="p">,</span>      <span class="mf">399.47</span><span class="p">,</span>         <span class="mi">810</span><span class="p">,</span>       <span class="mf">881.8</span><span class="p">],</span>
        <span class="p">[</span>    <span class="mf">0.87665</span><span class="p">,</span>      <span class="mf">544.77</span><span class="p">,</span>      <span class="mf">75.116</span><span class="p">,</span>      <span class="mf">878.61</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">scores</span><span class="o">=</span><span class="n">array</span><span class="p">([</span>    <span class="mf">0.99916</span><span class="p">,</span>     <span class="mf">0.99915</span><span class="p">,</span>     <span class="mf">0.99888</span><span class="p">,</span>     <span class="mf">0.99819</span><span class="p">,</span>     <span class="mf">0.98551</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))]</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">render_wrapped_results</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">wrapped_rcnn_preds</span><span class="p">,</span> <span class="n">wrapped_rcnn_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/wrap_object_detection_model_22_0.png" src="../_images/wrap_object_detection_model_22_0.png"/>
<img alt="../_images/wrap_object_detection_model_22_1.png" src="../_images/wrap_object_detection_model_22_1.png"/>
</section>
<section id="wrap-the-yolo-model">
<h3>3.2 Wrap the YOLO Model<a class="headerlink" href="#wrap-the-yolo-model" title="Permalink to this heading">#</a></h3>
<p>We previously passed PIL images to the (native) YOLO model, while the
MAITE-compliant wrapper will be getting inputs of type
<code class="docutils literal notranslate"><span class="pre">maite.protocols.object_detection.InputBatchType</span></code> (which is an alias
for <code class="docutils literal notranslate"><span class="pre">Sequence[ArrayLike]</span></code>).</p>
<p>Note that MAITE requires the dimensions of each image in the input batch
to be <code class="docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code>, which corresponds to the image’s color channels,
height, and width, respectively.</p>
<p>YOLO models, however, expect the input data to be <code class="docutils literal notranslate"><span class="pre">(H,</span> <span class="pre">W,</span> <span class="pre">C)</span></code>, so we
will need to add an additional preprocessing step to tranpose the data.</p>
<p>Furthermore, the underlying YOLO models process different input formats
(e.g., filename, PIL image, NumPy array, PyTorch tensor) differently, so
we cast the underlying input as an <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> for consistency with
how PIL images are handled.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">WrappedYOLO</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">YOLO</span><span class="p">,</span> <span class="nb">id</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/stdtypes.html#str" title="str"><span class="nb">str</span></a><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="c1"># Add required model metadata attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metadata</span><span class="p">:</span> <span class="n">ModelMetadata</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"id"</span><span class="p">:</span> <span class="nb">id</span><span class="p">,</span>
            <span class="s2">"index2label"</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">names</span> <span class="c1"># already a mapping from integer class index to string name</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">od</span><span class="o">.</span><span class="n">InputBatchType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ObjectDetectionTargetImpl</span><span class="p">]:</span>
        <span class="c1"># Get MAITE inputs ready for native model</span>
        <span class="c1"># Bridge/convert input to np.ndarray and tranpose (C, H, W) -&gt; (H, W, C)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

        <span class="c1"># Perform inference</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Restructure results to conform to MAITE</span>
        <span class="n">all_detections</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">detections</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">boxes</span>
            <span class="k">if</span> <span class="n">detections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">detections</span> <span class="o">=</span> <span class="n">detections</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">boxes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">detections</span><span class="o">.</span><span class="n">xyxy</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">detections</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">detections</span><span class="o">.</span><span class="n">conf</span><span class="p">)</span>
            <span class="n">all_detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">boxes</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">scores</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">all_detections</span>

<span class="n">wrapped_yolov8_model</span><span class="p">:</span> <span class="n">od</span><span class="o">.</span><span class="n">Model</span> <span class="o">=</span> <span class="n">WrappedYOLO</span><span class="p">(</span><span class="n">yolov8_model</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"Ultralytics.YOLOv8"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>We can now visualize the wrapped model’s output for both images.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wrapped_yolo_preds</span> <span class="o">=</span> <span class="n">wrapped_yolov8_model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
<span class="n">wrapped_yolo_preds</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">array</span><span class="p">([[</span>     <span class="mf">9.0822</span><span class="p">,</span>      <span class="mf">20.553</span><span class="p">,</span>         <span class="mi">517</span><span class="p">,</span>      <span class="mf">604.41</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">218.77</span><span class="p">,</span>      <span class="mf">414.25</span><span class="p">,</span>      <span class="mf">300.06</span><span class="p">,</span>      <span class="mf">538.04</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">199.25</span><span class="p">,</span>      <span class="mf">414.16</span><span class="p">,</span>      <span class="mf">300.59</span><span class="p">,</span>      <span class="mf">605.61</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">array</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">scores</span><span class="o">=</span><span class="n">array</span><span class="p">([</span>    <span class="mf">0.89559</span><span class="p">,</span>     <span class="mf">0.60285</span><span class="p">,</span>     <span class="mf">0.47824</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)),</span>
 <span class="n">ObjectDetectionTargetImpl</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">array</span><span class="p">([[</span>     <span class="mf">50.009</span><span class="p">,</span>      <span class="mf">397.59</span><span class="p">,</span>      <span class="mf">243.39</span><span class="p">,</span>      <span class="mf">905.03</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">223.38</span><span class="p">,</span>      <span class="mf">405.18</span><span class="p">,</span>      <span class="mf">344.82</span><span class="p">,</span>      <span class="mf">857.35</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">670.62</span><span class="p">,</span>      <span class="mf">378.47</span><span class="p">,</span>      <span class="mf">809.87</span><span class="p">,</span>      <span class="mf">875.35</span><span class="p">],</span>
        <span class="p">[</span>     <span class="mf">29.816</span><span class="p">,</span>      <span class="mf">228.97</span><span class="p">,</span>      <span class="mf">797.11</span><span class="p">,</span>      <span class="mf">751.33</span><span class="p">],</span>
        <span class="p">[</span>    <span class="mf">0.15492</span><span class="p">,</span>       <span class="mf">550.5</span><span class="p">,</span>      <span class="mf">61.616</span><span class="p">,</span>      <span class="mf">870.62</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">scores</span><span class="o">=</span><span class="n">array</span><span class="p">([</span>    <span class="mf">0.87989</span><span class="p">,</span>     <span class="mf">0.87631</span><span class="p">,</span>     <span class="mf">0.86818</span><span class="p">,</span>     <span class="mf">0.84481</span><span class="p">,</span>     <span class="mf">0.44715</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))]</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">render_wrapped_results</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">wrapped_yolo_preds</span><span class="p">,</span> <span class="n">wrapped_yolov8_model</span><span class="o">.</span><span class="n">metadata</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/wrap_object_detection_model_27_0.png" src="../_images/wrap_object_detection_model_27_0.png"/>
<img alt="../_images/wrap_object_detection_model_27_1.png" src="../_images/wrap_object_detection_model_27_1.png"/>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get predictions from both models for first image in batch</span>
<span class="n">wrapped_rcnn_pred</span> <span class="o">=</span> <span class="n">wrapped_rcnn_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">wrapped_yolo_pred</span> <span class="o">=</span> <span class="n">wrapped_yolo_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">wrapped_rcnn_fields</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#vars" title="vars"><span class="nb">vars</span></a><span class="p">(</span><span class="n">wrapped_rcnn_pred</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">wrapped_yolo_fields</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#vars" title="vars"><span class="nb">vars</span></a><span class="p">(</span><span class="n">wrapped_yolo_pred</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

<a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#print" title="print"><span class="nb">print</span></a><span class="p">(</span>
    <span class="sa">f</span><span class="s2">"""</span>
<span class="s2">Wrapped R-CNN Outputs</span>
<span class="s2">=====================</span>
<span class="s2">Result Type: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">wrapped_rcnn_pred</span><span class="p">)</span><span class="si">}</span>
<span class="s2">Result Attributes: </span><span class="si">{</span><span class="n">wrapped_rcnn_fields</span><span class="si">}</span>

<span class="s2">YOLO Outputs</span>
<span class="s2">============</span>
<span class="s2">Result Type: </span><span class="si">{</span><a class="sphinx-codeautolink-a" href="https://docs.python.org/3/library/functions.html#type" title="type"><span class="nb">type</span></a><span class="p">(</span><span class="n">wrapped_yolo_pred</span><span class="p">)</span><span class="si">}</span>
<span class="s2">Result Attributes: </span><span class="si">{</span><span class="n">wrapped_yolo_fields</span><span class="si">}</span>
<span class="s2">"""</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Wrapped R-CNN Outputs
=====================
Result Type: &lt;class '__main__.ObjectDetectionTargetImpl'&gt;
Result Attributes: dict_keys(['boxes', 'labels', 'scores'])

YOLO Outputs
============
Result Type: &lt;class '__main__.ObjectDetectionTargetImpl'&gt;
Result Attributes: dict_keys(['boxes', 'labels', 'scores'])
</pre></div>
</div>
<p>Notice that by wrapping both models to be MAITE-compliant, we were able
to:</p>
<ul class="simple">
<li><p>Use the same input data, <code class="docutils literal notranslate"><span class="pre">imgs</span></code>, for both models.</p></li>
<li><p>Create standardized output, conforming to <code class="docutils literal notranslate"><span class="pre">ObjectDetectionTarget</span></code>,
from both models.</p></li>
<li><p>Render the results of both models using the same function, due to the
standardized inputs and outputs.</p></li>
</ul>
</section>
</section>
<section id="summary">
<h2>4 Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>Wrapping object detection models with MAITE ensures interoperability and
simplifies integration with additional T&amp;E processes. By standardizing
inputs and outputs, you can create consistent workflows that work
seamlessly across models, like Faster R-CNN and YOLOv8.</p>
<p>The key to model wrapping is to define the following:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method that receives a
<code class="docutils literal notranslate"><span class="pre">maite.protocols.object_detection.InputBatchType</span></code> as input and
returns <code class="docutils literal notranslate"><span class="pre">maite.protocols.object_detection.TargetBatchType</span></code>.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">metadata</span></code> attribute that’s a typed dictionary with at least an
“id field.</p></li>
</ul>
</section>
</section>
</article>
<footer class="bd-footer-article">
<div class="footer-article-items footer-article__inner">
<div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="wrap_object_detection_dataset.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Wrap an Object Detection Dataset</p>
</div>
</a>
<a class="right-next" href="../explanation.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Explanation</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div></div>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-pretrained-faster-r-cnn-and-yolov8-models">1 Load the Pretrained Faster R-CNN and YOLOv8 Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-model-inference-on-sample-images">2 Perform Model Inference on Sample Images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-maite-wrappers-for-the-r-cnn-and-yolov8-models">3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrap-the-r-cnn-model">3.1 Wrap the R-CNN Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrap-the-yolo-model">3.2 Wrap the YOLO Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4 Summary</a></li>
</ul>
</nav></div>
<div class="sidebar-secondary-item">
<div class="tocsection sourcelink">
<a href="../_sources/how_to/wrap_object_detection_model.rst.txt">
<i class="fa-solid fa-file-lines"></i> Show Source
    </a>
</div>
</div>
</div></div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2024 Massachusetts Institute of Technology.
      <br/>
</p>
</div>
<div class="footer-item">
<p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
    <br/>
</p>
</div>
</div>
<div class="footer-items__end">
<div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
</div>
</div>
</footer>
</body>
</html>