{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2023, MASSACHUSETTS INSTITUTE OF TECHNOLOGY<br/>\n",
    "> Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).<br/>\n",
    "> SPDX-License-Identifier: MIT\n",
    "\n",
    "# Run a Basic Evaluation\n",
    "\n",
    "The MAITE library provides APIs for datasets, models, metrics, and evaluation to make their use more consistent across test and evaluation (T&E) tools.\n",
    "\n",
    "In this tutorial, you will use methods from MAITE to:\n",
    "\n",
    "* List datasets that are available from different providers, and load the CIFAR-10 test set from [HuggingFace](https://huggingface.co/datasets),\n",
    "* List models that are available from different providers, and load a model from [HuggingFace](https://huggingface.co/models) that has been pretrained on CIFAR-10,\n",
    "* List metrics that are available from different providers, and load an accuracy metric from [TorchMetrics](https://github.com/Lightning-AI/torchmetrics), and\n",
    "* Run an evaluation to compute the accuracy of the loaded model on the CIFAR-10 test set.\n",
    "\n",
    "Once complete, you will have a basic understanding of MAITE’s APIs for loading datasets, models, and metrics from various external libraries, and how to use MAITE’s native API for running evaluations.\n",
    "\n",
    "This tutorial does not assume any prior knowledge, but some experience with Python, machine learning, and the PyTorch framework may be helpful.\n",
    "\n",
    "Note: This tutorial can be found as a Jupyter notebook [here](https://github.com/mit-ll-ai-technology/maite/examples/basic_evaluation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "This tutorial requires PyTorch to be installed in your Python environment. If you don’t already have it installed, follow the instructions on the PyTorch website [here](https://pytorch.org/get-started/locally/) to install the version that matches your OS, compute platform, and preferred package manager.\n",
    "\n",
    "To install MAITE in your Python environment, run the following command in your terminal:\n",
    "\n",
    "```python\n",
    "pip install maite[all_interop]\n",
    "```\n",
    "\n",
    "``all_interop`` ensures you have the necessary packages to interoperate with external providers such as TorchVision, HuggingFace, and TorchMetrics. This will allow you to access datasets, models, and metrics from these providers, which are needed for this tutorial.\n",
    "\n",
    "Alternatively, you can also install MAITE by cloning the git repository and installing locally:\n",
    "```python\n",
    "git clone git@gitlab.jatic.net:jatic/cdao/maite.git\n",
    "cd maite\n",
    "pip install .[all_interop]\n",
    "```\n",
    "\n",
    "To verify that MAITE is installed as expected, open a Python console and try importing:\n",
    "```\n",
    ">>> import maite\n",
    "```\n",
    "We will now use MAITE to list and load a dataset, model, and metrics, and then use those to run an evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Loading Datasets\n",
    "The MAITE library provides APIs for listing and loading datasets from multiple providers, including [TorchVision](https://pytorch.org/vision/stable/datasets.html) and [HuggingFace](https://huggingface.co/datasets).\n",
    "\n",
    "### List datasets\n",
    "First, we’ll import the ``list_datasets`` method, and then use it to list the first 20 datasets that are available from TorchVision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'CLEVRClassification',\n",
       " 'CREStereo',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CarlaStereo',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'Country211',\n",
       " 'DTD',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'ETH3DStereo',\n",
       " 'EuroSAT',\n",
       " 'FER2013',\n",
       " 'FGVCAircraft',\n",
       " 'FakeData']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maite import list_datasets\n",
    "\n",
    "list_datasets(provider=\"torchvision\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also count the number of datasets available from TorchVision, and compare it to the number of datasets available from another provider, HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_datasets(provider=\"torchvision\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_datasets(provider=\"huggingface\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your numbers may differ slightly, as both providers are continuing to add new datasets. We can also further filter the datasets from HuggingFace, such as only considering the datasets for the task of image classification, and including datasets provided by the community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_datasets(\n",
    "    provider=\"huggingface\",\n",
    "    task_categories=[\"image-classification\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(list_datasets(\n",
    "    provider=\"huggingface\",\n",
    "    task_categories=[\"image-classification\"],\n",
    "    with_community_datasets=True\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that including community datasets provides us with a much larger number of potential datasets to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset\n",
    "We’ll use the CIFAR-10 dataset for this tutorial, due to its moderate size and the availability of pretrained models.\n",
    "\n",
    "To load the test set from the HuggingFace version of CIFAR-10, we’ll use MAITE’s ``load_dataset`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    provider=\"huggingface\",\n",
    "    dataset_name=\"cifar10\",\n",
    "    task=\"image-classification\",\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the first sample from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x2ABB8DF0D90>, 'label': 3}\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data sample is in the form of a dictionary, with keys for ``image`` and ``label``. The ``image`` is currently in the form of a PIL image. For this tutorial, we’ll be loading a model from the [PyTorch](https://pytorch.org/) framework, so we’ll need to convert the images in our dataset to [Tensors](https://pytorch.org/docs/stable/tensors.html). To do this, we’ll leverage TorchVision’s ``to_tensor`` method, with a resize to match our eventual model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_tensor\n",
    "dataset.set_transform(\n",
    "    lambda x: {\n",
    "        \"image\": to_tensor(x[\"image\"].resize((224,224))),\n",
    "        \"label\": x[\"label\"]\n",
    "    }\n",
    ")\n",
    "data = dataset[0]\n",
    "print(data[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your dataset is configured, it’s time to select and load a model to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Loading Models\n",
    "The MAITE library  provides APIs for listing and loading models and pretrained weights from multiple providers, including [TorchVision](https://pytorch.org/vision/stable/models.html) and [HuggingFace](https://huggingface.co/models).\n",
    "\n",
    "### Listing Models\n",
    "We’ll start by using the ``list_models`` method to explore the models that are available from TorchVision and HuggingFace for image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maite import list_models\n",
    "\n",
    "len(list_models(provider=\"torchvision\", task=\"image-classification\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8168"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_models(provider=\"huggingface\", task=\"image-classification\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be testing on the CIFAR-10 dataset, we’d like to load a model that we know has been pretrained on CIFAR-10. We can find these candidate models from HuggingFace by searching for models that contain “cifar10” in their ``model_name``. Let’s look at the first 20 models in this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02shanky/vit-finetuned-cifar10',\n",
       " '02shanky/vit-finetuned-vanilla-cifar10-0',\n",
       " 'Ahmed9275/Vit-Cifar100',\n",
       " 'DeepCyber/Enhanced-CIFAR10-CNN',\n",
       " 'JamesCS462/JamesCS462_cifar100',\n",
       " 'LaCarnevali/vit-cifar10',\n",
       " 'MazenAmria/swin-base-finetuned-cifar100',\n",
       " 'MazenAmria/swin-small-finetuned-cifar100',\n",
       " 'MazenAmria/swin-tiny-finetuned-cifar100',\n",
       " 'NouRed/fine-tuned-vit-cifar10',\n",
       " 'SajjadAlam/beit_Cifar10_finetune_model',\n",
       " 'Sendeky/Cifar10',\n",
       " 'Skafu/swin-tiny-patch4-window7-224-cifar10',\n",
       " 'Skafu/swin-tiny-patch4-window7-224-finetuned-eurosat-finetuned-cifar100',\n",
       " 'TirathP/cifar10-lt',\n",
       " 'Weili/resnet-18-cifar100',\n",
       " 'Weili/swin-base-patch4-window7-224-in22k-finetuned-cifar10',\n",
       " 'Weili/swin-tiny-patch4-window7-224-finetuned-cifar10',\n",
       " 'Weili/vit-base-patch16-224-finetuned-cifar10',\n",
       " 'aaraki/vit-base-patch16-224-in21k-finetuned-cifar10']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = list_models(\n",
    "    provider=\"huggingface\",\n",
    "    task=\"image-classification\",\n",
    "    model_name=\"cifar10\"\n",
    ")\n",
    "sorted([m.id for m in models])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your list may look slightly different, since new models are constantly being added to the HuggingFace hub by users in the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "Next, let’s load one of those HuggingFace models using MAITE’s ``load_model`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite import load_model\n",
    "\n",
    "model = load_model(\n",
    "    provider=\"huggingface\",\n",
    "    model_name=\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\",\n",
    "    task=\"image-classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you can pass an input from your dataset through this model to get a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "output = model(data[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compare the model’s prediction (i.e., output with highest probability) to truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(output.probs.argmax(dim=1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for this model, the prediction matches truth! However, if you loaded a different model (or a different dataset), you may end up with a different outcome.\n",
    "\n",
    "Now that we’ve verified we can run a single input through our model to get an output, let’s load a metric that we’ll use to compute performance of the model across the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Loading Metrics\n",
    "The MAITE library also provides APIs for listing and loading metrics from common providers, including [TorchMetrics](https://github.com/Lightning-AI/torchmetrics) and [TorchEval](https://github.com/pytorch/torcheval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Metrics\n",
    "We’ll start by using the ``list_metrics`` method to compare the number of metrics from each provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maite import list_metrics\n",
    "\n",
    "len(list_metrics(provider=\"torchmetrics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_metrics(provider=\"torcheval\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s list the first 20 metrics from each provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy',\n",
       " 'AUROC',\n",
       " 'AveragePrecision',\n",
       " 'BLEUScore',\n",
       " 'CalibrationError',\n",
       " 'CatMetric',\n",
       " 'CharErrorRate',\n",
       " 'CHRFScore',\n",
       " 'ConcordanceCorrCoef',\n",
       " 'CohenKappa',\n",
       " 'ConfusionMatrix',\n",
       " 'CosineSimilarity',\n",
       " 'CramersV',\n",
       " 'Dice',\n",
       " 'TweedieDevianceScore',\n",
       " 'ErrorRelativeGlobalDimensionlessSynthesis',\n",
       " 'ExactMatch',\n",
       " 'ExplainedVariance',\n",
       " 'ExtendedEditDistance',\n",
       " 'F1Score']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metrics(provider=\"torchmetrics\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUC',\n",
       " 'BinaryAccuracy',\n",
       " 'BinaryAUPRC',\n",
       " 'BinaryAUROC',\n",
       " 'BinaryBinnedAUPRC',\n",
       " 'BinaryBinnedAUROC',\n",
       " 'BinaryBinnedPrecisionRecallCurve',\n",
       " 'BinaryConfusionMatrix',\n",
       " 'BinaryF1Score',\n",
       " 'BinaryNormalizedEntropy',\n",
       " 'BinaryPrecision',\n",
       " 'BinaryPrecisionRecallCurve',\n",
       " 'BinaryRecall',\n",
       " 'BinaryRecallAtFixedPrecision',\n",
       " 'BLEUScore',\n",
       " 'Cat',\n",
       " 'ClickThroughRate',\n",
       " 'FrechetInceptionDistance',\n",
       " 'HitRate',\n",
       " 'Max']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metrics(provider=\"torcheval\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each provider has similar metrics, but often with different names (e.g., AUROC vs. AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metrics\n",
    "For this tutorial, we’ll evaluate the performance of our model using a common metric: accuracy. We’ll use the MAITE ``load_metric`` method to load and configure the accuracy metric from TorchMetrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite import load_metric\n",
    "\n",
    "metric = load_metric(\n",
    "    provider=\"torchmetrics\",\n",
    "    metric_name=\"Accuracy\",\n",
    "    task=\"multiclass\",\n",
    "    num_classes=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to run a full evaluation using your dataset, model, and metric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an Evaluation\n",
    "First, instantiate an evaluator using the MAITE ``evaluate`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite import evaluate\n",
    "\n",
    "evaluator = evaluate(task=\"image-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the evaluator to compute metrics using a subset of your dataset and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subset = [dataset[i] for i in range(256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f333b1e4147245978c56920700d1586b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = evaluator(\n",
    "    model,\n",
    "    dataset_subset,\n",
    "    metric=dict(accuracy=metric),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, print your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': tensor(0.9531)}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have now successfully used MAITE to load a dataset, model, and metric from external providers, and run an evaluation to compute the accuracy of the loaded model on the CIFAR-10 test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "issue_278",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
