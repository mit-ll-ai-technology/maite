{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of MAITE Protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAITE provides protocols for the following AI components:\n",
    "\n",
    "* models\n",
    "* datasets\n",
    "* dataloaders\n",
    "* augmentations\n",
    "* metrics\n",
    "\n",
    " MAITE protocols specify expected interfaces of these components (i.e, a minimal set of required attributes, methods, and method type signatures) to promote interoperability in test and evaluation (T&E). This enables the creation of higher-level workflows (e.g., an `evaluate` utility) that can interact with any components that conform to the protocols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Concept: Bridging ArrayLikes\n",
    "\n",
    "MAITE defines a protocol called `ArrayLike` (inspired by NumPy's [interoperability approach](https://numpy.org/devdocs/user/basics.interoperability.html)) that helps components that natively use different flavors of tensors (e.g., NumPy ndarray, PyTorch Tensor, JAX ndarray) work together.\n",
    "\n",
    "In this example, the functions \"type narrow\" from `ArrayLike` to the type they want to work with internally. Note that this doesn't necessarily require a conversion depending on the actual input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from maite.protocols import ArrayLike\n",
    "\n",
    "def my_numpy_fn(x: ArrayLike) -> np.ndarray:\n",
    "    arr = np.asarray(x)\n",
    "    # ...\n",
    "    return arr\n",
    "\n",
    "def my_torch_fn(x: ArrayLike) -> torch.Tensor:\n",
    "    tensor = torch.as_tensor(x)\n",
    "    # ...\n",
    "    return tensor\n",
    "\n",
    "# can apply NumPy function to PyTorch Tensor\n",
    "np_out = my_numpy_fn(torch.rand(2, 3))\n",
    "\n",
    "# can apply PyTorch function to NumPy array\n",
    "torch_out = my_torch_fn(np.random.rand(2, 3))\n",
    "\n",
    "# note: no performance hit from conversion when all `ArrayLike`s are from same library\n",
    "# or when can share the same underlying memory\n",
    "torch_out = my_torch_fn(torch.rand(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using bridging, we MAITE can permit implementers of the protocol to internally interact with their own types while exposing a more open interface to other MAITE-compliant components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Types\n",
    "\n",
    "MAITE represents an *individual* data item as a tuple of:\n",
    "\n",
    "* input (i.e., image),\n",
    "* target (i.e., label), and\n",
    "* metadata (at the datum level)\n",
    "\n",
    "and a *batch* of data items as a tuple of:\n",
    "\n",
    "* input batches,\n",
    "* target batches, and\n",
    "* metadata batches.\n",
    "\n",
    "MAITE provides versions of `Model`, `Dataset`, `DataLoader`, `Augmentation`, and `Metric` protocols that correspond to different machine learning tasks (e.g. image classification, object detection) by parameterizing protocol interfaces on the particular input, target, and metadata types associated with that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Image Classification\n",
    "\n",
    "For image classification with `Cl` image classes, we have:\n",
    "\n",
    "```python\n",
    "InputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\n",
    "TargetType: TypeAlias = ArrayLike  # shape-(Cl) tensor of one-hot encoded true class or predicted probabilities\n",
    "DatumMetadataType: TypeAlias = Dict[str, Any]\n",
    "\n",
    "InputBatchType: TypeAlias = Sequence[ArrayLike]  # element shape-(C, H, W) tensor of N images\n",
    "TargetBatchType: TypeAlias = Sequence[ArrayLike]  # element shape-(Cl,)\n",
    "DatumMetadataBatchType: TypeAlias = Sequence[DatumMetadataType]\n",
    "```\n",
    "\n",
    "Notes:\n",
    "* `TargetType` is used for both ground truth (coming from a dataset) and predictions (output from a model). So for a problem with 4 classes,\n",
    "  * true label of class 2 would be one-hot encoded as `[0, 0, 1, 0]`\n",
    "  * prediction from a model would be a vector of pseudo-probabilities, e.g., `[0.1, 0.0, 0.7, 0.2]`\n",
    "* `InputType` and `InputBatchType` are shown with shapes following PyTorch channels-first convention\n",
    "\n",
    "These type aliases along with the versions of the various component protocols that use these types can be imported from `maite.protocols.image_classification` (if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import protocol classes\n",
    "from maite.protocols.image_classification import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    Model,\n",
    "    Augmentation,\n",
    "    Metric\n",
    ")\n",
    "\n",
    "# import type aliases\n",
    "from maite.protocols.image_classification import (\n",
    "    InputType,\n",
    "    TargetType,\n",
    "    DatumMetadataType,\n",
    "    InputBatchType,\n",
    "    TargetBatchType,\n",
    "    DatumMetadataBatchType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, image classification components and types can be accessed via the module directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maite.protocols.image_classification as ic\n",
    "\n",
    "# model: ic.Model = load_model(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Object Detection\n",
    "\n",
    "For object detection with `D_i` detections in an image `i`, we have:\n",
    "\n",
    "```python\n",
    "class ObjectDetectionTarget(Protocol):\n",
    "    @property \n",
    "    def boxes(self) -> ArrayLike: ...  # shape-(D_i, 4) tensor of bounding boxes w/format X0, Y0, X1, Y1\n",
    "\n",
    "    @property\n",
    "    def labels(self) -> ArrayLike: ... # shape-(D_i) tensor of labels for each box\n",
    "\n",
    "    @property\n",
    "    def scores(self) -> ArrayLike: ... # shape-(D_i) tensor of scores for each box (e.g., probabilities)\n",
    "\n",
    "InputType: TypeAlias = ArrayLike  # shape-(C, H, W) tensor with single image\n",
    "TargetType: TypeAlias = ObjectDetectionTarget\n",
    "DatumMetadataType: TypeAlias = Dict[str, Any]\n",
    "\n",
    "InputBatchType: TypeAlias = Sequence[ArrayLike]  # sequence of N ArrayLikes each of shape (C, H, W)\n",
    "TargetBatchType: TypeAlias = Sequence[TargetType]  # sequence of object detection \"target\" objects\n",
    "DatumMetadataBatchType: TypeAlias = Sequence[DatumMetadataType]\n",
    "```\n",
    "\n",
    "Notes:\n",
    "* `ObjectDetectionTarget` contains a single label and score per box\n",
    "* `InputType` and `InputBatchType` are shown with shapes following PyTorch channels-first convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Models\n",
    "\n",
    "All models implement a `__call__` method that takes the `InputBatchType` and produces the `TargetBatchType` appropriate for the given machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A model protocol for the image classification ML subproblem.\n",
      "\n",
      "    Implementers must provide a `__call__` method that operates on a batch of model\n",
      "    inputs (as `Sequence[ArrayLike]) and returns a batch of model targets (as\n",
      "    `Sequence[ArrayLike]`)\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __call__(input_batch: Sequence[ArrayLike]) -> Sequence[ArrayLike]\n",
      "        Make a model prediction for inputs in input batch. Input batch is expected to\n",
      "        be `Sequence[ArrayLike]` with each element of shape `(C, H, W)`.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import maite.protocols.image_classification as ic\n",
    "print(ic.Model.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A model protocol for the object detection ML subproblem.\n",
      "\n",
      "    Implementers must provide a `__call__` method that operates on a batch of model inputs\n",
      "    (as `Sequence[ArrayLike]`s) and returns a batch of model targets (as\n",
      "    `Sequence[ObjectDetectionTarget]`)\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __call__(input_batch: Sequence[ArrayLike]) -> Sequence[ObjectDetectionTarget]\n",
      "        Make a model prediction for inputs in input batch. Elements of input batch\n",
      "        are expected in the shape `(C, H, W)`.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import maite.protocols.object_detection as od\n",
    "print(od.Model.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Datasets and DataLoaders\n",
    "\n",
    "`Dataset`s provide access to single data items and `DataLoader`s  provide access to batches of data with the input, target, and metadata types corresponding to the given machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A dataset protocol for image classification ML subproblem providing datum-level\n",
      "    data access.\n",
      "\n",
      "    Implementers must provide index lookup (via `__getitem__(ind: int)` method) and\n",
      "    support `len` (via `__len__()` method). Data elements looked up this way correspond\n",
      "    to individual examples (as opposed to batches).\n",
      "\n",
      "    Indexing into or iterating over the an image_classification dataset returns a\n",
      "    `Tuple` of types `ArrayLike`, `ArrayLike`, and `Dict[str,Any]`.\n",
      "    These correspond to the model input type, model target type, and datum-level\n",
      "    metadata, respectively.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __getitem__(ind: int) -> Tuple[ArrayLike, ArrayLike, Dict[str, Any]]\n",
      "        Provide map-style access to dataset elements. Returned tuple elements\n",
      "        correspond to model input type, model target type, and datum-specific metadata type,\n",
      "        respectively.\n",
      "\n",
      "    __len__() -> int\n",
      "        Return the number of data elements in the dataset.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "\n",
      "    We create a dummy set of data and use it to create a class that implements\n",
      "    this lightweight dataset protocol:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> from typing import List, Dict, Any, Tuple\n",
      "    >>> from maite.protocols import ArrayLike\n",
      "\n",
      "    Assume we have 5 classes, 10 datapoints, and 10 target labels, and that we want\n",
      "    to simply have an integer 'id' field in each datapoint's metadata:\n",
      "\n",
      "    >>> N_CLASSES: int = 5\n",
      "    >>> N_DATUM: int = 10\n",
      "    >>> images: List[np.ndarray] = [np.random.rand(3, 32, 16) for _ in range(N_DATUM)]\n",
      "    >>> targets: np.ndarray = np.eye(N_CLASSES)[np.random.choice(N_CLASSES, N_DATUM)]\n",
      "    >>> metadata: List[Dict] = [{'id': i} for i in range(N_DATUM)]\n",
      "\n",
      "    Constructing a compliant dataset just involves a simple wrapper that fetches\n",
      "    individual datapoints, where a datapoint is a single image, target, metadata 3-tuple.\n",
      "\n",
      "    >>> class ImageDataset:\n",
      "    ...     def __init__(self,\n",
      "    ...                  images: List[np.ndarray],\n",
      "    ...                  targets: np.ndarray,\n",
      "    ...                  metadata: List[Dict[str, Any]]):\n",
      "    ...         self.images = images\n",
      "    ...         self.targets = targets\n",
      "    ...         self.metadata = metadata\n",
      "    ...     def __len__(self) -> int:\n",
      "    ...         return len(images)\n",
      "    ...     def __getitem__(self, ind: int) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:\n",
      "    ...         return self.images[ind], self.targets[ind], self.metadata[ind]\n",
      "\n",
      "    We can instantiate this class and typehint it as an image_classification.Dataset.\n",
      "    By using typehinting, we permit a static typechecker to verify protocol compliance.\n",
      "\n",
      "    >>> from maite.protocols import image_classification as ic\n",
      "    >>> dataset: ic.Dataset = ImageDataset(images, targets, metadata)\n",
      "\n",
      "    Note that when writing a Dataset implementer, return types may be narrower than the\n",
      "    return types promised by the protocol (np.ndarray is a subtype of ArrayLike), but\n",
      "    the argument types must be at least as general as the argument types promised by the\n",
      "    protocol.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ic.Dataset.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A dataloader protocol for the image classification ML subproblem providing\n",
      "    batch-level data access.\n",
      "\n",
      "    Implementers must provide an iterable object (returning an iterator via the\n",
      "    `__iter__` method) that yields tuples containing batches of data. These tuples\n",
      "    contain types `Sequence[ArrayLike]` (elements of shape `(C, H, W)`),\n",
      "    `Sequence[ArrayLike]` (elements shape `(Cl, )`), and `Sequence[Dict[str, Any]]`,\n",
      "    which correspond to model input batch, model target type batch, and a datum metadata batch.\n",
      "\n",
      "    Note: Unlike Dataset, this protocol does not require indexing support, only iterating.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __iter__ -> Iterator[tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[Dict[str, Any]]]]\n",
      "        Return an iterator over batches of data, where each batch contains a tuple of\n",
      "        of model input batch (as `Sequence[ArrayLike]`), model target batch (as\n",
      "        `Sequence[ArrayLike]`), and batched datum-level metadata\n",
      "        (as `Sequence[Dict[str,Any]]`), respectively.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ic.DataLoader.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A dataloader protocol for the object detection ML subproblem providing\n",
      "    batch-level data access.\n",
      "\n",
      "    Implementers must provide an iterable object (returning an iterator via the\n",
      "    `__iter__` method) that yields tuples containing batches of data. These tuples\n",
      "    contain types `Sequence[ArrayLike]` (elements of shape `(C, H, W)`),\n",
      "    `Sequence[ObjectDetectionTarget]`, and `Sequence[Dict[str, Any]]`,\n",
      "    which correspond to model input batch, model target type batch, and a datum metadata batch.\n",
      "\n",
      "    Note: Unlike Dataset, this protocol does not require indexing support, only iterating.\n",
      "\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __iter__ -> Iterator[tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[Dict[str, Any]]]]\n",
      "        Return an iterator over batches of data, where each batch contains a tuple of\n",
      "        of model input batch (as `Sequence[ArrayLike]`), model target batch (as\n",
      "        `Sequence[ObjectDetectionTarget]`), and batched datum-level metadata\n",
      "        (as `Sequence[Dict[str,Any]]`), respectively.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(od.DataLoader.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Augmentations\n",
    "\n",
    "`Augmentation`s take in and return a batch of data with the `InputBatchType`, `TargetBatchType`, and `DatumMetadataBatchType` types corresponding to the given machine learning task.\n",
    "\n",
    "Augmentations can access the datum-level metadata associated with each data item to potentially tailor the augmentation to individual items. Augmentations can also associate new datum-level metadata with each data item, e.g., documenting aspects of the actual change that was applied (e.g., the actual rotation angle sampled from a range of possible angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    An augmentation protocol for the image classification subproblem.\n",
      "\n",
      "    An augmentation is expected to take a batch of data and return a modified version of\n",
      "    that batch. Implementers must provide a single method that takes and returns a\n",
      "    labeled data batch, where a labeled data batch is represented by a tuple of types\n",
      "    `Sequence[ArrayLike]` (with elements of shape `(C, H, W)`), `Sequence[ArrayLike]`\n",
      "    (with elements of shape `(Cl, )`), and `Sequence[Dict[str,Any]]`. These correspond\n",
      "    to the model input batch type, model target batch type, and datum-level metadata\n",
      "    batch type, respectively.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __call__(datum: Tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[dict[str, Any]]]) ->          Tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[dict[str, Any]]])\n",
      "        Return a modified version of original data batch. A data batch is represented\n",
      "        by a tuple of model input batch (as `Sequence[ArrayLike]` with elements of shape\n",
      "        `(C, H, W)`), model target batch (as an `Sequence[ArrayLike]` of shape `(N, Cl)`),\n",
      "        and batch metadata (as `Sequence[Dict[str, Any]]`), respectively.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "\n",
      "    We can write an implementer of the augmentation class as either a function or a class.\n",
      "    The only requirement is that the object provide a __call__ method that takes objects\n",
      "    at least as general as the types promised in the protocol signature and return types\n",
      "    at least as specific.\n",
      "\n",
      "    >>> import copy\n",
      "    >>> import numpy as np\n",
      "    >>> from typing import Dict, Any, Tuple, Sequence\n",
      "    >>> from maite.protocols import ArrayLike\n",
      "    >>>\n",
      "    >>> class ImageAugmentation:\n",
      "    ...     def __call__(\n",
      "    ...         self,\n",
      "    ...         data_batch: Tuple[Sequence[ArrayLike], Sequence[ArrayLike], Sequence[Dict[str, Any]]]\n",
      "    ...     ) -> Tuple[Sequence[np.ndarray], Sequence[np.ndarray], Sequence[Dict[str, Any]]]:\n",
      "    ...         inputs, targets, mds = data_batch\n",
      "    ...         # We copy data passed into the constructor to avoid mutating original inputs\n",
      "    ...         # By using np.ndarray constructor, the static type-checker will let us treat\n",
      "    ...         # generic ArrayLike as a more narrow return type\n",
      "    ...         inputs_aug = [copy.copy(np.array(input)) for input in inputs]\n",
      "    ...         targets_aug = [copy.copy(np.array(target)) for target in targets]\n",
      "    ...         mds_aug = copy.deepcopy(mds)  # deepcopy in case of nested structure\n",
      "    ...         # Modify inputs_aug, targets_aug, or mds_aug as needed\n",
      "    ...         # In this example, we just add a new metadata field\n",
      "    ...         for i, md in enumerate(mds_aug):\n",
      "    ...             md['new_key'] = i\n",
      "    ...         return inputs_aug, targets_aug, mds_aug\n",
      "\n",
      "    We can typehint an instance of the above class as an Augmentation in the\n",
      "    image_classification domain:\n",
      "\n",
      "    >>> from maite.protocols import image_classification as ic\n",
      "    >>> im_aug: ic.Augmentation = ImageAugmentation()\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ic.Augmentation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    An augmentation protocol for the object detection subproblem.\n",
      "\n",
      "    An augmentation is expected to take a batch of data and return a modified version of\n",
      "    that batch. Implementers must provide a single method that takes and returns a\n",
      "    labeled data batch, where a labeled data batch is represented by a tuple of types\n",
      "    `Sequence[ArrayLike]`, `Sequence[ObjectDetectionTarget]`, and `Sequence[Dict[str,Any]]`.\n",
      "    These correspond to the model input batch type, model target batch type, and datum-level\n",
      "    metadata batch type, respectively.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    __call__(datum: Tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[dict[str, Any]]]) ->          Tuple[Sequence[ArrayLike], Sequence[ObjectDetectionTarget], Sequence[dict[str, Any]]]\n",
      "        Return a modified version of original data batch. A data batch is represented\n",
      "        by a tuple of model input batch (as `Sequence ArrayLike` with elements of shape\n",
      "        `(C, H, W)`), model target batch (as `Sequence[ObjectDetectionTarget]`), and\n",
      "        batch metadata (as `Sequence[Dict[str,Any]]`), respectively.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(od.Augmentation.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Metrics\n",
    "\n",
    "The `Metric` protocol is inspired by the design of existing libraries like Torchmetrics and Torcheval. The `update` method operates on batches of predictions and truth labels by either caching them for later computation of the metric (via `compute`) or updating sufficient statistics in an online fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A metric protocol for the image classification ML subproblem.\n",
      "\n",
      "    A metric in this sense is expected to measure the level of agreement between model\n",
      "    predictions and ground-truth labels.\n",
      "\n",
      "    Methods\n",
      "    -------\n",
      "\n",
      "    update(preds: Sequence[ArrayLike], targets: Sequence[ArrayLike]) -> None\n",
      "        Add predictions and targets to metric's cache for later calculation. Both\n",
      "        preds and targets are expected to be sequences with elements of shape `(Cl,)`.\n",
      "\n",
      "    compute() -> Dict[str, Any]\n",
      "        Compute metric value(s) for currently cached predictions and targets, returned as\n",
      "        a dictionary.\n",
      "\n",
      "    reset() -> None\n",
      "        Clear contents of current metric's cache of predictions and targets.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ic.Metric.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A metric protocol for the object detection ML subproblem.\n",
      "\n",
      "     A metric in this sense is expected to measure the level of agreement between model\n",
      "     predictions and ground-truth labels.\n",
      "\n",
      "     Methods\n",
      "     -------\n",
      "\n",
      "     update(preds: Sequence[ObjectDetectionTarget], targets: Sequence[ObjectDetectionTarget]) -> None\n",
      "         Add predictions and targets to metric's cache for later calculation.\n",
      "\n",
      "     compute() -> Dict[str, Any]\n",
      "         Compute metric value(s) for currently cached predictions and targets, returned as\n",
      "         a dictionary.\n",
      "\n",
      "     reset() -> None\n",
      "         Clear contents of current metric's cache of predictions and targets.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(od.Metric.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Workflows\n",
    "\n",
    "MAITE provides high-level utilities for common workflows such as `evaluate` and `predict`. They can be called with either `Dataset`s or `DataLoader`s, and with optional `Augmentation`.\n",
    "\n",
    "The `evaluate` function can optionally return the model predictions and (potentially-augmented) data batches used during inference.\n",
    "\n",
    "The `predict` function returns the model predictions and (potentially-augmented) data batches used during inference, essentially calling `evaluate` with a dummy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite.workflows import evaluate, predict\n",
    "# we can also import from object_detection module\n",
    "# where the function call signature is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Evaluate a model's performance on data according to some metric with optional augmentation.\n",
      "\n",
      "    Some data source (either a dataloader or a dataset) must be provided\n",
      "    or an InvalidArgument exception is raised.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    model : SomeModel\n",
      "        Maite Model object.\n",
      "\n",
      "    metric : Optional[SomeMetric], (default=None)\n",
      "        Compatible maite Metric.\n",
      "\n",
      "    dataloader : Optional[SomeDataloader], (default=None)\n",
      "        Compatible maite dataloader.\n",
      "\n",
      "    dataset : Optional[SomeDataset], (default=None)\n",
      "        Compatible maite dataset.\n",
      "\n",
      "    batch_size : int, (default=1)\n",
      "        Batch size for use with dataset (ignored if dataset=None).\n",
      "\n",
      "    augmentation : Optional[SomeAugmentation], (default=None)\n",
      "        Compatible maite augmentation.\n",
      "\n",
      "    return_augmented_data : bool, (default=False)\n",
      "        Set to True to return post-augmentation data as a function output.\n",
      "\n",
      "    return_preds : bool, (default=False)\n",
      "        Set to True to return raw predictions as a function output.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Tuple[Dict[str, Any], Sequence[TargetType], Sequence[Tuple[InputBatchType, TargetBatchType, DatumMetadataBatchType]]]\n",
      "        Tuple of returned metric value, sequence of model predictions, and\n",
      "        sequence of data batch tuples fed to the model during inference. The actual\n",
      "        types represented by InputBatchType, TargetBatchType, and DatumMetadataBatchType will vary\n",
      "        by the domain of the components provided as input arguments (e.g. image\n",
      "        classification or object detection.)\n",
      "        Note that the second and third return arguments will be empty if\n",
      "        return_augmented_data is False or return_preds is False, respectively.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(evaluate.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Make predictions for a given model & data source with optional augmentation.\n",
      "\n",
      "    Some data source (either a dataloader or a dataset) must be provided\n",
      "    or an InvalidArgument exception is raised.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    model : SomeModel\n",
      "        Maite Model object.\n",
      "\n",
      "    dataloader : Optional[SomeDataloader], (default=None)\n",
      "        Compatible maite dataloader.\n",
      "\n",
      "    dataset : Optional[SomeDataset], (default=None)\n",
      "        Compatible maite dataset.\n",
      "\n",
      "    batch_size : int, (default=1)\n",
      "        Batch size for use with dataset (ignored if dataset=None).\n",
      "\n",
      "    augmentation : Optional[SomeAugmentation], (default=None)\n",
      "        Compatible maite augmentation.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Tuple[Sequence[SomeTargetBatchType], Sequence[Tuple[SomeInputBatchType, SomeTargetBatchType, SomeMetadataBatchType]],\n",
      "        A tuple of the predictions (as a sequence of batches) and a sequence\n",
      "        of tuples containing the information associated with each batch.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(predict.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maite_mwe3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
