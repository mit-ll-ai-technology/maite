{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of image classification protocols\n",
    "\n",
    "This is an end-to-end demonstration of the MAITE protocols using \"dummy\" implementations of the various components.\n",
    "\n",
    "It shows how a component implementer/provider might wrap a component to conform to MAITE protocols.\n",
    "\n",
    "It also shows how components can be used in MAITE workflows (`predict` and `evaluate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from torch import nn\n",
    "from typing import Any, Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MAITE protocols that don't depend on machine learning task type\n",
    "from maite.protocols import ArrayLike\n",
    "\n",
    "# import versions of generic MAITE component protocols specialized to image classification task\n",
    "from maite.protocols.image_classification import (\n",
    "    Augmentation,\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    InputBatchType,\n",
    "    MetadataBatchType,\n",
    "    Metric,\n",
    "    Model,\n",
    "    OutputBatchType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify Pylance configured properly in vscode --> should see red squiggle under \"abc\"\n",
    "an_int: int = \"abc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledSoftmax(nn.Module):\n",
    "    def __init__(self, temperature: float):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(x / self.temperature, dim=1)\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self, name: str, num_classes: int, device: str):\n",
    "        self.name = name\n",
    "        self.device = device\n",
    "\n",
    "        # mimick a single-layer feedforward NN\n",
    "        in_features = 3 * 32 * 32\n",
    "        out_features = num_classes\n",
    "        self._model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, out_features, bias=False),\n",
    "            #nn.Softmax(dim=1)\n",
    "            ScaledSoftmax(0.1)\n",
    "        )\n",
    "\n",
    "        # overwrite weights in Linear layer to make deterministic\n",
    "        self._model[1].weight.data = 0.01 * torch.randn(\n",
    "            out_features,\n",
    "            in_features,\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "        # set to evaluate mode\n",
    "        self._model.eval()\n",
    "\n",
    "        # move to device\n",
    "        self._model.to(device)\n",
    "\n",
    "    def __call__(self, input: ArrayLike) -> torch.Tensor:\n",
    "        # tensor library bridging\n",
    "        xb = torch.as_tensor(input)\n",
    "\n",
    "        # make sure is batch\n",
    "        assert(xb.ndim == 4)\n",
    "\n",
    "        # move data to device\n",
    "        xb = xb.to(self.device)\n",
    "\n",
    "        # apply model\n",
    "        output = self._model(xb)\n",
    "        return output.detach()\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        return dict(name=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that model conforms to `Model` protocol\n",
    "model: Model = MyModel(\"mymodel1\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.0270, 0.0060, 0.4170, 0.0040, 0.0000, 0.4390, 0.0000, 0.0000,\n",
       "         0.0920]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test determinism\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "x = torch.rand(3, 32, 32, generator=torch.Generator().manual_seed(12345678))\n",
    "output = model(x.unsqueeze(0)) # convert to batch, which model expects\n",
    "output = torch.as_tensor(output)\n",
    "torch.round(output * 1e3) / 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: maybe having map-style dataset protocol as well would be handy?\n",
    "# - since easier to directly implement (although i might just be missing some obvious way to do it)\n",
    "# - and since probably can use either map-style or iterable-style in `predict` and `evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# dataset that mimicks CIFAR-10 w/inputs of shape (3, 32, 32)\n",
    "# target class/label for image i is `i % num_classes`\n",
    "\n",
    "# see: https://realpython.com/python-iterators-iterables/\n",
    "class MyIterator(collections.abc.Iterator):\n",
    "    def __init__(self, sequence: Sequence):\n",
    "        self.sequence = sequence\n",
    "        self.i = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i < len(self.sequence):\n",
    "            x = self.sequence[self.i]\n",
    "            self.i += 1\n",
    "            return x\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "class MyDataset(Sequence):\n",
    "    def __init__(self, name: str, num_classes: int, num_items: int):\n",
    "        self.name = name\n",
    "        self.num_classes = num_classes\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_items\n",
    "\n",
    "    def __getitem__(\n",
    "        self, i: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, dict[str, Any]]:\n",
    "        assert i < self.num_items\n",
    "        input = i * torch.ones(3, 32, 32)\n",
    "        output = torch.zeros(self.num_classes)\n",
    "        output[i % self.num_classes] = 1\n",
    "        metadata = dict(uuid=i, gsd=i/10.0)\n",
    "\n",
    "        return input, output, metadata\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return MyIterator(self)\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        return dict(name=self.name)\n",
    "\"\"\";        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    "    def __init__(self, name: str, num_classes: int, num_items: int):\n",
    "        self.name = name\n",
    "        self.num_classes = num_classes\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_items\n",
    "\n",
    "    def __getitem__(\n",
    "        self, i: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, dict[str, Any]]:\n",
    "        assert i < self.num_items\n",
    "        #input = i * torch.ones(3, 32, 32)\n",
    "        input = torch.rand(3, 32, 32, generator=torch.Generator().manual_seed(i))\n",
    "        output = torch.zeros(self.num_classes)\n",
    "        output[i % self.num_classes] = 1\n",
    "        metadata = dict(uuid=i, gsd=i/10.0)\n",
    "\n",
    "        return input, output, metadata\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        return dict(name=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this would trigger AssertionError\n",
    "# - i.e., don't seem to be able to iterate over any old thing with __len__ and __getitem__\n",
    "\n",
    "#for x, y, md in dataset:\n",
    "#    print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6147, 0.3810, 0.6371,  ..., 0.2571, 0.0458, 0.1755],\n",
       "          [0.6177, 0.8291, 0.5246,  ..., 0.0727, 0.6463, 0.9804],\n",
       "          [0.9441, 0.4921, 0.6659,  ..., 0.5409, 0.7992, 0.7677],\n",
       "          ...,\n",
       "          [0.8539, 0.6372, 0.7458,  ..., 0.1298, 0.6168, 0.3205],\n",
       "          [0.2958, 0.9967, 0.1822,  ..., 0.8969, 0.2356, 0.5888],\n",
       "          [0.0706, 0.0296, 0.8922,  ..., 0.5491, 0.0876, 0.3411]],\n",
       " \n",
       "         [[0.4372, 0.4878, 0.6424,  ..., 0.5184, 0.8872, 0.9632],\n",
       "          [0.5844, 0.6769, 0.5594,  ..., 0.6569, 0.2506, 0.8598],\n",
       "          [0.7092, 0.0267, 0.8670,  ..., 0.9183, 0.4187, 0.3030],\n",
       "          ...,\n",
       "          [0.1170, 0.9725, 0.6277,  ..., 0.2707, 0.6050, 0.7176],\n",
       "          [0.6282, 0.6714, 0.4452,  ..., 0.8159, 0.0394, 0.0110],\n",
       "          [0.8284, 0.1825, 0.2938,  ..., 0.1049, 0.7608, 0.4508]],\n",
       " \n",
       "         [[0.6736, 0.4308, 0.5341,  ..., 0.0463, 0.9253, 0.6669],\n",
       "          [0.7646, 0.6069, 0.7050,  ..., 0.9626, 0.1037, 0.9806],\n",
       "          [0.0337, 0.9157, 0.1781,  ..., 0.7657, 0.7680, 0.6885],\n",
       "          ...,\n",
       "          [0.5026, 0.1372, 0.4472,  ..., 0.6469, 0.7689, 0.5133],\n",
       "          [0.1542, 0.5459, 0.5439,  ..., 0.6956, 0.9942, 0.4857],\n",
       "          [0.2985, 0.5439, 0.9109,  ..., 0.2859, 0.0956, 0.2709]]]),\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " {'uuid': 2, 'gsd': 0.2})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Note: typical end users will probably not need to deal directly with dataloaders.\n",
    "\n",
    "But it's handy to see how something conforming to MAITE DataLoader protocol would work. And also gives us batches to show things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid default collate behavior for maps, e.g.,\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.default_collate\n",
    "# default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])\n",
    "# --> {'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    from torch.utils.data import default_collate\n",
    "    return (\n",
    "        default_collate([t[0] for t in batch]), # collate sequence of inputs (into single tensor)\n",
    "        default_collate([t[1] for t in batch]), # collate sequence of outputs (into single tensor)\n",
    "        [t[2] for t in batch], # leave as sequence of dicts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap PyTorch DataLoader?\n",
    "class MyDataLoader:\n",
    "    def __init__(self, dataset: Dataset, batch_size: int):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # reason for type ignore is that MAITE `Dataset` doesn't completely match PyTorch `Dataset` or `IterableDataset`\n",
    "        # - which has `__add__()` method that i don't think is needed by PyTorch dataloader\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, collate_fn=collate_fn, batch_size=batch_size) # type: ignore\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader: DataLoader = MyDataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 32]),\n",
       " torch.Size([4, 10]),\n",
       " [{'uuid': 0, 'gsd': 0.0},\n",
       "  {'uuid': 1, 'gsd': 0.1},\n",
       "  {'uuid': 2, 'gsd': 0.2},\n",
       "  {'uuid': 3, 'gsd': 0.3}])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, mdb = next(iter(dataloader))\n",
    "\n",
    "# tensor bridging\n",
    "xb_pt = torch.as_tensor(xb)\n",
    "yb_pt = torch.as_tensor(yb)\n",
    "\n",
    "xb_pt.shape, yb_pt.shape, mdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0030, 0.0130, 0.0000, 0.5930, 0.0010, 0.0030, 0.3740, 0.0000, 0.0010,\n",
       "         0.0130],\n",
       "        [0.0000, 0.0020, 0.0010, 0.0200, 0.0000, 0.0000, 0.9760, 0.0000, 0.0000,\n",
       "         0.0010],\n",
       "        [0.0140, 0.0140, 0.0000, 0.1050, 0.0010, 0.0010, 0.1860, 0.0000, 0.0000,\n",
       "         0.6790],\n",
       "        [0.0030, 0.6500, 0.0190, 0.1870, 0.0740, 0.0010, 0.0610, 0.0000, 0.0000,\n",
       "         0.0040]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send batch through model to look at \"probabilities\"\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "preds = model(xb)\n",
    "torch.round(preds * 1e3) / 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "Example of augmentation that:\n",
    "- changes inputs in way that depends on datum-level metadata\n",
    "  - expects \"gsd\" datum-level metadata\n",
    "- adds augmentation-generated datum-level metadata\n",
    "  - similar to recording particular rotation angle applied\n",
    "- doesn't change targets/labels since doesn't change image geometry\n",
    "  - and for image classification, the targets/labels don't have geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAugmentation:\n",
    "    def __init__(self, name: str, multiplier: float):\n",
    "        self.name = name\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: tuple[InputBatchType, OutputBatchType, MetadataBatchType],\n",
    "    ) -> tuple[torch.Tensor, OutputBatchType, MetadataBatchType]:\n",
    "        xb, yb, metadata = batch\n",
    "\n",
    "        # tensor bridging\n",
    "        xb = torch.as_tensor(xb)\n",
    "        yb = torch.as_tensor(yb)\n",
    "        assert xb.ndim == 4\n",
    "\n",
    "        # iterate over (parallel) elements in batch\n",
    "        x_augs = [] # list of individual augmented inputs\n",
    "        md_augs = [] # list of individual image-level metadata\n",
    "\n",
    "        for x, y, md in zip(xb, yb, metadata):\n",
    "            assert \"gsd\" in md\n",
    "            gsd = md[\"gsd\"]\n",
    "\n",
    "            # \"augment\" by changing the input (with seed based on original input)\n",
    "            seed = int(gsd * xb.sum().item())\n",
    "            x_aug = torch.rand(3, 32, 32, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "            # replace small slice of image with gsd value so can see a change\n",
    "            x_aug[0, 0, 0] = self.multiplier * gsd\n",
    "            x_aug[1, 0, 0] = self.multiplier * gsd\n",
    "            x_aug[2, 0, 0] = self.multiplier * gsd\n",
    "\n",
    "            x_augs.append(x_aug)\n",
    "\n",
    "            # generate fake metadata\n",
    "\n",
    "            # save original first\n",
    "            md_aug = copy.deepcopy(md)\n",
    "            \n",
    "            # add new metadata under a \"namespace\" key to avoid collisions (and also for organization)\n",
    "            aug_parent_key = self.name\n",
    "            md_aug[aug_parent_key] = {\n",
    "                \"aug_param\": self.multiplier * gsd\n",
    "            }\n",
    "            md_augs.append(md_aug)\n",
    "        \n",
    "        # return batch of augmented inputs, original outputs (since unchanged), and updated metadata\n",
    "        return torch.stack(x_augs), yb, md_augs\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        return dict(\n",
    "            name=self.name,\n",
    "            multiplier=self.multiplier\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test typing\n",
    "augmentation: Augmentation = MyAugmentation(\"aug1\", 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 32]),\n",
       " torch.Size([4, 10]),\n",
       " [{'uuid': 0, 'gsd': 0.0, 'aug1': {'aug_param': 0.0}},\n",
       "  {'uuid': 1, 'gsd': 0.1, 'aug1': {'aug_param': 1.0}},\n",
       "  {'uuid': 2, 'gsd': 0.2, 'aug1': {'aug_param': 2.0}},\n",
       "  {'uuid': 3, 'gsd': 0.3, 'aug1': {'aug_param': 3.0}}])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show application to batch of data\n",
    "batch = next(iter(dataloader))\n",
    "xb_aug, yb_aug, mdb_aug = augmentation(batch)\n",
    "\n",
    "# tensor bridging\n",
    "xb_aug = torch.as_tensor(xb_aug)\n",
    "yb_aug = torch.as_tensor(yb_aug)\n",
    "\n",
    "# inspect augmented batch\n",
    "xb_aug.shape, yb_aug.shape, mdb_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.7682, 0.0885,  ..., 0.3734, 0.3051, 0.9320],\n",
       "         [0.1759, 0.2698, 0.1507,  ..., 0.7011, 0.2038, 0.6511],\n",
       "         [0.7745, 0.4369, 0.5191,  ..., 0.6870, 0.0051, 0.1757],\n",
       "         ...,\n",
       "         [0.8787, 0.6569, 0.9944,  ..., 0.2269, 0.6664, 0.5225],\n",
       "         [0.1427, 0.6076, 0.9553,  ..., 0.7924, 0.5431, 0.8903],\n",
       "         [0.5937, 0.3392, 0.8387,  ..., 0.1876, 0.2099, 0.7210]],\n",
       "\n",
       "        [[0.0000, 0.0278, 0.2117,  ..., 0.8647, 0.0605, 0.4548],\n",
       "         [0.9106, 0.6936, 0.9212,  ..., 0.8531, 0.7173, 0.4575],\n",
       "         [0.4692, 0.1864, 0.3191,  ..., 0.1398, 0.0620, 0.3074],\n",
       "         ...,\n",
       "         [0.9365, 0.3450, 0.3035,  ..., 0.2062, 0.6444, 0.6147],\n",
       "         [0.7693, 0.4257, 0.7569,  ..., 0.6605, 0.8492, 0.5603],\n",
       "         [0.4499, 0.8180, 0.1410,  ..., 0.5875, 0.8263, 0.2909]],\n",
       "\n",
       "        [[0.0000, 0.3556, 0.1764,  ..., 0.2433, 0.6071, 0.2682],\n",
       "         [0.3052, 0.1653, 0.0830,  ..., 0.8141, 0.5898, 0.3632],\n",
       "         [0.5211, 0.9456, 0.5542,  ..., 0.9786, 0.0670, 0.1634],\n",
       "         ...,\n",
       "         [0.8500, 0.4284, 0.5777,  ..., 0.0587, 0.4500, 0.6681],\n",
       "         [0.9824, 0.3804, 0.8070,  ..., 0.0895, 0.6932, 0.1993],\n",
       "         [0.3121, 0.3752, 0.5355,  ..., 0.6533, 0.6952, 0.7752]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upperleft element of each channel for image 0 should be 0 (based on fake augmentation)\n",
    "xb_aug = torch.as_tensor(xb_aug)\n",
    "xb_aug[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0030, 0.0120, 0.0000, 0.6330, 0.0010, 0.0030, 0.3360, 0.0000, 0.0000,\n",
       "         0.0110],\n",
       "        [0.0050, 0.0520, 0.0010, 0.5740, 0.0020, 0.0050, 0.3560, 0.0000, 0.0000,\n",
       "         0.0050],\n",
       "        [0.0140, 0.1460, 0.0010, 0.6380, 0.0070, 0.0120, 0.0820, 0.0000, 0.0000,\n",
       "         0.1000],\n",
       "        [0.0030, 0.0020, 0.0000, 0.0030, 0.0000, 0.0000, 0.9900, 0.0000, 0.0000,\n",
       "         0.0020]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hopefully model outputs are at least a little different\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "preds = model(xb_aug)\n",
    "torch.round(preds * 1e3) / 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird metric that sees if score of correct class is >= threshold\n",
    "\n",
    "class MyMetric:\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.total = 0\n",
    "        self.correct = 0\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total = 0\n",
    "        self.correct = 0\n",
    "\n",
    "    def update(self, preds: OutputBatchType, targets: OutputBatchType) -> None:\n",
    "        # tensor bridging\n",
    "        preds = torch.as_tensor(preds)\n",
    "        targets = torch.as_tensor(targets)\n",
    "        \n",
    "        # actual accuracy\n",
    "        #self.total += len(preds)\n",
    "        #self.correct += (preds.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "\n",
    "        # weird accuracy\n",
    "        self.total += len(preds)\n",
    "\n",
    "        target_cols = targets.argmax(dim=1)\n",
    "        for i in range(len(preds)):\n",
    "            self.correct += 1 if preds[i, target_cols[i]].item() >= self.threshold else 0\n",
    "\n",
    "    def compute(self) -> dict[str, Any]:\n",
    "        return {\"threshold_exceedance\": self.correct / self.total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric: Metric = MyMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite.workflows import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 32]),\n",
       " torch.Size([4, 10]),\n",
       " torch.Size([4, 10]),\n",
       " [{'uuid': 0, 'gsd': 0.0},\n",
       "  {'uuid': 1, 'gsd': 0.1},\n",
       "  {'uuid': 2, 'gsd': 0.2},\n",
       "  {'uuid': 3, 'gsd': 0.3}])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test predict with dataloader and no augmentation\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "dataloader: DataLoader = MyDataLoader(dataset, batch_size=4)\n",
    "\n",
    "pred_batches, data_batches = predict(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    augmentation=None\n",
    "\n",
    ")\n",
    "\n",
    "# look at first batch\n",
    "(x, y, md) = data_batches[0]\n",
    "pred = pred_batches[0]\n",
    "\n",
    "# tensor bridging\n",
    "x = torch.as_tensor(x)\n",
    "y = torch.as_tensor(y)\n",
    "pred = torch.as_tensor(pred)\n",
    "\n",
    "x.shape, y.shape, pred.shape, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred.shape = torch.Size([4, 10])\n",
      "pred.shape = torch.Size([4, 10])\n",
      "pred.shape = torch.Size([4, 10])\n",
      "pred.shape = torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# show all batch sizes\n",
    "for (x, y, md), pred in zip(data_batches, pred_batches):\n",
    "    pred = torch.as_tensor(pred)\n",
    "    print(f\"{pred.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 32]),\n",
       " torch.Size([4, 10]),\n",
       " torch.Size([4, 10]),\n",
       " [{'uuid': 0, 'gsd': 0.0, 'aug1': {'aug_param': 0.0}},\n",
       "  {'uuid': 1, 'gsd': 0.1, 'aug1': {'aug_param': 1.0}},\n",
       "  {'uuid': 2, 'gsd': 0.2, 'aug1': {'aug_param': 2.0}},\n",
       "  {'uuid': 3, 'gsd': 0.3, 'aug1': {'aug_param': 3.0}}])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test predict with augmentation\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "dataloader: DataLoader = MyDataLoader(dataset, batch_size=4)\n",
    "augmentation: Augmentation = MyAugmentation(\"aug1\", 10.0)\n",
    "\n",
    "pred_batches, data_batches = predict(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "# look at first batch\n",
    "(x, y, md), pred = data_batches[0], pred_batches[0]\n",
    "\n",
    "# tensor bridging\n",
    "x = torch.as_tensor(x)\n",
    "y = torch.as_tensor(y)\n",
    "pred = torch.as_tensor(pred)\n",
    "\n",
    "x.shape, y.shape, pred.shape, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]),\n",
       " torch.Size([1, 10]),\n",
       " torch.Size([1, 10]),\n",
       " [{'uuid': 0, 'gsd': 0.0}])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test predict with dataset and no augmentation\n",
    "# - will use batch_size of 1\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "\n",
    "pred_batches, data_batches = predict(\n",
    "    model=model,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "# look at first batch\n",
    "(x, y, md) = data_batches[0]\n",
    "pred = pred_batches[0]\n",
    "\n",
    "# tensor bridging\n",
    "x = torch.as_tensor(x)\n",
    "y = torch.as_tensor(y)\n",
    "pred = torch.as_tensor(pred)\n",
    "\n",
    "x.shape, y.shape, pred.shape, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n",
      "pred.shape = torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# show all batch sizes\n",
    "for (x, y, md), pred in zip(data_batches, pred_batches):\n",
    "    pred = torch.as_tensor(pred)\n",
    "    print(f\"{pred.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite.workflows import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold_exceedance': 0.6428571428571429}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate with dataloader, no augmentation\n",
    "\n",
    "threshold = 0.002\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "dataloader: DataLoader = MyDataLoader(dataset, batch_size=4)\n",
    "metric: Metric = MyMetric(threshold=threshold)\n",
    "\n",
    "results, _, _ = evaluate(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold_exceedance': 0.6428571428571429}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate with dataset, no augmentation\n",
    "\n",
    "THRESHOLD = 0.002\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "metric: Metric = MyMetric(threshold=THRESHOLD)\n",
    "\n",
    "results, _, _ = evaluate(model=model, dataloader=dataloader, metric=metric)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold_exceedance': 0.6428571428571429}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate with dataset, different batch size, no augmentation\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "metric: Metric = MyMetric(threshold=threshold)\n",
    "\n",
    "results, _, _ = evaluate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold_exceedance': 0.5714285714285714}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate with dataloader, augmentation\n",
    "\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "dataloader: DataLoader = MyDataLoader(dataset, batch_size=4)\n",
    "metric: Metric = MyMetric(threshold=threshold)\n",
    "augmentation: Augmentation = MyAugmentation(\"aug1\", 10.0)\n",
    "\n",
    "results, _, _ = evaluate(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    metric=metric,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate that takes predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(\n",
    "    preds: Sequence[OutputBatchType],\n",
    "    targets: Sequence[OutputBatchType],\n",
    "    metric: Metric,\n",
    ") -> dict[str, Any]:\n",
    "\n",
    "    metric.reset()\n",
    "    for predb, yb, in zip(preds, targets):\n",
    "        metric.update(predb, yb)\n",
    "    results = metric.compute()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold_exceedance': 0.6428571428571429}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate that takes predict output\n",
    "\n",
    "# get predictions (no augmentation)\n",
    "model = MyModel(\"mymodel1\", 10, \"cpu\")\n",
    "dataset: Dataset = MyDataset(\"pseudo-cifar-10\", num_classes=10, num_items=14)\n",
    "\n",
    "pred_batches, data_batches = predict(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# get predictions and targets/labels out of sequence of tuples\n",
    "preds = [pred for pred in pred_batches]\n",
    "targets = [t[1] for t in data_batches]\n",
    "\n",
    "# send through evaluate\n",
    "result = evaluate_preds(\n",
    "    preds=preds,\n",
    "    targets=targets,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    "- make version of `evaluate` that takes dataset, dataloader, OR, preds/targets?\n",
    "- OR\n",
    "- put overloaded versions of `predict` and `evaluate` on a class?\n",
    "  - that's also generic so that can easily get image classification and object detection flavors of it?\n",
    "- allow `predict` to return sequence of tuples of non-batch types? (e.g., if batch_size 0 or 1)\n",
    "- return namedtuple from `predict` if that still conforms to protocol (so that more descriptive)?\n",
    "- should we have aliases for Target and Prediction to make clearer?\n",
    "  - e.g., `TargetBatchType`, `PredictionBatchType`\n",
    "- could make version of `evaluate` that takes predictions literally take output from `predict` (i.e., sequence of tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maite_mwe3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
