{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n",
    "Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).\n",
    "SPDX-License-Identifier: MIT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap an Object Detection Dataset\n",
    "\n",
    "In this how-to, we will show you how to wrap an object detection dataset as a [maite.protocols.object_detection.Dataset](../generated/maite.protocols.object_detection.Dataset.html). The general steps for wrapping a dataset are:\n",
    "\n",
    "* Understand the source (native) dataset\n",
    "* Create a wrapper that makes the source dataset conform to the MAITE dataset protocol (interface)\n",
    "* Verify that the wrapped dataset works correctly\n",
    "\n",
    "Note:\n",
    "\n",
    "* Implementors do not need to have their class inherit from a base class like `maite.protocols.object_detection.Dataset` (although they can) since MAITE uses Python protocols for defining interfaces. This note applies to other MAITE protocols as well, e.g., `Model` and `Metric`. See the [Primer on Python Typing](../explanation/type_hints_for_API_design.html) for more background on protocols and structural subtyping.\n",
    "* There are multiple ways of implementing a `Dataset` class. Implementors can start from scratch or leverge dataset utilities provided by other libraries such as Torchvision or Hugging Face.\n",
    "\n",
    "This how-to guide will be using Torchvision's [datasets.load_dataset.CocoDetection](https://pytorch.org/vision/stable/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection) class to load images and COCO-formatted annotations from a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load a subset of the COCO dataset with Torchvision\n",
    "\n",
    "First we load the necessary packages for running this notebook and display their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from maite.protocols import DatasetMetadata, DatumMetadata, object_detection as od\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.ops.boxes import box_convert\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from typing import Any\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this how-to guide faster to run and not require a large download, we'll work with a subset of the [Common Objects in Context (COCO)](https://cocodataset.org) dataset. We've modified the annotations JSON file from the validation split of the COCO 2017 Object Detection Task to contain only the first 4 images (and will dynamically download only those images using the \"coco_url\").\n",
    "\n",
    "Note that the COCO annotations are licensed under a [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode) (see COCO [terms of use](https://cocodataset.org/#termsofuse)).\n",
    "\n",
    "We use the following function and code to download the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(coco_json_subset: dict[str, Any], root: Path):\n",
    "    \"\"\"Download a subset of COCO images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coco_json_subset : dict[str, Any]\n",
    "        COCO val2017_first4 JSON file.\n",
    "    root : Path\n",
    "        Location of COCO data.\n",
    "    \"\"\"\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for image in coco_json_subset[\"images\"]:\n",
    "        url = image[\"coco_url\"]\n",
    "        filename = Path(root) / image[\"file_name\"]\n",
    "        if filename.exists():\n",
    "            print(f\"skipping {url}\")\n",
    "        else:\n",
    "            print(f\"saving {url} to {filename} ... \", end=\"\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT = Path(\"../sample_data/coco/coco_val2017_subset\")\n",
    "coco_subset_json = dict()\n",
    "ann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\n",
    "coco_subset_json = json.load(open(ann_subset_file, \"r\"))\n",
    "download_images(coco_subset_json, root=COCO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Torchvision's `CocoDetection` dataset class to load our COCO subset located under `maite/examples/sample_data/coco/coco_val2017_subset`. The keyword arguments of `CocoDetection` are:\n",
    "\n",
    "- `root`: the location of the folder containing the images\n",
    "\n",
    "- `annFile`: the location of the annotation JSON file\n",
    "\n",
    "Please see Torchvision's [documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection) for more information on how to use the `CocoDetection` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dataset = CocoDetection(\n",
    "    root=str(COCO_ROOT),\n",
    "    annFile=str(ann_subset_file),\n",
    ")\n",
    "\n",
    "print(f\"\\n{len(tv_dataset) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Examine the source dataset\n",
    "\n",
    "One datum from `tv_dataset` includes an image and its corresponding annotations. Let's get one datum and inspect it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, annotations = tv_dataset[0]\n",
    "\n",
    "# `img` is a PIL Image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Optional: Hide the axes\n",
    "plt.show()\n",
    "print(f\"{type(img) = }\")\n",
    "\n",
    "# `annotations` is a list of dictionaries (corresponding to 14 objects in this case)\n",
    "print(f\"{len(annotations) = }\")\n",
    "\n",
    "# Each annotation dictionary contains the object's bounding box (bbox) plus other info\n",
    "print(f\"{annotations[0].keys() = }\")\n",
    "\n",
    "# Note that the COCO bounding box format is [x_min, y_min, width, height]\n",
    "print(f\"{annotations[0]['bbox'] = }\")\n",
    "\n",
    "# Class/category labels and ids are available via the `cats` attribute on the dataset itself\n",
    "print(f\"{tv_dataset.coco.cats[1] = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Create a MAITE wrapper for the source dataset\n",
    "\n",
    "So far we've used Torchvision's `CocoDetection` class to load and explore the dataset. It doesn't comply with `maite.protocols.object_detection.Dataset`; however, it can be used as a starting point to construct a MAITE-compliant object detection dataset.\n",
    "\n",
    "A class implementing `maite.protocols.object_detection.Dataset` needs the following two methods:\n",
    "\n",
    "- `__getitem__(index: int)`: returns a tuple containing a datum's input image, ground truth detections, and metadata.\n",
    "- `__len__()`: returns the number of data elements in the dataset.\n",
    "\n",
    "and an attribute:\n",
    "\n",
    "- `metadata`: a typed dictionary containing an `id` string and an optional (but recommended) map from class indexes to labels.\n",
    "\n",
    "The datum returned by `__getitem__` is of type `tuple[InputType, ObjectDetectionTarget, DatumMetadata]`.\n",
    "\n",
    "- MAITE's `InputType` is an `ArrayLike` (e.g., `numpy.ndarray` or `torch.Tensor`) with shape `(C, H, W)` representing the channel, height, and width of an image. Since `tv_dataset` has images in the PIL image format, we need to convert them to a type compatible with `ArrayLike`, which we'll do using the `pil_to_tensor` function.\n",
    "\n",
    "- MAITE's `ObjectDetectionTarget` protocol can be implemenented by defining a `dataclass`, `CocoDetectionTarget`, which encodes the ground-truth labels of the object detection task, namely:\n",
    "\n",
    "    - `boxes`: a shape-(N_DETECTIONS, 4) `ArrayLike`, where N_DETECTIONS is the number of detections (objects) in the current single image, and each row is a bounding box in `x0, y0, x1, y1` format\n",
    "    - `labels`: a shape-(N_DETECTIONS,) `ArrayLike` containing the integer label associated with each detection\n",
    "    - `scores`: a shape-(N_DETECTIONS,) `ArrayLike` containing the score (confidence) associated with each detection. For a dataset's ground truth (as opposed to model predictions), `scores` are always 1.\n",
    "\n",
    "- MAITE's `DatumMetadata` is a `TypedDict` containing metadata associated with a single datum. An `id` of type `int` or `str` is required.\n",
    "\n",
    "Putting everything together, below is an implementation of the `maite.protocols.object_detection.Dataset` protocol called `CocoDetectionDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CocoDetectionTarget:\n",
    "    boxes: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    scores: torch.Tensor\n",
    "\n",
    "class CocoDetectionDataset:\n",
    "    def __init__(self, dataset: CocoDetection, id: str):\n",
    "        self._dataset = dataset\n",
    "\n",
    "        # Get mapping from COCO category to name\n",
    "        index2label = {k: v[\"name\"] for k, v in dataset.coco.cats.items()}\n",
    "\n",
    "        # Add dataset-level metadata attribute, with required id and optional index2label mapping\n",
    "        self.metadata: DatasetMetadata = {\n",
    "            \"id\": id,\n",
    "            \"index2label\": index2label,\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, CocoDetectionTarget, DatumMetadata]:\n",
    "\n",
    "        # Get original data item\n",
    "        img_pil, annotations = self._dataset[index]\n",
    "\n",
    "        # Format input\n",
    "        img_pt = pil_to_tensor(img_pil)\n",
    "\n",
    "        # Format ground truth\n",
    "        num_boxes = len(annotations)\n",
    "        boxes = torch.zeros(num_boxes, 4)\n",
    "        for i, ann in enumerate(annotations):\n",
    "            bbox = torch.as_tensor(ann[\"bbox\"])\n",
    "            boxes[i, :] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "\n",
    "        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n",
    "        scores = torch.ones(num_boxes)\n",
    "\n",
    "        # Format metadata\n",
    "        datum_metadata: DatumMetadata = {\n",
    "            \"id\": self._dataset.ids[index],\n",
    "        }\n",
    "\n",
    "        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset: od.Dataset = CocoDetectionDataset(tv_dataset, \"COCO Detection Subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a dataset class that conforms to the `maite.protocols.object_detection.Dataset` protocol, and wrapped the native dataset with it.\n",
    "\n",
    "Here the `coco_dataset` variable has `od.Dataset` as the type hint. If your development environment has a static type checker like Pyright enabled (see the [Enable Static Type Checking](./static_typing.html) guide), then the type checker will verify that our wrapped dataset conforms to the protocol and indicate a problem if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Examine the MAITE-wrapped dataset\n",
    "\n",
    "Now let's inspect MAITE-compliant object detection dataset to verify that it's behaving as expected.\n",
    "\n",
    "First we verify that the length is 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(coco_dataset) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the dataset-level metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(coco_dataset.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we inspect a single datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first datum\n",
    "image, target, datum_metadata = coco_dataset[0]\n",
    "\n",
    "# Display image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Optional: Hide the axes\n",
    "plt.show()\n",
    "\n",
    "# Bridge/convert ArrayLike's to PyTorch tensors\n",
    "image = torch.as_tensor(image)\n",
    "boxes = torch.as_tensor(target.boxes)\n",
    "labels = torch.as_tensor(target.labels)\n",
    "scores = torch.as_tensor(target.scores)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"{image.shape = }\")  # image has height 230 and weight 352\n",
    "print(f\"{boxes.shape = }\")  # there are 14 bounding boxes\n",
    "print(f\"{labels.shape = }\")\n",
    "print(f\"{scores.shape = }\")\n",
    "\n",
    "# Print datum-level metadata\n",
    "print(f\"{datum_metadata = }\")  # this datum corresponds to image file 000000037777.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "In this how-to guide, we demonstrated how to wrap an object detection dataset to be MAITE compliant. Key implementation steps included:\n",
    "\n",
    "- Defining `__len__` and `__getitem__` methods\n",
    "- Choosing a specific image `InputType` (e.g., `numpy.ndarray` or `torch.Tensor`)\n",
    "- Creating a target dataclass that conforms to the `ObjectDetectionTarget` protocol\n",
    "- Setting up `DatasetMetadata` and `DatumMetadata`\n",
    "\n",
    "More dataset protocol details can be found here: [maite.protocols.object_detection.Dataset](../generated/maite.protocols.object_detection.Dataset.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i628_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
