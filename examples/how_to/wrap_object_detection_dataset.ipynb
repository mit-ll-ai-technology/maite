{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n",
    "Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).\n",
    "SPDX-License-Identifier: MIT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap an Object Detection Dataset\n",
    "\n",
    "In this how-to, we will show you how to wrap an object detection dataset as a [maite.protocols.object_detection.Dataset](../generated/maite.protocols.object_detection.Dataset.html). The general steps for wrapping a dataset are:\n",
    "\n",
    "* Understand the source (native) dataset\n",
    "* Create a wrapper that makes the source dataset conform to the MAITE dataset protocol (interface)\n",
    "* Verify that the wrapped dataset works correctly\n",
    "\n",
    "Note:\n",
    "\n",
    "* Implementors do not need to have their class inherit from a base class like `maite.protocols.object_detection.Dataset` (although they can) since MAITE uses Python protocols for defining interfaces. This note applies to other MAITE protocols as well, e.g., `Model` and `Metric`. See the [Primer on Python Typing](../explanation/type_hints_for_API_design.html) for more background on protocols and structural subtyping.\n",
    "* There are multiple ways of implementing a `Dataset` class. Implementors can start from scratch or leverge dataset utilities provided by other libraries such as Torchvision or Hugging Face.\n",
    "\n",
    "This how-to guide will be using Torchvision's [datasets.load_dataset.CocoDetection](https://pytorch.org/vision/stable/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection) class to load images and COCO-formatted annotations from a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load a subset of the COCO dataset with Torchvision\n",
    "\n",
    "First we load the necessary packages for running this notebook and display their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.ops.boxes import box_convert\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "from maite.protocols import DatasetMetadata, DatumMetadata\n",
    "from maite.protocols import object_detection as od\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this how-to guide faster to run and not require a large download, we'll work with a subset of the [Common Objects in Context (COCO)](https://cocodataset.org) dataset. We've modified the annotations JSON file from the validation split of the COCO 2017 object detection dataset to contain only the first 4 images (and will dynamically download only those images using the \"coco_url\").\n",
    "\n",
    "Note that the COCO annotations are licensed under a [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode) (see COCO [terms of use](https://cocodataset.org/#termsofuse)).\n",
    "\n",
    "We use the following function and code to download the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(coco_json_subset: dict[str, Any], root: Path):\n",
    "    \"\"\"Download a subset of COCO images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coco_json_subset : dict[str, Any]\n",
    "        COCO val2017_first4 JSON file.\n",
    "    root : Path\n",
    "        Location of COCO data.\n",
    "    \"\"\"\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for image in coco_json_subset[\"images\"]:\n",
    "        url = image[\"coco_url\"]\n",
    "        filename = Path(root) / image[\"file_name\"]\n",
    "        if filename.exists():\n",
    "            print(f\"skipping {url}\")\n",
    "        else:\n",
    "            print(f\"saving {url} to {filename} ... \", end=\"\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_ROOT = Path(\"../sample_data/coco/coco_val2017_subset\")\n",
    "coco_subset_json = dict()\n",
    "ann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\n",
    "coco_subset_json = json.load(open(ann_subset_file, \"r\"))\n",
    "download_images(coco_subset_json, root=COCO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Torchvision's `CocoDetection` dataset class to load our COCO subset located under `maite/examples/sample_data/coco/coco_val2017_subset`. The keyword arguments of `CocoDetection` are:\n",
    "\n",
    "- `root`: the location of the folder containing the images\n",
    "\n",
    "- `annFile`: the location of the annotation JSON file\n",
    "\n",
    "Please see Torchvision's [documentation](https://pytorch.org/vision/stable/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection) for more information on how to use the `CocoDetection` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dataset = CocoDetection(\n",
    "    root=str(COCO_ROOT),\n",
    "    annFile=str(ann_subset_file),\n",
    ")\n",
    "\n",
    "print(f\"\\n{len(tv_dataset) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Examine the source dataset\n",
    "\n",
    "One datum from `tv_dataset` includes an image and its corresponding annotations. Let's get one datum and inspect it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, annotations = tv_dataset[0]\n",
    "\n",
    "# `img` is a PIL Image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Optional: Hide the axes\n",
    "plt.show()\n",
    "print(f\"{type(img) = }\")\n",
    "\n",
    "# `annotations` is a list of dictionaries (corresponding to 14 objects in this case)\n",
    "print(f\"{len(annotations) = }\")\n",
    "\n",
    "# Each annotation dictionary contains the object's bounding box (bbox) plus other info\n",
    "print(f\"{annotations[0].keys() = }\")\n",
    "\n",
    "# Note that the COCO bounding box format is [x_min, y_min, width, height]\n",
    "print(f\"{annotations[0]['bbox'] = }\")\n",
    "\n",
    "# Class/category labels and ids are available via the `cats` attribute on the dataset itself\n",
    "print(f\"{tv_dataset.coco.cats[1] = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Create a MAITE wrapper for the source dataset\n",
    "\n",
    "So far we've used Torchvision's `CocoDetection` class to load and explore the dataset. It doesn't comply with `maite.protocols.object_detection.Dataset`; however, it can be used as a starting point to construct a MAITE-compliant object detection dataset.\n",
    "\n",
    "A class implementing `maite.protocols.object_detection.Dataset` needs the following two methods:\n",
    "\n",
    "- `__getitem__(index: int)`: returns a tuple containing a datum's input image, ground truth detections, and metadata.\n",
    "- `__len__()`: returns the number of data elements in the dataset.\n",
    "\n",
    "and an attribute:\n",
    "\n",
    "- `metadata`: a typed dictionary containing an `id` string and an optional (but recommended) map from class indexes to labels.\n",
    "\n",
    "The datum returned by `__getitem__` is of type `tuple[InputType, ObjectDetectionTarget, DatumMetadata]`.\n",
    "\n",
    "- MAITE's `InputType` is an `ArrayLike` (e.g., `numpy.ndarray` or `torch.Tensor`) with shape `(C, H, W)` representing the channel, height, and width of an image. Since `tv_dataset` has images in the PIL image format, we need to convert them to a type compatible with `ArrayLike`, which we'll do using the `pil_to_tensor` function.\n",
    "\n",
    "- MAITE's `ObjectDetectionTarget` protocol can be implemenented by defining a `dataclass`, `CocoDetectionTarget`, which encodes the ground-truth labels of the object detection problem, namely:\n",
    "\n",
    "    - `boxes`: a shape-(N_DETECTIONS, 4) `ArrayLike`, where N_DETECTIONS is the number of detections (objects) in the current single image, and each row is a bounding box in `x0, y0, x1, y1` format\n",
    "    - `labels`: a shape-(N_DETECTIONS,) `ArrayLike` containing the integer label associated with each detection\n",
    "    - `scores`: a shape-(N_DETECTIONS,) or (N_DETECTIONS, N_CLASSES) `ArrayLike` containing the score associated with each detection. For a dataset's ground truth (as opposed to model predictions), `scores` are always 1.\n",
    "\n",
    "- MAITE's `DatumMetadata` is a `TypedDict` containing metadata associated with a single datum. An `id` of type `int` or `str` is required.\n",
    "\n",
    "Putting everything together, below is an implementation of the `maite.protocols.object_detection.Dataset` protocol called `CocoDetectionDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CocoDetectionTarget:\n",
    "    boxes: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    scores: torch.Tensor\n",
    "\n",
    "\n",
    "class CocoDetectionDataset:\n",
    "    def __init__(self, dataset: CocoDetection, id: str):\n",
    "        self._dataset = dataset\n",
    "\n",
    "        # Get mapping from COCO category to name\n",
    "        index2label = {k: v[\"name\"] for k, v in dataset.coco.cats.items()}\n",
    "\n",
    "        # Add dataset-level metadata attribute, with required id and optional index2label mapping\n",
    "        self.metadata: DatasetMetadata = {\n",
    "            \"id\": id,\n",
    "            \"index2label\": index2label,\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, CocoDetectionTarget, DatumMetadata]:\n",
    "        # Get original data item\n",
    "        img_pil, annotations = self._dataset[index]\n",
    "\n",
    "        # Format input\n",
    "        img_pt = pil_to_tensor(img_pil)\n",
    "\n",
    "        # Format ground truth\n",
    "        num_boxes = len(annotations)\n",
    "        boxes = torch.zeros(num_boxes, 4)\n",
    "        for i, ann in enumerate(annotations):\n",
    "            bbox = torch.as_tensor(ann[\"bbox\"])\n",
    "            boxes[i, :] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "\n",
    "        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n",
    "        scores = torch.ones(num_boxes)\n",
    "\n",
    "        # Format metadata\n",
    "        datum_metadata: DatumMetadata = {\n",
    "            \"id\": self._dataset.ids[index],\n",
    "        }\n",
    "\n",
    "        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset: od.Dataset = CocoDetectionDataset(tv_dataset, \"COCO Detection Subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a dataset class that conforms to the `maite.protocols.object_detection.Dataset` protocol, and wrapped the native dataset with it.\n",
    "\n",
    "Here the `coco_dataset` variable has `od.Dataset` as the type hint. If your development environment has a static type checker like Pyright enabled (see the [Enable Static Type Checking](./static_typing.html) guide), then the type checker will verify that our wrapped dataset conforms to the protocol and indicate a problem if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Examine the MAITE-wrapped dataset\n",
    "\n",
    "Now let's inspect MAITE-compliant object detection dataset to verify that it's behaving as expected.\n",
    "\n",
    "First we verify that the length is 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(coco_dataset) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the dataset-level metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(coco_dataset.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we inspect a single datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first datum\n",
    "image, target, datum_metadata = coco_dataset[0]\n",
    "\n",
    "# Display image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Optional: Hide the axes\n",
    "plt.show()\n",
    "\n",
    "# Bridge/convert ArrayLike's to PyTorch tensors\n",
    "image = torch.as_tensor(image)\n",
    "boxes = torch.as_tensor(target.boxes)\n",
    "labels = torch.as_tensor(target.labels)\n",
    "scores = torch.as_tensor(target.scores)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"{image.shape = }\")  # image has height 230 and weight 352\n",
    "print(f\"{boxes.shape = }\")  # there are 14 bounding boxes\n",
    "print(f\"{labels.shape = }\")\n",
    "print(f\"{scores.shape = }\")\n",
    "\n",
    "# Print datum-level metadata\n",
    "print(f\"{datum_metadata = }\")  # this datum corresponds to image file 000000037777.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "In this how-to guide, we demonstrated how to wrap an object detection dataset to be MAITE compliant. Key implementation steps included:\n",
    "\n",
    "- Defining `__len__` and `__getitem__` methods\n",
    "- Choosing a specific image `InputType` (e.g., `numpy.ndarray` or `torch.Tensor`)\n",
    "- Creating a target dataclass that conforms to the `ObjectDetectionTarget` protocol\n",
    "- Setting up `DatasetMetadata` and `DatumMetadata`\n",
    "\n",
    "More dataset protocol details can be found here: [maite.protocols.object_detection.Dataset](../generated/maite.protocols.object_detection.Dataset.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i628_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
