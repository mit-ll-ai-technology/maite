{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n",
    "Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).\n",
    "SPDX-License-Identifier: MIT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap an Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide explains how to wrap common object detection models, such as the [Torchvision Faster R-CNN](https://pytorch.org/vision/stable/models.html#object-detection) and [Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8/), to create models that conform to MAITE's [maite.protocols.object_detection.Model](../generated/maite.protocols.object_detection.Model.html) protocol.\n",
    "\n",
    "The general steps for wrapping a model are:\n",
    "\n",
    "* Understand the source (native) model\n",
    "* Create a wrapper that makes the source model conform to the MAITE model protocol (interface)\n",
    "* Verify that the wrapped model works correctly and has no static type checking errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load the Pretrained Faster R-CNN and YOLOv8 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the required Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import urllib.request\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    ")\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import maite.protocols.object_detection as od\n",
    "from maite.protocols import ArrayLike, ModelMetadata\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the (native) Faster R-CNN and YOLOv8 models using the pretrained weights provided by their respective libraries. The models were trained on the [COCO dataset](https://cocodataset.org/#home).\n",
    "\n",
    "Note that there are additional parameters you could pass into each model's initialization function, which can affect its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load R-CNN model\n",
    "rcnn_weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "rcnn_model = fasterrcnn_resnet50_fpn_v2(weights=rcnn_weights, box_score_thresh=0.9)\n",
    "rcnn_model.eval()  # set the RCNN to eval mode (defaults to training)\n",
    "\n",
    "# Load YOLOv8 Nano model\n",
    "yolov8_model = YOLO(\"yolov8n.pt\")  # weights will download automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Perform Model Inference on Sample Images\n",
    "\n",
    "Object detection models vary in how they handle input data preprocessing. For example, the R-CNN model requires manual input transformations (e.g., resizing and normalization) and conversion to tensors prior to conducting inference. In contrast, the YOLOv8 model automatically resizes and normalizes inputs, which can be in a variety of formats including PIL.\n",
    "\n",
    "We will first download two sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://github.com/pytorch/vision/blob/main/test/assets/encode_jpeg/grace_hopper_517x606.jpg?raw=true\"\n",
    "image_data = urllib.request.urlopen(img_url).read()\n",
    "pil_img_1 = PIL.Image.open(io.BytesIO(image_data))\n",
    "\n",
    "img_url = \"https://www.ultralytics.com/images/bus.jpg\"\n",
    "image_data = urllib.request.urlopen(img_url).read()\n",
    "pil_img_2 = PIL.Image.open(io.BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we preprocess the images for the R-CNN model and run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "tensor_img_1 = pil_to_tensor(pil_img_1)\n",
    "tensor_img_2 = pil_to_tensor(pil_img_2)\n",
    "rcnn_imgs = [tensor_img_1, tensor_img_2]\n",
    "\n",
    "# Get the inference transforms assocated with these pretrained weights\n",
    "preprocess = rcnn_weights.transforms()\n",
    "\n",
    "# Apply inference preprocessing transforms\n",
    "batch = [preprocess(img) for img in rcnn_imgs]\n",
    "\n",
    "rcnn_preds = rcnn_model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run inference with the YOLO model on the PIL images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_imgs = [pil_img_1, pil_img_2]\n",
    "\n",
    "yolo_preds = yolov8_model(yolo_imgs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the input differences, notice the differences in output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_pred = rcnn_preds[0]\n",
    "yolo_pred = yolo_preds[0]\n",
    "\n",
    "rcnn_boxes = rcnn_pred[\"boxes\"]\n",
    "yolo_boxes = yolo_pred.boxes\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "R-CNN Outputs\n",
    "=============\n",
    "Result Type: {type(rcnn_pred)}\n",
    "Result Attributes: {rcnn_pred.keys()}\n",
    "Box Types: {type(rcnn_boxes)}\n",
    "\n",
    "YOLO Outputs\n",
    "============ \n",
    "Result Type: {type(yolo_pred)}\n",
    "Result Attributes: {yolo_pred._keys}\n",
    "Box Types: {type(yolo_boxes)}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-CNN model returns a dictionary with certain keys, while the YOLO model returns a custom Results class. We now proceed to wrap both models with MAITE to get the benefits of standarizing model inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Create MAITE Wrappers for the R-CNN and YOLOv8 Models\n",
    "\n",
    "We create two separate classes that implement the [maite.protocols.object_detection.Model](../generated/maite.protocols.object_detection.Model.html) protocol. A MAITE object detection model only needs to have the following method:\n",
    "\n",
    "- `__call__(input_batch: Sequence[ArrayLike])` to make model predictions for inputs in input batch. \n",
    "\n",
    "and an attribute:\n",
    "\n",
    "- `metadata`, which is a typed dictionary containing an `id` field for the model plus an optional (but highly recommended) map from class indexes to labels called `index2label`.\n",
    "\n",
    "Wrapping the models with MAITE provides a consistent interface for handling inputs and outputs (e.g., boxes and labels); this interoperabilty simplifies integration across diverse workflows and tools, e.g., downstream test & evaluation pipelines.\n",
    "\n",
    "We begin by creating common input and output types to be used by _both_ models, as well as an image rendering utility function.\n",
    "\n",
    "MAITE object detection models must output a type compatible with `maite.protocols.object_detection.ObjectDetectionTarget`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs: Sequence[od.InputType] = [tensor_img_1, tensor_img_2]\n",
    "\n",
    "@dataclass\n",
    "class ObjectDetectionTargetImpl:\n",
    "    boxes: np.ndarray\n",
    "    labels: np.ndarray\n",
    "    scores: np.ndarray\n",
    "\n",
    "def render_wrapped_results(imgs, preds, model_metadata):\n",
    "    names = model_metadata[\"index2label\"]\n",
    "    for img, pred in zip(imgs, preds):\n",
    "        pred_labels = [names[label] for label in pred.labels]\n",
    "        box = draw_bounding_boxes(\n",
    "            img,\n",
    "            boxes=torch.as_tensor(pred.boxes),\n",
    "            labels=pred_labels,\n",
    "            colors=\"red\",\n",
    "            width=4,\n",
    "            font=\"DejaVuSans\", # if necessary, change to TrueType font available on your system\n",
    "            font_size=30,\n",
    "        )\n",
    "        im = to_pil_image(box.detach())\n",
    "        h, w = im.size\n",
    "        im = im.resize((h // 2, w // 2))\n",
    "        display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Wrap the R-CNN Model\n",
    "\n",
    "As mentioned in Section 2, the R-CNN model requires manual preprocessing of our current input data.\n",
    "\n",
    "Since the input expected by the native model already conforms to `Sequence[od.InputType]` as-is, we can perform the required preprocessing inside of our wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedRCNN:\n",
    "    def __init__(\n",
    "        self, model: FasterRCNN, weights: FasterRCNN_ResNet50_FPN_V2_Weights, id: str, **kwargs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.weights = weights\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # Add required model metadata attribute\n",
    "        index2label = {i: category for i, category in enumerate(weights.meta[\"categories\"])}\n",
    "        self.metadata: ModelMetadata = {\n",
    "            \"id\": id,\n",
    "            \"index2label\": index2label\n",
    "        }\n",
    "\n",
    "    def __call__(self, batch: Sequence[od.InputType]) -> Sequence[ObjectDetectionTargetImpl]:\n",
    "        # Get MAITE inputs ready for native model\n",
    "        preprocess = self.weights.transforms()\n",
    "        batch = [preprocess(img) for img in batch]\n",
    "\n",
    "        # Perform inference\n",
    "        results = self.model(batch, **self.kwargs)\n",
    "\n",
    "        # Restructure results to conform to MAITE\n",
    "        all_detections = []\n",
    "        for result in results:\n",
    "            boxes = result[\"boxes\"].detach().numpy()\n",
    "            labels = result[\"labels\"].detach().numpy()\n",
    "            scores = result[\"scores\"].detach().numpy()\n",
    "            all_detections.append(\n",
    "                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n",
    "            )\n",
    "\n",
    "        return all_detections\n",
    "\n",
    "wrapped_rcnn_model: od.Model = WrappedRCNN(rcnn_model, rcnn_weights, \"TorchVision.FasterRCNN_ResNet50_FPN_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_rcnn_preds = wrapped_rcnn_model(imgs)\n",
    "wrapped_rcnn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_wrapped_results(imgs, wrapped_rcnn_preds, wrapped_rcnn_model.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Wrap the YOLO Model\n",
    "\n",
    "We previously passed PIL images to the (native) YOLO model, while the MAITE-compliant wrapper will be getting inputs of type `Sequence[od.InputType]` (which is an alias for `Sequence[ArrayLike]`).\n",
    "\n",
    "Note that MAITE requires the dimensions of each image in the input batch to be `(C, H, W)`, which corresponds to the image's color channels, height, and width, respectively.\n",
    "\n",
    "YOLO models, however, expect the input data to be `(H, W, C)`, so we will need to add an additional preprocessing step to tranpose the data.\n",
    "\n",
    "Furthermore, the underlying YOLO models process different input formats (e.g., filename, PIL image, NumPy array, PyTorch tensor) differently, so we cast the underlying input as an `np.ndarray` for consistency with how PIL images are handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedYOLO:\n",
    "    def __init__(self, model: YOLO, id: str, **kwargs):\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # Add required model metadata attribute\n",
    "        self.metadata: ModelMetadata = {\n",
    "            \"id\": id,\n",
    "            \"index2label\": model.names # already a mapping from integer class index to string name\n",
    "        }\n",
    "\n",
    "    def __call__(self, batch: Sequence[od.InputType]) -> Sequence[ObjectDetectionTargetImpl]:\n",
    "        # Get MAITE inputs ready for native model\n",
    "        # Bridge/convert input to np.ndarray and tranpose (C, H, W) -> (H, W, C)\n",
    "        batch = [np.asarray(x).transpose((1, 2, 0)) for x in batch]\n",
    "\n",
    "        # Perform inference\n",
    "        results = self.model(batch, **self.kwargs)\n",
    "\n",
    "        # Restructure results to conform to MAITE\n",
    "        all_detections = []\n",
    "        for result in results:\n",
    "            detections = result.boxes\n",
    "            if detections is None:\n",
    "                continue\n",
    "            detections = detections.cpu().numpy()\n",
    "            boxes = np.asarray(detections.xyxy)\n",
    "            labels = np.asarray(detections.cls, dtype=np.uint8)\n",
    "            scores = np.asarray(detections.conf)\n",
    "            all_detections.append(\n",
    "                ObjectDetectionTargetImpl(boxes=boxes, labels=labels, scores=scores)\n",
    "            )\n",
    "\n",
    "        return all_detections\n",
    "\n",
    "wrapped_yolov8_model: od.Model = WrappedYOLO(yolov8_model, id=\"Ultralytics.YOLOv8\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the wrapped model's output for both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_yolo_preds = wrapped_yolov8_model(imgs)\n",
    "wrapped_yolo_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_wrapped_results(imgs, wrapped_yolo_preds, wrapped_yolov8_model.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from both models for first image in batch\n",
    "wrapped_rcnn_pred = wrapped_rcnn_preds[0]\n",
    "wrapped_yolo_pred = wrapped_yolo_preds[0]\n",
    "\n",
    "wrapped_rcnn_fields = vars(wrapped_rcnn_pred).keys()\n",
    "wrapped_yolo_fields = vars(wrapped_yolo_pred).keys()\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Wrapped R-CNN Outputs\n",
    "=====================\n",
    "Result Type: {type(wrapped_rcnn_pred)}\n",
    "Result Attributes: {wrapped_rcnn_fields}\n",
    "\n",
    "YOLO Outputs\n",
    "============\n",
    "Result Type: {type(wrapped_yolo_pred)}\n",
    "Result Attributes: {wrapped_yolo_fields}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by wrapping both models to be MAITE-compliant, we were able to:\n",
    "\n",
    "* Use the same input data, `imgs`, for both models.\n",
    "* Create standardized output, conforming to `ObjectDetectionTarget`, from both models.\n",
    "* Render the results of both models using the same function, due to the standardized inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Summary\n",
    "Wrapping object detection models with MAITE ensures interoperability and simplifies integration with additional T&E processes. By standardizing inputs and outputs, you can create consistent workflows that work seamlessly across models, like Faster R-CNN and YOLOv8.\n",
    "\n",
    "The key to model wrapping is to define the following:\n",
    "\n",
    "* A `__call__` method that receives a `Sequence[od.InputType]` as input and returns `Sequence[od.TargetType]`.\n",
    "* A `metadata` attribute that's a typed dictionary with at least an \"id field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
