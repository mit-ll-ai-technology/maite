{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2025, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n",
    "Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).\n",
    "SPDX-License-Identifier: MIT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap an Image Classification Model\n",
    "\n",
    "In this how-to, we will show you how to wrap a [Torchvision ResNet50](https://pytorch.org/vision/stable/models.html#classification) to create a MAITE-compliant [maite.protocols.image_classification.Model](../generated/maite.protocols.image_classification.Model.html).\n",
    "\n",
    "The general steps for wrapping a model are:\n",
    "\n",
    "* Understand the source (native) model\n",
    "* Create a wrapper that makes the source model conform to the MAITE model protocol (interface)\n",
    "* Verify that the wrapped model works correctly and has no static type checking errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load the Pretrained ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the required Python libaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import urllib.request\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch as pt\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "\n",
    "import maite.protocols.image_classification as ic\n",
    "from maite.protocols import ArrayLike, ModelMetadata\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -iv -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the ResNet50 model (pretrained on the [ImageNet dataset](https://image-net.org/)), following the [Torchvision documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
    "\n",
    "model = torchvision.models.resnet50(\n",
    "    weights=model_weights\n",
    ")  # weights will download automatically\n",
    "\n",
    "model = model.eval()  # set the ResNet50 model to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Perform Model Inference on Sample Images\n",
    "\n",
    "Download the ImageNet labels and a couple of sample ImageNet images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_url = \"https://raw.githubusercontent.com/raghakot/keras-vis/refs/heads/master/resources/imagenet_class_index.json\"\n",
    "response = urllib.request.urlopen(labels_url).read()\n",
    "labels = json.loads(response.decode(\"utf-8\"))\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01491361_tiger_shark.JPEG\"\n",
    "image_data = urllib.request.urlopen(img_url).read()\n",
    "example_img_1 = PIL.Image.open(io.BytesIO(image_data))\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01695060_Komodo_dragon.JPEG\"\n",
    "image_data = urllib.request.urlopen(img_url).read()\n",
    "example_img_2 = PIL.Image.open(io.BytesIO(image_data))\n",
    "\n",
    "example_imgs = [example_img_1, example_img_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded labels are a dictionary from label number strings to lists containing [WordNet IDs](https://en.wikipedia.org/wiki/ImageNet#Categories) and human readable labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(labels.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check that the model works as expected on the sample images.\n",
    "\n",
    "Note the weights for Torchvision models include a `transforms()` method that performs model-specific input transformations, such as resizing, interpolating, etc., as required by the model. It is important to remember that there is no standard way to do this and it can vary for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_label(logits):\n",
    "    logits = logits.unsqueeze(0)\n",
    "    label_pred = logits.argmax().item()\n",
    "    return f\"{label_pred} {labels[str(label_pred)]}\"\n",
    "\n",
    "\n",
    "preprocess = model_weights.transforms()\n",
    "\n",
    "for example_img in example_imgs:\n",
    "    input = preprocess(example_img)\n",
    "    logits = model(input.unsqueeze(0))  # use unsqueeze to add batch dimension\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    ResNet50 Outputs\n",
    "    ================\n",
    "    Result Type: {type(logits)}\n",
    "    Result Shape: {logits.shape}\n",
    "    Sample Prediction: {prediction_label(logits)}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    display(example_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Create the MAITE Model Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MAITE [maite.protocols.image_classification.Model](../generated/maite.protocols.image_classification.Model.html) wrapper stores a reference to a model and model metadata. For this model we also store the preprocessing function.\n",
    "\n",
    "A MAITE-compliant image classification Model need only implement the following method:\n",
    "\n",
    "- `__call__(batch: Sequence[ArrayLike])` to make a model prediction for inputs in an input batch. It must transform its inputs from a `Sequence[ArrayLike]` to the format expected by the model, and transform the model outputs to a `Sequence[ArrayLike]` containing the predictions.\n",
    "\n",
    "and an attribute:\n",
    "\n",
    "- `metadata`, which is a typed dictionary containing an `id` field for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that MAITE requires the dimensions of each image in the input batch to be `(C, H, W)`, which corresponds to the image's color channels, height, and width, respectively. \n",
    "\n",
    "Torchvision's data preprocessing function, `transforms()`, mentioned in Section 2, already accepts `torch.Tensors` of shape `(C, H, W)`, which are compatible with MAITE `ArrayLike`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionResNetModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torchvision.models.ResNet,\n",
    "        preprocess: Callable[[pt.Tensor], pt.Tensor],\n",
    "    ) -> None:\n",
    "        self.metadata: ModelMetadata = {\n",
    "            \"id\": \"Torchvision ResNet ImageNet 1k\",\n",
    "            \"index2label\": {\n",
    "                int(idx): label for idx, [_wordnetid, label] in labels.items()\n",
    "            },\n",
    "        }\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[pt.Tensor]:\n",
    "        # Preprocess the inputs to ensure they match the model's input format\n",
    "        imgs_chw = []\n",
    "        for img_chw in batch:\n",
    "            imgs_chw.append(self.preprocess(pt.as_tensor(img_chw)))\n",
    "\n",
    "        # Create a shape-(N,C,H,W) tensor from the list of (C,H,W) tensors\n",
    "        # Note: Images have been preprocessed to all have the same shape\n",
    "        img_nchw = pt.stack(imgs_chw)\n",
    "\n",
    "        # Call the model\n",
    "        logits = self.model(img_nchw)\n",
    "\n",
    "        # Convert the shape-(N,num_classes) logits tensor into a list of shape-(num_classes,) tensors\n",
    "        return [t for t in logits]\n",
    "\n",
    "\n",
    "# Wrap the Torchvision ResNet model\n",
    "wrapped_model: ic.Model = TorchvisionResNetModel(model, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Examine the MAITE-wrapped Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a batch of MAITE `ArrayLike` test images and visualize the wrapped model's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_image_to_maite(img_pil):\n",
    "    # Convert the PIL image to a Numpy array\n",
    "    img_hwc = np.array(img_pil)  # shape (H, W, C)\n",
    "\n",
    "    # Use MAITE array index convention for representing images: (C, H, W)\n",
    "    img_chw = img_hwc.transpose(2, 0, 1)\n",
    "\n",
    "    return img_chw\n",
    "\n",
    "\n",
    "batch = [pil_image_to_maite(i) for i in example_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = wrapped_model(batch)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "ResNet50 Outputs\n",
    "================\n",
    "Result Type: {type(predictions)}\n",
    "Individual Prediction Type: {type(predictions[0])}\n",
    "Individual Prediction Shape: {pt.as_tensor(predictions[0]).shape}\n",
    "Sample Predictions:\"\"\"\n",
    ")\n",
    "for prediction, example_img in zip(predictions, example_imgs):\n",
    "    print(f\"    Predicted label = {prediction_label(prediction)}\")\n",
    "    display(example_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "Wrapping image classification models with MAITE ensures interoperability and simplifies integration into test & evaluation (T&E) procedures that \"know\" how to work with MAITE models (since the model's inputs and outputs are standardized). T&E procedures designed around MAITE protocols, such as MAITE's [evaluate](../generated/maite.tasks.evaluate.html), will work seamlessly across models, including ResNet50.\n",
    "\n",
    "The key to model wrapping is to define the following:\n",
    "\n",
    "* a `__call__` method that receives an input of type `Sequence[ic.InputType]` and returns a `Sequence[ic.TargetType]`.\n",
    "* a `metadata` typed dictionary attribute with at least an `id` field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i625_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
