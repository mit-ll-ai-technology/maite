{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2024, MASSACHUSETTS INSTITUTE OF TECHNOLOGY\n",
    "Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).\n",
    "SPDX-License-Identifier: MIT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision Object Detection Example\n",
    "\n",
    "The MAITE library provides interfaces for AI components such as datasets, models, metrics, and augmentations to make their use more consistent across test and evaluation (T&E) tools and workflows.\n",
    "\n",
    "In this tutorial, you will use MAITE in conjunction with a set of common libraries to:\n",
    "\n",
    "* Wrap an object detection dataset from Torchvision (COCO),\n",
    "* Wrap an object detection model from Torchvision (Faster RCNN),\n",
    "* Wrap a metric from Torchmetrics (mean average precision),\n",
    "* Compute performance on the clean dataset using MAITE's `evaluate` workflow utility,\n",
    "* Wrap an augmenation from Kornia (Gaussian noise), and\n",
    "* Compute performance on the perturbed dataset using `evaluate`.\n",
    "\n",
    "Once complete, you will have a basic understanding of MAITE’s interfaces for datasets, models, metric, and augmentations, as well as how to use MAITE’s native API for running evaluations.\n",
    "\n",
    "This tutorial does not assume any prior knowledge, but some experience with Python, machine learning, and the PyTorch framework may be helpful. Portions of code are adapted from the object detection example in the Torchvision [documentation](https://pytorch.org/vision/stable/models.html#object-detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This tutorial uses MAITE, PyTorch, Torchvision, Torchmetrics, Kornia, and Matplotlib. You can use the following commands to create a conda environment with these dependencies:\n",
    "\n",
    "```\n",
    "conda create --name torchvision_obj_det python=3.10\n",
    "conda activate torchvision_obj_det\n",
    "pip install maite jupyter torch torchvision torchmetrics pycocotools kornia\n",
    "```\n",
    "\n",
    "Note that the notebook was tested with these exact versions:\n",
    "\n",
    "```\n",
    "pip install maite jupyter==1.0.0 torch==2.3.1 torchvision==0.18.1 torchmetrics==1.4.0.post0 pycocotools==2.0.7 kornia==0.7.2\n",
    "```\n",
    "\n",
    "Now that we've created an environment, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import kornia.augmentation as K\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.ops.boxes import box_convert\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import maite.protocols.object_detection as od\n",
    "\n",
    "from maite.protocols import (\n",
    "    ArrayLike,\n",
    "    DatasetMetadata,\n",
    "    ModelMetadata,\n",
    "    MetricMetadata,\n",
    "    AugmentationMetadata,\n",
    "    DatumMetadata\n",
    ")\n",
    "from maite.workflows import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping a Torchvision Dataset\n",
    "\n",
    "We'll be wrapping Torchvision's [CocoDetection](https://pytorch.org/vision/stable/generated/torchvision.datasets.CocoDetection.html#torchvision.datasets.CocoDetection) dataset, which provides support for the COCO object detection dataset.\n",
    "\n",
    "In order to make this tutorial faster to run and not require a large download, we've provided a modified annotations JSON file from the validation split of the COCO 2017 Object Detection Task containing only the first 4 images (and will dynamically download only those images using the \"coco_url\").\n",
    "\n",
    "Note that the COCO annotations are licensed under a [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode) (see COCO [terms of use](https://cocodataset.org/#termsofuse)).\n",
    "\n",
    "### Native Dataset\n",
    "\n",
    "First we download the first 4 images of the validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(coco_json_subset: dict[str, Any], root: str | Path):\n",
    "    root = Path(root)\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for image in coco_json_subset[\"images\"]:\n",
    "        url = image[\"coco_url\"]\n",
    "        filename = Path(root) / image[\"file_name\"]\n",
    "        if filename.exists():\n",
    "            print(f\"skipping {url}\")\n",
    "        else:\n",
    "            print(f\"saving {url} to {filename} ... \", end=\"\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"done\")\n",
    "\n",
    "COCO_ROOT = Path(\"coco_val2017_subset\")\n",
    "ann_subset_file = COCO_ROOT / \"instances_val2017_first4.json\"\n",
    "coco_subset_json = json.load(open(ann_subset_file, \"r\"))\n",
    "\n",
    "download_images(coco_subset_json, root=COCO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the Torchvision dataset and verify that its reduced length is 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dataset = CocoDetection(\n",
    "    root=\"coco_val2017_subset\",\n",
    "    annFile=\"coco_val2017_subset/instances_val2017_first4.json\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{len(tv_dataset) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item of the dataset contains an image along with a list of annotation dictionaries. The bounding box (`bbox`) format is \"xywh\" and the `category_id` is one of COCO's 90 categories (each of which has a name and a supercategory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first image and its annotations\n",
    "img, annotations = tv_dataset[0]\n",
    "\n",
    "# Explore structure\n",
    "print(f\"{type(img) = }\")\n",
    "print(f\"{type(annotations) = }\")\n",
    "print(f\"{type(annotations[0]) = }\")\n",
    "print(f\"{len(annotations[0]) = }\")\n",
    "print(f\"{annotations[0].keys() = }\")\n",
    "print(f\"{annotations[0]['bbox'] = }\")\n",
    "print(f\"{annotations[0]['category_id'] = }\")\n",
    "print(f\"{tv_dataset.coco.cats[64] = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped Dataset\n",
    "\n",
    "In order to facilitate executing T&E workflows with datasets from difference sources (e.g., existing libraries like Torchvision or Hugging Face or custom datasets), MAITE provides a `Dataset` protocol that specifies the expected interface (i.e, a minimal set of required attributes, methods, and method type signatures).\n",
    "\n",
    "At a high level, a MAITE object detection `Dataset` needs to have two methods (`__len__` and `__getitem__`) and return the image, target (label/class), and metadata associated with a requested dataset index. \n",
    "\n",
    "Note: the `Dataset`-implementing class should raise an IndexError when `__getitem__` is called with indices beyond container bounds; this permits python to create a default `__iter__` implementation on objects implementing the `Dataset` protocol. Alternatively, users may directly implement their own `__iter__` method to support iteration.\n",
    "\n",
    "The following wrapper internally converts from the \"native\" format of the dataset to types compatible with MAITE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CocoDetectionTarget:\n",
    "    boxes: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    scores: torch.Tensor\n",
    "\n",
    "# Get mapping from COCO category to name\n",
    "index2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}\n",
    "\n",
    "class MaiteCocoDetection:\n",
    "    metadata: DatasetMetadata = {'id': 'COCO Detection Dataset'}\n",
    "    # We can optionally include index2label mapping within DatasetMetadata\n",
    "    metadata: DatasetMetadata = {**metadata, 'index2label': index2label} \n",
    "\n",
    "    def __init__(self, dataset: CocoDetection):\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[ArrayLike, CocoDetectionTarget, DatumMetadata]:\n",
    "        try:\n",
    "            # get original data item\n",
    "            img_pil, annotations = self._dataset[index]\n",
    "        except IndexError as e:\n",
    "            # Here the underlying dataset is raising an IndexError since the index is beyond the \n",
    "            # container's bounds. When wrapping custom datasets, wrappers are responsible to for\n",
    "            # raising an IndexError in `__getitem__` when an index exceeds the container's bounds;\n",
    "            # this enables iteration on the wrapper to properly terminate.\n",
    "            print(f\"The index number {index} is out of range for the dataset which has length {len(self._dataset)}\")\n",
    "            raise(e)\n",
    "        \n",
    "        # format input\n",
    "        img_pt = pil_to_tensor(img_pil)\n",
    "\n",
    "        # format ground truth\n",
    "        num_boxes = len(annotations)\n",
    "        boxes = torch.zeros(num_boxes, 4)\n",
    "        for i, ann in enumerate(annotations):\n",
    "            bbox = torch.as_tensor(ann[\"bbox\"])\n",
    "            boxes[i,:] = box_convert(bbox, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
    "        \n",
    "        labels = torch.as_tensor([ann[\"category_id\"] for ann in annotations])\n",
    "        scores = torch.ones(num_boxes)\n",
    "\n",
    "        # format metadata\n",
    "        datum_metadata: DatumMetadata = {\n",
    "            \"id\": self._dataset.ids[index],\n",
    "        }\n",
    "\n",
    "        return img_pt, CocoDetectionTarget(boxes, labels, scores), datum_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a wrapped version of the dataset that conforms to the MAITE protocol.\n",
    "\n",
    "Note that the `dataset` variable has `od.Dataset` as the type hint. If your environment has a static type checker enabled (e.g., the Pyright type checker via the Pylance language server in VS Code), then the type checker will verify that our wrapped dataset conforms to the protocol and indicate a problem if not (e.g., by underlining with a red squiggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: od.Dataset = MaiteCocoDetection(tv_dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll display a sample image along with the ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pil_image(\n",
    "    input: ArrayLike,\n",
    "    target: od.ObjectDetectionTarget,\n",
    "    index2label: dict[int, str],\n",
    "    color: str = \"red\"\n",
    ") -> Image.Image:\n",
    "    img_pt = torch.as_tensor(input)\n",
    "    boxes = torch.as_tensor(target.boxes)\n",
    "    label_ids = torch.as_tensor(target.labels)\n",
    "    label_names = [index2label[int(id.item())] for id in label_ids]\n",
    "    box = draw_bounding_boxes(img_pt, boxes=boxes, labels=label_names, colors=color, width=2)\n",
    "    return to_pil_image(box.detach()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mapping from COCO category to name\n",
    "index2label = {k: v[\"name\"] for k, v in tv_dataset.coco.cats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample image and overlay ground truth annotations (bounding boxes)\n",
    "i = 0\n",
    "input, target, _ = dataset[i]\n",
    "img = create_pil_image(input, target, index2label)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Ground Truth\")\n",
    "ax.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping a Torchvision Model\n",
    "\n",
    "In this section, we'll wrap a Torchvision object detection model that's been pretrained on the COCO dataset.\n",
    "\n",
    "First we create the \"native\" Torchvision model with a specified set of pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "tv_model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9, progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wrap the model to conform to the MAITE `od.Model` protocol, which requires a `__call__` method that takes a batch of inputs and returns a batch of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionDetector:\n",
    "    def __init__(self, model: torch.nn.Module, metadata: ModelMetadata, transforms: Any, device: str):\n",
    "        self.model = model\n",
    "        self.metadata = metadata\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[CocoDetectionTarget]:\n",
    "\n",
    "        # convert to list of tensors, transfer to device, and apply inference transforms\n",
    "        # - https://pytorch.org/vision/stable/models.html\n",
    "        # - \"The models expect a list of Tensor[C, H, W].\"\n",
    "        tv_input = [self.transforms(torch.as_tensor(b_elem)).to(self.device) for b_elem in batch]\n",
    "\n",
    "        # get predictions\n",
    "        tv_predictions = self.model(tv_input)\n",
    "\n",
    "        # reformat output\n",
    "        predictions = [\n",
    "            CocoDetectionTarget(\n",
    "                p[\"boxes\"].detach().cpu(),\n",
    "                p[\"labels\"].detach().cpu(),\n",
    "                p[\"scores\"].detach().cpu()\n",
    "            )\n",
    "            for p in tv_predictions\n",
    "        ]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: od.Model = TorchvisionDetector(\n",
    "    model=tv_model,\n",
    "    metadata={'id': 'TorchvisionDetector', 'index2label': index2label},\n",
    "    transforms=weights.transforms(),\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial test, we'll manually create an input batch and perform inference on it with the wrapped model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch with sample image\n",
    "i = 0\n",
    "x, y, md = dataset[i]\n",
    "x = torch.as_tensor(x)\n",
    "xb, yb, mdb = x.unsqueeze(0), [y], [md]\n",
    "print(f\"{xb.shape = }\")\n",
    "\n",
    "# Get predictions for batch(which just has one image for this example)\n",
    "preds = model([xb[0]])\n",
    "\n",
    "# Overlay detections on image\n",
    "img = create_pil_image(xb[0], preds[0], index2label)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Prediction\")\n",
    "ax.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively, it appears that the model has detected the majority of the objects, but not all.\n",
    "\n",
    "At this point, we'd like to perform a more quantitative evaluation across a larger set of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section we wrap a Torchmetrics object detection metric to conform to the MAITE `od.Metric` protocol.\n",
    "\n",
    "First we create a \"native\" Torchmetrics mean average prediction (mAP) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metric = MeanAveragePrecision(\n",
    "    box_format=\"xyxy\",\n",
    "    iou_type=\"bbox\",\n",
    "    iou_thresholds=[0.5],\n",
    "    rec_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    max_detection_thresholds=[1, 10, 100],\n",
    "    class_metrics=False,\n",
    "    extended_summary=False,\n",
    "    average=\"macro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wrap the metric as a MAITE `od.Metric` that has the required `update`, `compute`, and `reset` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedTorchmetricsMetric:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tm_metric: Callable[\n",
    "            [list[dict[str, torch.Tensor]], list[dict[str, torch.Tensor]]],\n",
    "            dict[str, Any],\n",
    "        ],\n",
    "        metadata: MetricMetadata\n",
    "    ):\n",
    "        self._tm_metric = tm_metric\n",
    "        self.metadata = metadata\n",
    "\n",
    "    # Create utility function to convert ObjectDetectionTarget_impl type to what\n",
    "    # the type expected by torchmetrics IntersectionOverUnion metric\n",
    "    @staticmethod\n",
    "    def to_tensor_dict(target: od.ObjectDetectionTarget) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convert an ObjectDetectionTarget_impl into a dictionary expected internally by\n",
    "        raw `update` method of raw torchmetrics method\n",
    "        \"\"\"\n",
    "        out = {\n",
    "            \"boxes\": torch.as_tensor(target.boxes),\n",
    "            \"scores\": torch.as_tensor(target.scores),\n",
    "            \"labels\": torch.as_tensor(target.labels),\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, preds: od.TargetBatchType, targets: od.TargetBatchType) -> None:\n",
    "        # Convert to natively-typed from of preds/targets\n",
    "        preds_tm = [self.to_tensor_dict(pred) for pred in preds]\n",
    "        targets_tm = [self.to_tensor_dict(tgt) for tgt in targets]\n",
    "        self._tm_metric.update(preds_tm, targets_tm)\n",
    "\n",
    "    def compute(self) -> dict[str, Any]:\n",
    "        return self._tm_metric.compute()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._tm_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_metric: od.Metric = WrappedTorchmetricsMetric(tm_metric, metadata={'id': 'torchmetrics_map_metric'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows\n",
    "\n",
    "Now we'll run MAITE's `evaluate` workflow, which manages the process of applying the model to the dataset (performing inference) and computing the desired metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluate over original (clean) dataset\n",
    "results, _, _ = evaluate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    metric=mAP_metric\n",
    ")\n",
    "\n",
    "# Report mAP_50 performance\n",
    "results[\"map_50\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "The `evaluate` workflow takes an optional augmentation to allow measuring performance on a perturbed (degraded) version of the dataset. This is useful for evaluating the robustness of a model to a natural perturbation like noise.\n",
    "\n",
    "In this section we'll wrap an augmentation from the Kornia library and re-run the evaluation.\n",
    "\n",
    "First we create the \"native\" Kornia augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kornia_noise = K.RandomGaussianNoise(\n",
    "    mean=0.0,\n",
    "    std=0.08, # relative to [0, 1] pixel values\n",
    "    p=1.0,\n",
    "    keepdim=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wrap it as a MAITE `od.Augmentation`, with a `__call__` method that operations on batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedKorniaAugmentation:\n",
    "    def __init__(self, kornia_aug: Any, metadata: AugmentationMetadata):\n",
    "        self.kornia_aug = kornia_aug\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType],\n",
    "    ) -> tuple[od.InputBatchType, od.TargetBatchType, od.DatumMetadataBatchType]:\n",
    "        # Unpack tuple\n",
    "        xb, yb, metadata = batch\n",
    "\n",
    "        # Type narrow / bridge input batch to PyTorch tensor\n",
    "        xb_pt = [torch.as_tensor(xb_i) for xb_i in xb]\n",
    "        assert xb_pt[0].ndim == 3, 'Input should be sequence of 3d ArrayLikes'\n",
    "\n",
    "        # Apply augmentation to batch\n",
    "        # Return augmentation outputs as uint8\n",
    "        # - NOTE: assumes input batch has pixels in [0, 255]\n",
    "        xb_aug = [(self.kornia_aug(xb_pti / 255.0).clamp(min=0.0, max=1.0) * 255.0).to(torch.uint8) for xb_pti in xb_pt]\n",
    "\n",
    "        # Return augmented inputs and pass through unchanged targets and metadata\n",
    "        return xb_aug, yb, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise: od.Augmentation = WrappedKorniaAugmentation(\n",
    "    kornia_noise, metadata={\"id\": \"kornia_rnd_gauss_noise\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial test, we manually create an input batch and perturb it with the wrapped augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch with sample image\n",
    "i = 0\n",
    "x, y, md = dataset[i]\n",
    "x = torch.as_tensor(x)\n",
    "xb, yb, mdb = [x], [y], [md]\n",
    "\n",
    "# Apply augmentation\n",
    "xb_aug, yb_aug, mdb_aug = noise((xb, yb, mdb))\n",
    "\n",
    "# Get predictions for augmented batch (which just has one image for this example)\n",
    "preds_aug = model(xb_aug)\n",
    "\n",
    "# Overlay detections on image\n",
    "xb_aug = torch.as_tensor(xb_aug[0])\n",
    "img_aug = create_pil_image(xb_aug, preds_aug[0], index2label)\n",
    "\n",
    "# Show result\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Perturbed\")\n",
    "ax.imshow(img_aug);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the noise has resulted in some errors.\n",
    "\n",
    "Finally, we run `evaluate` with the augmentation over the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluate over perturbed dataset\n",
    "results, _, _ = evaluate(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    metric=mAP_metric,\n",
    "    augmentation=noise,\n",
    ")\n",
    "\n",
    "# Report mAP_50 performance\n",
    "results[\"map_50\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mAP_50 performance has decreased due to the simulated image degradation.\n",
    "\n",
    "Congrats! You have now successfully used MAITE to wrap a dataset, model, metric, and augmentation from various libraries, and run an evaluation to compute the performance of the pretrained model on both clean and perturbed versions of the COCO validation subset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv565",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
