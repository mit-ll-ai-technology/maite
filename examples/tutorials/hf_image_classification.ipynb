{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright 2024, MASSACHUSETTS INSTITUTE OF TECHNOLOGY<br/>\n",
    "> Subject to FAR 52.227-11 – Patent Rights – Ownership by the Contractor (May 2014).<br/>\n",
    "> SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Image Classification Example\n",
    "\n",
    "The MAITE library provides interfaces for AI components such as datasets, models, metrics, and augmentations to make their use more consistent across test and evaluation (T&E) tools and workflows.\n",
    "\n",
    "In this tutorial you will use MAITE, in conjunction with a set of common libraries, to:\n",
    "\n",
    "- Wrap an image classification dataset from Hugging Face (CIFAR-10),\n",
    "- Wrap an image classification model from Hugging Face (Vision Transformer),\n",
    "- Wrap a metric from TorchMetrics (multiclass accuracy), and\n",
    "- Compute performance on the clean dataset using MAITE's evaluate workflow utility.\n",
    "\n",
    "Once complete, you will have a basic understanding of MAITE’s interfaces for datasets, models, and metrics, as well as how to use MAITE’s native API for running evaluations.\n",
    "\n",
    "This tutorial does not assume any prior knowledge, but some experience with Python, machine learning, and the PyTorch framework may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This tutorial uses MAITE, PyTorch, Torchvision, TorchMetrics, Hugging Face datasets and transformers, and Matplotlib. \n",
    "\n",
    "For running this notebook on your local machine, you can use the following commands to create a conda environment with the required dependencies:\n",
    "\n",
    "```\n",
    "conda create --name hf_image_classification python=3.10 pip\n",
    "conda activate hf_image_classification\n",
    "pip install maite datasets jupyter matplotlib torch torchmetrics torchvision transformers watermark\n",
    "```\n",
    "\n",
    "Now that you have an environment, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import maite.protocols.image_classification as ic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from maite.protocols import ArrayLike, DatasetMetadata, MetricMetadata, ModelMetadata\n",
    "from maite.workflows import evaluate\n",
    "from torchmetrics import Accuracy, Metric\n",
    "from torchvision.transforms.functional import to_tensor, resize\n",
    "from transformers import AutoModelForImageClassification, ViTForImageClassification\n",
    "from typing import Any, Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watermark import watermark\n",
    "print(\"This notebook was executed with the following:\\n\")\n",
    "print(watermark(python=True, packages=\"datasets,jupyter,matplotlib,numpy,torch,torchmetrics,torchvision,transformers,watermark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping a Hugging Face Dataset\n",
    "\n",
    "We'll be working with a common computer vision benchmark dataset called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of color images (size 32 x 32 pixels) covering 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). The dataset is available through the Hugging Face `datasets` library, which provides access to officially curated datasets as well as datasets contributed to [Hugging Face Hub](https://huggingface.co/datasets) from the machine learning community.\n",
    "\n",
    "First we load a subset of the \"native\" Hugging Face dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 256\n",
    "hf_dataset: datasets.Dataset = datasets.load_dataset(\"cifar10\", split=f\"test[:{subset_size}]\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap the dataset so it can be used with MAITE.\n",
    "\n",
    "In order to facilitate executing T&E workflows with datasets from difference sources (e.g., existing libraries like Torchvision or Hugging Face or custom datasets), MAITE provides a `Dataset` protocol that specifies the expected interface (i.e, a minimal set of required attributes, methods, and method type signatures).\n",
    "\n",
    "At a high level, a MAITE image classification dataset needs to have two methods (`__len__` and `__getitem__`) and return the image, target (label/class), and metadata associated with a requested dataset index. The dataset also needs to have a `metadata` attribute containing some basic metadata (at least an `id` field).\n",
    "\n",
    "The following wrapper internally converts from the \"native\" format of the dataset to types compatible with MAITE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceDataset:\n",
    "    def __init__(self, hf_dataset: datasets.Dataset, id: str, index2label: dict[int, str], resize_shape: Optional[list[int]] = None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.num_classes = hf_dataset.features[\"label\"].num_classes\n",
    "        self.resize_shape = resize_shape\n",
    "\n",
    "        # Create required dataset metadata attribute\n",
    "        self.metadata: DatasetMetadata = DatasetMetadata(id=id, index2label=index2label)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor, ic.DatumMetadataType]:\n",
    "        if index < 0 or index >= len(self):\n",
    "            raise IndexError(f\"Index {index} is out of range for the dataset, which has length {len(self)}.\")\n",
    "\n",
    "        # Get the PIL image and integer label from the base HF dataset element (which is a dictionary)\n",
    "        item = self.hf_dataset[index]\n",
    "        img_pil = item[\"img\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Convert the PIL image to a PyTorch tensor for compatibility with PyTorch libraries\n",
    "        img_pt = to_tensor(img_pil)\n",
    "\n",
    "        # Apply resizing if requested\n",
    "        if self.resize_shape is not None:\n",
    "            img_pt = resize(img_pt, self.resize_shape)\n",
    "\n",
    "        # Create one-hot encoded tensor with true class label for this image\n",
    "        target = torch.zeros(self.num_classes)\n",
    "        target[label] = 1\n",
    "\n",
    "        return img_pt, target, ic.DatumMetadataType(id=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an instance of the MAITE complient version of the Hugging Face dataset.\n",
    "\n",
    "Note that the dataset variable has `ic.Dataset` as the type hint. If your environment has a static type checker enabled (e.g., the Pyright type checker via the Pylance language server in VS Code), then the type checker will verify that our wrapped dataset conforms to the protocol and indicate a problem if not (e.g., by underlining with a red squiggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map from integer class index to string label\n",
    "num_classes = hf_dataset.features[\"label\"].num_classes\n",
    "index2label = {i: hf_dataset.features[\"label\"].int2str(i) for i in range(num_classes)}\n",
    "\n",
    "# Wrap dataset\n",
    "wrapped_hf_dataset: ic.Dataset = HuggingFaceDataset(\n",
    "    hf_dataset,\n",
    "    id=\"CIFAR-10\",\n",
    "    index2label=index2label,\n",
    "    resize_shape=[224, 224]\n",
    ")\n",
    "\n",
    "print(f\"{len(wrapped_hf_dataset) = }\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some sample CIFAR-10 images along with their ground truth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 6\n",
    "fig, ax = plt.subplots(1, ncols, figsize=(6, 2))\n",
    "for i in range(ncols):\n",
    "    # Get datum i\n",
    "    img, label_onehot, md = wrapped_hf_dataset[i]\n",
    "    \n",
    "    # Convert to NumPy array in height, width, color channel (HWC) order (for display with matplotlib)\n",
    "    img_np = np.asarray(img).transpose(1, 2, 0)\n",
    "    \n",
    "    # Get ground truth class index and label\n",
    "    index = torch.as_tensor(label_onehot).argmax().item()\n",
    "    label = index2label[int(index)]\n",
    "\n",
    "    # Plot image with label\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].imshow(img_np)\n",
    "    ax[i].set_title(label)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping a Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll wrap a Hugging Face Vision Transformer (ViT) classification model that is available through Hugging Face Hub. The model has been trained on ImageNet-21k and fine-tuned on the CIFAR-10 dataset. \n",
    "\n",
    "First we load the \"native\" Hugging Face model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model: ViTForImageClassification = AutoModelForImageClassification.from_pretrained(\n",
    "    \"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wrap the model to conform to the MAITE `ic.Model` protocol, which requires a `__call__` method that takes a batch of inputs and returns a batch of predictions. The model also needs to have a `metadata` attribute containing some basic metadata (at least an `id` field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModel:\n",
    "    def __init__(self, hf_model: ViTForImageClassification, id: str, index2label: dict[int, str], device: str = \"cpu\"):\n",
    "        self.hf_model = hf_model\n",
    "        self.device = device\n",
    "\n",
    "        # Create required model metadata attribute\n",
    "        self.metadata: ModelMetadata = ModelMetadata(id=id, index2label=index2label)\n",
    "\n",
    "        # Move the model to requested device and set to eval mode\n",
    "        self.hf_model.to(device) # type: ignore\n",
    "        self.hf_model.eval()\n",
    "\n",
    "    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[torch.Tensor]:\n",
    "        # Combine inputs into PyTorch tensor of shape-(N,C,H,W) (batch size, color channels, height, width)\n",
    "        batch_pt = torch.stack([torch.as_tensor(x) for x in batch])\n",
    "        \n",
    "        # Move tensor to the desired device\n",
    "        batch_pt = batch_pt.to(self.device)\n",
    "\n",
    "        # Apply model to batch (NOTE: preprocessing not needed for this particular HF model)\n",
    "        output = self.hf_model(batch_pt)\n",
    "\n",
    "        # Restructure to expected output format (sequence of probability/logit vectors)\n",
    "        result = [x for x in output.logits.detach().cpu()]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_hf_model: ic.Model = HuggingFaceModel(\n",
    "    hf_model,\n",
    "    id=\"vit-base-patch16-224-in21k-finetuned-cifar10\",\n",
    "    index2label=index2label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial test, we'll manually create an input batch and perform inference on it with the wrapped model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch with single image\n",
    "i = 0\n",
    "x, y, md = wrapped_hf_dataset[i]\n",
    "xb, yb, mdb = [x], [y], [md]\n",
    "\n",
    "# Apply model and get first (only) prediction of size-1 batch of results\n",
    "preds = wrapped_hf_model(xb)[0]\n",
    "y_hat = torch.as_tensor(preds).argmax().item()\n",
    "    \n",
    "# Plot image with model prediction\n",
    "fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
    "img_np = np.asarray(x).transpose(1, 2, 0)\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(img_np)\n",
    "ax.set_title(f\"pred: {index2label[int(y_hat)]}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model predicts the correct class for the first example. But we'd like to perform a more quantitative evaluation across a larger set of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section we wrap a TorchMetrics metric to conform to the MAITE `ic.Metric` protocol.\n",
    "\n",
    "First we create a \"native\" TorchMetrics accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_acc: Metric = Accuracy(task=\"multiclass\", num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wrap the metric as a MAITE `ic.Metric` that has the required update, compute, and reset methods, as well as the required metadata attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMetricsClassificationMetric:\n",
    "    def __init__(self, tm_metric: Metric, name: str, device: str = \"cpu\"):\n",
    "        self.tm_metric = tm_metric\n",
    "        self.name = name\n",
    "        self.device = device\n",
    "\n",
    "        # Create required metric metadata attribute\n",
    "        self.metadata: MetricMetadata = MetricMetadata(id=name)\n",
    "\n",
    "    def reset(self):\n",
    "        self.tm_metric.reset()\n",
    "    \n",
    "    def update(self, preds: Sequence[ArrayLike], targets: Sequence[ArrayLike]) -> None:\n",
    "        # Convert inputs to PyTorch tensors of shape-(N, num_classes)\n",
    "        preds_pt: torch.Tensor = torch.stack([torch.as_tensor(x) for x in preds]).to(self.device)\n",
    "        assert preds_pt.ndim == 2\n",
    "        \n",
    "        targets_pt: torch.Tensor = torch.stack([torch.as_tensor(x) for x in targets]).to(self.device)\n",
    "        assert targets_pt.ndim == 2\n",
    "\n",
    "        # Convert probabilities/logits to predicted class indices and update native TorchMetrics metric\n",
    "        self.tm_metric.update(preds_pt.argmax(dim=1), targets_pt.argmax(dim=1))\n",
    " \n",
    "    def compute(self) -> dict[str, Any]:\n",
    "        result = {}\n",
    "        result[self.name] = self.tm_metric.compute()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tm_acc: ic.Metric = TorchMetricsClassificationMetric(tm_acc, \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflows\n",
    "\n",
    "Now we'll run MAITE's `evaluate` workflow, which manages the process of performing model inference on the dataset and computing the desired metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _, _ = evaluate(\n",
    "    dataset=wrapped_hf_dataset,\n",
    "    model=wrapped_hf_model,\n",
    "    metric=wrapped_tm_acc\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model performs very well on this dataset, achieving an accuracy of over 95%.\n",
    "\n",
    "Congratulations! You have now successfully used MAITE to wrap a dataset, model, and metric from various libraries, and run an evaluation to compute the performance of the pretrained model on a subset of the CIFAR-10 test split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mr156_py39_nopin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
